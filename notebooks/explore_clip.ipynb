{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip list\n",
    "# ! pip freeze > requirements.txt\n",
    "# ! conda env export > environment.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages needed for the unified framework\n",
    "# ! pip install git+https://github.com/openai/CLIP.git\n",
    "# ! pip install open-clip-torch\n",
    "# ! pip install img2dataset\n",
    "# ! pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages needed to run this notebook\n",
    "# ! pip install git+https://github.com/ginihumer/Amumo.git\n",
    "# ! pip install plotly\n",
    "# ! pip install ipywidgets\n",
    "# ! pip install scikit-learn\n",
    "# ! pip install openTSNE\n",
    "# ! pip install umap-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see https://plotly.com/python/v3/cars-exploration/ for interactive plotly guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from ipywidgets import widgets\n",
    "import io\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cach data for faster interaction\n",
    "data_checkpoint_dir = 'data_checkpoints/'\n",
    "\n",
    "if not os.path.exists(data_checkpoint_dir):\n",
    "    os.makedirs(data_checkpoint_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amumo import model\n",
    "from amumo.model import get_model\n",
    "# from src.model import get_model\n",
    "# import src.model as model\n",
    "\n",
    "\n",
    "def get_embedding(model_name, dataset_name, all_images, all_prompts):\n",
    "    clip_model = get_model(model_name, device=device)\n",
    "\n",
    "    data_prefix = dataset_name + '_' + clip_model.model_name + '_' + clip_model.name\n",
    "    data_prefix = data_prefix.replace('/','-')\n",
    "    if not os.path.exists(data_checkpoint_dir + data_prefix + '_image-embedding.csv') or not os.path.exists(data_checkpoint_dir + data_prefix + '_text-embedding.csv'):\n",
    "        with torch.no_grad():\n",
    "            image_features = clip_model.encode_image(all_images).float()\n",
    "            text_features = clip_model.encode_text(all_prompts).float()\n",
    "\n",
    "        np.savetxt(data_checkpoint_dir + data_prefix + '_image-embedding.csv', image_features.cpu(), delimiter = ',')\n",
    "        np.savetxt(data_checkpoint_dir + data_prefix + '_text-embedding.csv', text_features.cpu(), delimiter = ',')\n",
    "    else:\n",
    "        print('found cached embeddings for', data_prefix)\n",
    "        image_features = torch.from_numpy(np.genfromtxt(data_checkpoint_dir + data_prefix + '_image-embedding.csv', delimiter=\",\"))\n",
    "        text_features = torch.from_numpy(np.genfromtxt(data_checkpoint_dir + data_prefix + '_text-embedding.csv', delimiter=\",\"))\n",
    "\n",
    "    return image_features/image_features.norm(dim=-1, keepdim=True), text_features/text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "def get_similarity(image_features_norm, text_features_norm):\n",
    "    # similarity between images and texts\n",
    "    similarity = text_features_norm.cpu().numpy() @ image_features_norm.cpu().numpy().T # text x image \n",
    "\n",
    "    # similarity between all features\n",
    "    features_norm = torch.cat((image_features_norm, text_features_norm), dim = 0)\n",
    "    similarity_features = features_norm.cpu().numpy() @ features_norm.cpu().numpy().T\n",
    "    \n",
    "    return similarity, similarity_features\n",
    "\n",
    "\n",
    "import scipy\n",
    "import scipy.cluster.hierarchy as sch\n",
    "\n",
    "def get_cluster_sorting(similarity):\n",
    "    # adapted from https://wil.yegelwel.com/cluster-correlation-matrix/\n",
    "    linkage = sch.linkage(1-similarity, method='complete')\n",
    "    cluster_distance_threshold = (1-similarity).max()/2\n",
    "    idx_to_cluster_array = sch.fcluster(linkage, cluster_distance_threshold, criterion='distance')\n",
    "    idx = np.argsort(idx_to_cluster_array)\n",
    "    return idx, idx_to_cluster_array[idx], idx_to_cluster_array\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def aggregate_texts(emb_ids, all_prompts, input_type=None):\n",
    "    c_vec = CountVectorizer(ngram_range=(1, 77), stop_words=\"english\")\n",
    "    ngrams = c_vec.fit_transform([all_prompts[int(i)] for i in emb_ids])\n",
    "    vocab = c_vec.vocabulary_\n",
    "    count_values = ngrams.toarray().sum(axis=0)\n",
    "    \n",
    "    ngrams_sorted = sorted([(count_values[i],k) for k,i in vocab.items()], reverse=False, key=lambda sl: (len(sl[1]), sl[0]))\n",
    "    \n",
    "    ngrams_distinct = []\n",
    "    for i in range(len(ngrams_sorted)):\n",
    "        ng_count_a = ngrams_sorted[i][0]\n",
    "        ng_text_a = ngrams_sorted[i][1]\n",
    "\n",
    "        is_included = False\n",
    "        for j in range(i+1, len(ngrams_sorted)):\n",
    "            ng_count_b = ngrams_sorted[j][0]\n",
    "            ng_text_b = ngrams_sorted[j][1]\n",
    "\n",
    "            if ng_text_a in ng_text_b and ng_count_a <= ng_count_b:\n",
    "                is_included = True\n",
    "                break\n",
    "\n",
    "        if not is_included:\n",
    "            ngrams_distinct.append({\"text\": ng_text_a, \"value\": int(ng_count_a)})\n",
    "    \n",
    "    return (sorted(ngrams_distinct, key=lambda i: i['value'], reverse=True), \"text-value\")\n",
    "    # return (sorted([{\"value\": int(count_values[i]), \"text\": k} for k,i in vocab.items()], key=lambda i: i[\"value\"], reverse=True), \"text-value\")\n",
    "    \n",
    "def get_textual_label_for_cluster(emb_ids, all_prompts, k=2):\n",
    "    ngrams,_ = aggregate_texts(emb_ids, all_prompts)\n",
    "    return ' | '.join([ngram[\"text\"] for ngram in ngrams[:k]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "def l2_norm(x):\n",
    "    return x/np.linalg.norm(x, axis=-1, keepdims=True)\n",
    "\n",
    "def get_modality_gap(modal1_features, modal2_features):\n",
    "    return modal1_features.mean(axis=0) - modal2_features.mean(axis=0)\n",
    "\n",
    "def get_modality_gap_normed(modal1_features, modal2_features):\n",
    "    # with the normed vector, we can use a delta value that defines how much we want to go in each direction\n",
    "    return l2_norm(get_modality_gap(modal1_features, modal2_features))\n",
    "\n",
    "def get_modality_distance(modal1_features, modal2_features):\n",
    "    # Euclidean distance between mass centers\n",
    "    return np.linalg.norm(get_modality_gap(modal1_features, modal2_features))\n",
    "\n",
    "# give two lists of features, calculate loss\n",
    "\n",
    "def calculate_val_loss(image_features_np, text_features_np, logit_scale = 100.0):\n",
    "# give two lists of features, calculate loss\n",
    "\n",
    "    # normalized features\n",
    "    image_features_np /= np.linalg.norm(image_features_np, axis=-1, keepdims=True) + 1e-12\n",
    "    text_features_np /= np.linalg.norm(text_features_np, axis=-1, keepdims=True) + 1e-12\n",
    "\n",
    "    total_loss_list = list()\n",
    "\n",
    "    loss_img = nn.CrossEntropyLoss()\n",
    "    loss_txt = nn.CrossEntropyLoss()\n",
    "\n",
    "    BATCH_SIZE = 50 # 5000 in total. \n",
    "    for idx in range(len(image_features_np)//BATCH_SIZE):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            image_features = image_features_np[idx*50:(idx+1)*50]\n",
    "            text_features  = text_features_np[idx*50:(idx+1)*50]      \n",
    "        \n",
    "            # cosine similarity as logits\n",
    "            logits_per_image = logit_scale * image_features @ text_features.t()\n",
    "            logits_per_text = logits_per_image.t()\n",
    "\n",
    "            # # symmetric loss function\n",
    "            labels = torch.arange(BATCH_SIZE,dtype=torch.long)#.cuda()\n",
    "            loss_i = loss_img(logits_per_image, labels)\n",
    "            loss_t = loss_txt(logits_per_text.T, labels)\n",
    "            total_loss = (loss_i + loss_t)/2\n",
    "            \n",
    "            total_loss_list.append(total_loss.item())\n",
    "    avg_val_loss = np.mean(total_loss_list)\n",
    "    # print('avg_val_loss', avg_val_loss)\n",
    "    return avg_val_loss\n",
    "\n",
    "\n",
    "def get_gap_direction(image_features_np, text_features_np, pca):\n",
    "    image_features_np /= np.linalg.norm(image_features_np, axis=-1, keepdims=True) + 1e-12\n",
    "    text_features_np /= np.linalg.norm(text_features_np, axis=-1, keepdims=True) + 1e-12\n",
    "\n",
    "    pca_result = pca.transform(image_features_np)\n",
    "    pca_result2 = pca.transform(text_features_np)\n",
    "    pca_one_delta = pca_result2[:,0].mean() - pca_result[:,0].mean()\n",
    "\n",
    "    return np.sign(pca_one_delta)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Widget components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traitlets\n",
    "\n",
    "\n",
    "class SimilarityHeatmapWidget(widgets.VBox):\n",
    "    \n",
    "    value = traitlets.Any(np.zeros((6,6))).tag(sync=True)\n",
    "    cluster = traitlets.Any().tag(sync=True)\n",
    "\n",
    "    hover_idx = traitlets.List([]).tag(sync=True)\n",
    "\n",
    "\n",
    "    def __init__(self, zmin=None, zmax=None):\n",
    "        super(SimilarityHeatmapWidget, self).__init__()\n",
    "\n",
    "        self.fig_widget = go.FigureWidget(data=[go.Heatmap(z=self.value, zmin=zmin, zmax=zmax)])\n",
    "        self.heatmap = self.fig_widget.data[0]\n",
    "        self.heatmap.hoverinfo = \"text\"\n",
    "        self.fig_widget.update_layout(width=500, height=420,\n",
    "            xaxis = dict(\n",
    "                tickmode = 'array',\n",
    "                tickvals = [len(self.value)/4, 3*len(self.value)/4],\n",
    "                ticktext = ['Image', 'Text']\n",
    "            ),\n",
    "            yaxis = dict(\n",
    "                tickmode = 'array',\n",
    "                tickvals = [len(self.value)/4, 3*len(self.value)/4],\n",
    "                ticktext = ['Image', 'Text']\n",
    "            ),\n",
    "            margin=dict(l=10, r=10, t=10, b=10),\n",
    "        )\n",
    "        self.fig_widget.update_yaxes(autorange='reversed', fixedrange=False)\n",
    "        self.fig_widget.update_xaxes(fixedrange=False)\n",
    "        self.fig_widget.layout.shapes = self._get_matrix_gridlines()\n",
    "\n",
    "        self.children = [widgets.HBox([self.fig_widget])]\n",
    "\n",
    "\n",
    "\n",
    "    def _get_matrix_gridlines(self):\n",
    "        return [\n",
    "            go.layout.Shape(type='line', x0=len(self.value)/2-0.5, y0=0-0.5, x1=len(self.value)/2-0.5, y1=len(self.value)-0.5, line=dict(color=\"black\", width=1)),\n",
    "            go.layout.Shape(type='line', y0=len(self.value)/2-0.5, x0=0-0.5, y1=len(self.value)/2-0.5, x1=len(self.value)-0.5, line=dict(color=\"black\", width=1))\n",
    "        ]\n",
    "\n",
    "\n",
    "    @traitlets.validate(\"value\")\n",
    "    def _validate_value(self, proposal):\n",
    "        # print(\"TODO: validate value\")\n",
    "        return proposal.value\n",
    "\n",
    "    @traitlets.observe(\"value\")\n",
    "    def onUpdateValue(self, change):\n",
    "        self.fig_widget.data[0].z = self.value\n",
    "        self.fig_widget.layout.shapes = self._get_matrix_gridlines()\n",
    "\n",
    "        self.fig_widget.update_layout(\n",
    "            xaxis = dict(tickvals = [len(self.value)/4, 3*len(self.value)/4]),\n",
    "            yaxis = dict(tickvals = [len(self.value)/4, 3*len(self.value)/4])\n",
    "        )\n",
    "\n",
    "\n",
    "    # @traitlets.validate(\"cluster\")\n",
    "    # def _validate_cluster(self, proposal):\n",
    "        # takes a list of cluster labels + sizes\n",
    "    #     print(\"TODO: validate cluster\")\n",
    "    #     return proposal.value\n",
    "\n",
    "    @traitlets.observe(\"cluster\")\n",
    "    def onUpdateCluster(self, change):\n",
    "        cluster_shapes = self._get_matrix_gridlines()\n",
    "        labels, sizes = self.cluster\n",
    "        offset = 0-0.5 # -0.5 because heatmap rectangles are drawn around [-0.5, 0.5]\n",
    "        for (cluster_label, cluster_size) in zip(labels, sizes):\n",
    "            if cluster_size > 5:\n",
    "                textposition = 'middle left' if offset < len(self.value)/2/2 else 'middle right'\n",
    "\n",
    "                # see https://plotly.com/python/shapes/\n",
    "                cluster_shapes += [go.layout.Shape(\n",
    "                    type='rect', \n",
    "                    x0=len(self.value)/2+offset, \n",
    "                    y0=offset, \n",
    "                    x1=len(self.value)/2+offset+cluster_size, \n",
    "                    y1=offset+cluster_size, \n",
    "                    label=dict(text=cluster_label, textposition=textposition, font=dict(size=10, color=\"white\"), padding=cluster_size*5), \n",
    "                    line=dict(width=1, color='white')\n",
    "                )]\n",
    "\n",
    "            offset += cluster_size\n",
    "            \n",
    "        self.fig_widget.layout.shapes = cluster_shapes\n",
    "\n",
    "\n",
    "    @traitlets.observe(\"hover_idx\")\n",
    "    def onUpdateHoverIdx(self, change):\n",
    "        shapes = [sh for sh in self.fig_widget.layout.shapes if sh.name != 'hover_idx' and sh.name != 'hover_idx']\n",
    "\n",
    "        for (x_idx, y_idx) in self.hover_idx:\n",
    "            if x_idx >= 0 and x_idx < len(self.value):\n",
    "                shapes.append(go.layout.Shape(name='hover_idx', type='line', x0=x_idx, y0=0-0.5, x1=x_idx, y1=len(self.value)-0.5, line=dict(color=\"grey\", width=1)))\n",
    "            if y_idx >= 0 and y_idx < len(self.value):\n",
    "                shapes.append(go.layout.Shape(name='hover_idx', type='line', y0=y_idx, x0=0-0.5, y1=y_idx, x1=len(self.value)-0.5, line=dict(color=\"grey\", width=1)))\n",
    "        \n",
    "        self.fig_widget.layout.shapes = shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HoverWidget(widgets.VBox):\n",
    "    \n",
    "    valueX = traitlets.Any().tag(sync=True)\n",
    "    valueY = traitlets.Any().tag(sync=True)\n",
    "\n",
    "\n",
    "    def __init__(self, width=300):\n",
    "        super(HoverWidget, self).__init__()\n",
    "\n",
    "        self.width = width\n",
    "\n",
    "        output_dummy_img = io.BytesIO()\n",
    "        Image.new('RGB', (self.width,self.width)).save(output_dummy_img, format=\"JPEG\")\n",
    "        self.img_widgets = {'valueX': widgets.Image(value=output_dummy_img.getvalue(), width=0, height=0), \n",
    "                            'valueY': widgets.Image(value=output_dummy_img.getvalue(), width=0)} #, height=0)}\n",
    "        self.txt_widgets = {'valueX': widgets.HTML(value='', layout=widgets.Layout(width=\"%ipx\"%self.width)), \n",
    "                            'valueY': widgets.HTML(value='', layout=widgets.Layout(width=\"%ipx\"%self.width)), }\n",
    "        \n",
    "        self.children = [widgets.VBox(list(self.txt_widgets.values())), widgets.VBox(list(self.img_widgets.values()))]\n",
    "\n",
    "        self.layout = widgets.Layout(width=\"%ipx\"%(self.width+10), height=\"inherit\")\n",
    "\n",
    "\n",
    "    @traitlets.validate(\"value1\", \"value2\")\n",
    "    def _validate_value(self, proposal):\n",
    "        print(\"TODO: validate value1\")\n",
    "        return proposal.value\n",
    "\n",
    "    @traitlets.observe(\"valueX\", \"valueY\")\n",
    "    def onUpdateValue(self, change):\n",
    "        cur_img_widget = self.img_widgets[change.name]\n",
    "        cur_txt_widget = self.txt_widgets[change.name]\n",
    "        if type(change.new) is io.BytesIO:\n",
    "            cur_img_widget.value = change.new.getvalue()\n",
    "            cur_img_widget.width = self.width\n",
    "            # cur_img_widget.height = self.width\n",
    "            cur_txt_widget.value = \"\"\n",
    "        else:\n",
    "            cur_txt_widget.value = \"<div style='word-wrap: break-word;'>{}</div>\".format(change.new)\n",
    "            cur_img_widget.width = 0\n",
    "            # cur_img_widget.height = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from openTSNE import TSNE\n",
    "from umap import UMAP\n",
    "\n",
    "available_projection_methods = {\n",
    "    'PCA': {'module': PCA, 'OOS':False}, # OOS: flag to signal whether or not out of sample is possible\n",
    "    'TSNE': {'module': TSNE, 'OOS':False},\n",
    "    'UMAP': {'module': UMAP, 'OOS':True},\n",
    "}\n",
    "\n",
    "class ScatterPlotWidget(widgets.VBox):\n",
    "    \n",
    "    embedding = traitlets.Any().tag(sync=True)\n",
    "    cluster = traitlets.Any().tag(sync=True)\n",
    "\n",
    "    def __init__(self, seed=31415):\n",
    "        super(ScatterPlotWidget, self).__init__()\n",
    "\n",
    "        self.seed=seed\n",
    "\n",
    "        self.nr_components_widget = widgets.BoundedIntText(\n",
    "            value=2,\n",
    "            min=2,\n",
    "            max=10,\n",
    "            step=1,\n",
    "            description='Nr Components:',\n",
    "            disabled=False,\n",
    "            layout=widgets.Layout(width=\"150px\")\n",
    "        )\n",
    "        self.nr_components_widget.observe(self.onUpdateValue, 'value')\n",
    "\n",
    "        self.x_component_widget = widgets.BoundedIntText(\n",
    "            value=1,\n",
    "            min=1,\n",
    "            max=10,\n",
    "            step=1,\n",
    "            description='X component:',\n",
    "            disabled=False,\n",
    "            layout=widgets.Layout(width=\"150px\")\n",
    "        )\n",
    "        self.x_component_widget.observe(self.update_scatter, 'value')\n",
    "\n",
    "        self.y_component_widget = widgets.BoundedIntText(\n",
    "            value=2,\n",
    "            min=1,\n",
    "            max=10,\n",
    "            step=1,\n",
    "            description='Y component:',\n",
    "            disabled=False,\n",
    "            layout=widgets.Layout(width=\"150px\")\n",
    "        )\n",
    "        self.y_component_widget.observe(self.update_scatter, 'value')\n",
    "\n",
    "        traitlets.dlink((self.nr_components_widget, 'value'), (self.x_component_widget, 'max'))\n",
    "        traitlets.dlink((self.nr_components_widget, 'value'), (self.y_component_widget, 'max'))\n",
    "\n",
    "\n",
    "        self.fig_widget = go.FigureWidget()\n",
    "        self.fig_widget.add_trace(go.Scatter(name = 'image', x=[0,1,2,3], y=[0,1,2,3], mode=\"markers\", marker_color='blue'))\n",
    "        self.fig_widget.add_trace(go.Scatter(name = 'text', x=[3,2,1,0], y=[0,1,2,3], mode=\"markers\", marker_color='orange'))\n",
    "        self.fig_widget.update_layout(width=400, \n",
    "                                      height=300, \n",
    "                                      margin=dict(l=10, r=10, t=10, b=10),\n",
    "                                      legend=dict(\n",
    "                                            yanchor=\"top\",\n",
    "                                            y=0.99,\n",
    "                                            xanchor=\"left\",\n",
    "                                            x=0.01  \n",
    "                                            )\n",
    "                                      )\n",
    "\n",
    "        self.scatter_image = self.fig_widget.data[0]\n",
    "        self.scatter_text = self.fig_widget.data[1]\n",
    "\n",
    "        self.select_projection_method = widgets.Dropdown(\n",
    "            description='Method: ',\n",
    "            value='PCA',\n",
    "            options=list(available_projection_methods),\n",
    "        )\n",
    "        self.select_projection_method.observe(self._update_projection_method, 'value')\n",
    "\n",
    "        self.use_oos_projection = widgets.Checkbox(\n",
    "            value=False,\n",
    "            description='Use out of sample projection',\n",
    "            disabled=not available_projection_methods[self.select_projection_method.value]['OOS'],\n",
    "            indent=False\n",
    "        )\n",
    "        self.use_oos_projection.observe(self._update_projection_method, 'value')\n",
    "\n",
    "        \n",
    "        self.children = [self.select_projection_method, self.use_oos_projection, self.nr_components_widget, widgets.HBox([self.x_component_widget, self.y_component_widget]), self.fig_widget]\n",
    "        \n",
    "\n",
    "    def _update_projection_method(self, change):\n",
    "        print('', available_projection_methods[self.select_projection_method.value]['OOS'])\n",
    "        self.use_oos_projection.disabled = not available_projection_methods[self.select_projection_method.value]['OOS']\n",
    "        self.onUpdateValue(change)\n",
    "\n",
    "\n",
    "    @traitlets.validate(\"embedding\")\n",
    "    def _validate_value(self, proposal):\n",
    "        print(\"TODO: validate embedding\")\n",
    "        return proposal.value\n",
    "\n",
    "    @traitlets.observe(\"embedding\")\n",
    "    def onUpdateValue(self, change):\n",
    "        projection_method = available_projection_methods[self.select_projection_method.value]\n",
    "        # TODO: add UI for distance metric\n",
    "        if self.select_projection_method.value == 'PCA':\n",
    "            projection = projection_method['module'](n_components=self.nr_components_widget.value, random_state=self.seed)\n",
    "        else:\n",
    "            projection = projection_method['module'](n_components=self.nr_components_widget.value, metric=\"cosine\", random_state=self.seed)\n",
    "            \n",
    "\n",
    "        if not self.use_oos_projection.disabled and self.use_oos_projection.value:\n",
    "            project_by = 'image' # TODO: add user select for this\n",
    "    \n",
    "            if project_by == \"image\":\n",
    "                self.image_embedding_projection = projection.fit_transform(self.embedding[:int(len(self.embedding)/2),:])\n",
    "                self.text_embedding_projection = projection.transform(self.embedding[int(len(self.embedding)/2):,:])\n",
    "            elif project_by == \"text\":\n",
    "                self.text_embedding_projection = projection.fit_transform(self.embedding[int(len(self.embedding)/2):,:])\n",
    "                self.image_embedding_projection = projection.transform(self.embedding[:int(len(self.embedding)/2),:])\n",
    "\n",
    "            self.embedding_projection = np.concatenate((self.image_embedding_projection, self.text_embedding_projection))\n",
    "            \n",
    "        else:\n",
    "            if self.select_projection_method.value == 'TSNE':\n",
    "                self.embedding_projection = projection.fit(self.embedding)\n",
    "            else:\n",
    "                self.embedding_projection = projection.fit_transform(self.embedding)\n",
    "\n",
    "            self.image_embedding_projection = self.embedding_projection[:int(len(self.embedding)/2),:]\n",
    "            self.text_embedding_projection = self.embedding_projection[int(len(self.embedding)/2):,:]\n",
    "\n",
    "        self.update_scatter(change)\n",
    "\n",
    "    def update_scatter(self, change):\n",
    "        self.scatter_image.x = self.image_embedding_projection[:,self.x_component_widget.value-1]\n",
    "        self.scatter_image.y = self.image_embedding_projection[:,self.y_component_widget.value-1]\n",
    "\n",
    "        self.scatter_text.x = self.text_embedding_projection[:,self.x_component_widget.value-1]\n",
    "        self.scatter_text.y = self.text_embedding_projection[:,self.y_component_widget.value-1]\n",
    "\n",
    "        lines = []\n",
    "\n",
    "        for line_idx in range(len(self.text_embedding_projection)):\n",
    "            lines.append(go.layout.Shape(name='hover_idx', \n",
    "                                         type='line', \n",
    "                                         x0=self.image_embedding_projection[line_idx,self.x_component_widget.value-1], \n",
    "                                         y0=self.image_embedding_projection[line_idx,self.y_component_widget.value-1], \n",
    "                                         x1=self.text_embedding_projection[line_idx,self.x_component_widget.value-1], \n",
    "                                         y1=self.text_embedding_projection[line_idx,self.y_component_widget.value-1], \n",
    "                                         line=dict(color=\"grey\", width=1)))\n",
    "            # plt.plot((image_embedding_pca[line_idx,0], text_embedding_pca[line_idx,0]), (image_embedding_pca[line_idx,1], text_embedding_pca[line_idx,1]), 'black', linestyle='-', marker='', linewidth=1, alpha=0.2)\n",
    "\n",
    "        self.fig_widget.layout.shapes = lines\n",
    "\n",
    "\n",
    "    @traitlets.validate(\"cluster\")\n",
    "    def _validate_cluster(self, proposal):\n",
    "        # takes a list of cluster labels + sizes\n",
    "        # print(\"TODO: validate cluster\")\n",
    "        return proposal.value\n",
    "\n",
    "    @traitlets.observe(\"cluster\")\n",
    "    def onUpdateCluster(self, change):\n",
    "        print(change)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CLIPExplorerWidget(widgets.AppLayout):\n",
    "    idcs = traitlets.Any().tag(sync=True)\n",
    "\n",
    "    def __init__(self, dataset_name, all_images, all_prompts):\n",
    "        super(CLIPExplorerWidget, self).__init__()\n",
    "        \n",
    "        self.dataset_name = dataset_name\n",
    "        self.all_images = np.array(all_images)\n",
    "        self.all_prompts = np.array(all_prompts)\n",
    "        self.size = len(all_images)\n",
    "        self.idcs = np.arange(self.size)\n",
    "\n",
    "        # ui select widgets\n",
    "        self.model_select_widget = widgets.Dropdown(\n",
    "            description='Model: ',\n",
    "            value='CLIP',\n",
    "            options=list(model.available_CLIP_models),\n",
    "        )\n",
    "\n",
    "        self.cluster_similarity_matrix_widget = widgets.Checkbox(\n",
    "            value=False,\n",
    "            description='Cluster matrix by similarity',\n",
    "            disabled=False,\n",
    "            indent=False\n",
    "        )\n",
    "\n",
    "        # output widgets\n",
    "        self.hover_widget = HoverWidget()\n",
    "\n",
    "        image_embedding_norm, text_embedding_norm = get_embedding(self.model_select_widget.value, self.dataset_name, self.all_images, self.all_prompts)\n",
    "        self.scatter_widget = ScatterPlotWidget()\n",
    "        self.scatter_widget.embedding = np.concatenate((image_embedding_norm, text_embedding_norm))\n",
    "\n",
    "        similarity_texts_images, similarity_all = get_similarity(image_embedding_norm, text_embedding_norm)\n",
    "        self.heatmap_widget = SimilarityHeatmapWidget()\n",
    "        self.heatmap_widget.value = similarity_all\n",
    "\n",
    "        self.log_widget = widgets.Output()\n",
    "\n",
    "\n",
    "        # callback functions\n",
    "        self.model_select_widget.observe(self.model_changed, names=\"value\")\n",
    "        self.cluster_similarity_matrix_widget.observe(self.model_changed, names='value')\n",
    "        self.heatmap_widget.heatmap.on_hover(self.hover_fn)\n",
    "        self.scatter_widget.scatter_image.on_hover(self.scatter_hover_fn)\n",
    "        self.scatter_widget.scatter_text.on_hover(self.scatter_hover_fn)\n",
    "        self.scatter_widget.scatter_image.on_unhover(self.scatter_unhover_fn)\n",
    "        self.scatter_widget.scatter_text.on_unhover(self.scatter_unhover_fn)\n",
    "\n",
    "        # display everyting\n",
    "        self.header = widgets.HBox([self.model_select_widget, self.cluster_similarity_matrix_widget, self.log_widget])\n",
    "        self.header.layout.height = '40px'\n",
    "        vis_widgets = widgets.HBox([self.heatmap_widget, self.scatter_widget])\n",
    "        self.center = vis_widgets\n",
    "        self.right_sidebar = self.hover_widget\n",
    "        self.height = '700px'\n",
    "\n",
    "\n",
    "    def model_changed(self, change):\n",
    "        with self.log_widget:\n",
    "            print(\"loading...\")\n",
    "\n",
    "        image_embedding, text_embedding = get_embedding(self.model_select_widget.value, self.dataset_name, self.all_images, self.all_prompts)\n",
    "        self.scatter_widget.embedding = np.concatenate((image_embedding, text_embedding))\n",
    "\n",
    "        similarity_texts_images, similarity_all = get_similarity(image_embedding, text_embedding)\n",
    "\n",
    "        cluster_labels = []\n",
    "        cluster_sizes = []\n",
    "\n",
    "        if self.cluster_similarity_matrix_widget.value:\n",
    "            self.idcs, clusters, clusters_unsorted = get_cluster_sorting(similarity_texts_images)\n",
    "            for c in set(clusters):\n",
    "                cluster_size = np.count_nonzero(clusters==c)\n",
    "                cluster_label = get_textual_label_for_cluster(np.where(clusters_unsorted==c)[0], self.all_prompts)\n",
    "                cluster_labels.append(cluster_label)\n",
    "                cluster_sizes.append(cluster_size)\n",
    "        else:\n",
    "            self.idcs = np.arange(self.size)\n",
    "\n",
    "        # with heatmap_widget.batch_update():\n",
    "        matrix_sort_idcs = np.concatenate([self.idcs, self.idcs+self.size], axis=0) # need to do double index because we combined images and texts\n",
    "        self.heatmap_widget.value = similarity_all[matrix_sort_idcs, :][:, matrix_sort_idcs]\n",
    "        self.heatmap_widget.cluster = (cluster_labels, cluster_sizes)\n",
    "\n",
    "        self.log_widget.clear_output()\n",
    "\n",
    "\n",
    "    def hover_fn(self, trace, points, state):\n",
    "        x_idx = points.xs[0]\n",
    "        y_idx = points.ys[0]\n",
    "            \n",
    "        self.heatmap_widget.hover_idx = [(x_idx, y_idx)]\n",
    "\n",
    "        if x_idx < self.size:\n",
    "            output_img = io.BytesIO()\n",
    "            self.all_images[self.idcs][x_idx].resize((300,300)).save(output_img, format='JPEG')\n",
    "            self.hover_widget.valueX = output_img\n",
    "        else:\n",
    "            self.hover_widget.valueX = self.all_prompts[self.idcs][x_idx%self.size]\n",
    "        \n",
    "        if y_idx < self.size:\n",
    "            output_img = io.BytesIO()\n",
    "            self.all_images[self.idcs][y_idx].resize((300,300)).save(output_img, format='JPEG')\n",
    "            self.hover_widget.valueY = output_img\n",
    "        else:\n",
    "            self.hover_widget.valueY = self.all_prompts[self.idcs][y_idx%self.size]\n",
    "\n",
    "    def scatter_hover_fn(self, trace, points, state):\n",
    "        if len(points.point_inds) < 1:\n",
    "            return\n",
    "        idx = points.point_inds[0]\n",
    "        # print(trace.name, idx) # image vs text trace\n",
    "\n",
    "        self.hover_widget.valueX = self.all_prompts[idx]\n",
    "\n",
    "        output_img = io.BytesIO()\n",
    "        self.all_images[idx].resize((300,300)).save(output_img, format='JPEG')\n",
    "        self.hover_widget.valueY = output_img\n",
    "        \n",
    "        inverse_idcs = np.argsort(self.idcs)\n",
    "        heatmap_idx = inverse_idcs[idx]\n",
    "        self.heatmap_widget.hover_idx = [(heatmap_idx, self.size + heatmap_idx), (self.size + heatmap_idx, heatmap_idx)]\n",
    "\n",
    "\n",
    "    def scatter_unhover_fn(self, trace, points, state):\n",
    "        self.heatmap_widget.hover_idx = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "\n",
    "class CLIPComparerWidget(widgets.AppLayout):\n",
    "\n",
    "\n",
    "    def __init__(self, dataset_name, all_images, all_prompts, models=list(model.available_CLIP_models)):\n",
    "        super(CLIPComparerWidget, self).__init__()\n",
    "        \n",
    "        self.dataset_name = dataset_name\n",
    "        self.all_images = np.array(all_images)\n",
    "        self.all_prompts = np.array(all_prompts)\n",
    "        self.size = len(all_images)\n",
    "        \n",
    "        # output widgets\n",
    "        self.hover_widget = HoverWidget()\n",
    "\n",
    "        self.heatmap_grid = widgets.GridspecLayout(math.ceil(len(models)/2), 2)\n",
    "        for i in range(len(models)):\n",
    "            model = models[i]\n",
    "            image_embedding_norm, text_embedding_norm = get_embedding(model, self.dataset_name, self.all_images, self.all_prompts)\n",
    "            _, similarity_all = get_similarity(image_embedding_norm, text_embedding_norm)\n",
    "            heatmap_widget = SimilarityHeatmapWidget(zmin=0, zmax=1)\n",
    "            heatmap_widget.value = similarity_all\n",
    "            heatmap_widget.heatmap.on_hover(self.hover_fn)\n",
    "            # heatmap_widget.layout = widgets.Layout(height='500px', width='auto')\n",
    "\n",
    "            text_widget = widgets.HTML(value='<h2>' + models[i] + '</h2>')\n",
    "\n",
    "            self.heatmap_grid[int(i/2), i%2] = widgets.VBox([text_widget, heatmap_widget])\n",
    "            self.heatmap_grid[int(i/2), i%2].layout.height = '500px'\n",
    "\n",
    "        self.right_sidebar = self.hover_widget\n",
    "        self.center = self.heatmap_grid\n",
    "        self.height = '900px'\n",
    "\n",
    "\n",
    "    def hover_fn(self, trace, points, state):\n",
    "        x_idx = points.xs[0]\n",
    "        y_idx = points.ys[0]\n",
    "            \n",
    "        for c in self.heatmap_grid.children:\n",
    "            c.children[1].hover_idx = [(x_idx, y_idx)]\n",
    "\n",
    "        if x_idx < self.size:\n",
    "            output_img = io.BytesIO()\n",
    "            self.all_images[x_idx].resize((300,300)).save(output_img, format='JPEG')\n",
    "            self.hover_widget.valueX = output_img\n",
    "        else:\n",
    "            self.hover_widget.valueX = self.all_prompts[x_idx%self.size]\n",
    "        \n",
    "        if y_idx < self.size:\n",
    "            output_img = io.BytesIO()\n",
    "            self.all_images[y_idx].resize((300,300)).save(output_img, format='JPEG')\n",
    "            self.hover_widget.valueY = output_img\n",
    "        else:\n",
    "            self.hover_widget.valueY = self.all_prompts[y_idx%self.size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModalityGapWidget(widgets.AppLayout):\n",
    "    \n",
    "\n",
    "    def __init__(self, image_embedding, text_embedding, title='Loss Landscape CLIP'):\n",
    "        super(ModalityGapWidget, self).__init__()\n",
    "        \n",
    "        image_embedding = np.array(image_embedding)\n",
    "        text_embedding = np.array(text_embedding)\n",
    "\n",
    "        modality_gap = get_modality_gap_normed(image_embedding, text_embedding)\n",
    "        \n",
    "        distance_lst = []\n",
    "        loss_lst = []\n",
    "        for delta in np.arange(-5.0, 5.0, 0.25): \n",
    "            modified_text_features = l2_norm(text_embedding) + 0.5 * delta * modality_gap\n",
    "            modified_text_features = l2_norm(modified_text_features)\n",
    "\n",
    "            modified_image_features = l2_norm(image_embedding) - 0.5 * delta * modality_gap\n",
    "            modified_image_features = l2_norm(modified_image_features)\n",
    "\n",
    "            avg_val_loss = calculate_val_loss(torch.from_numpy(modified_image_features), torch.from_numpy(modified_text_features), logit_scale = 100.0)\n",
    "\n",
    "            pca = PCA(n_components=6)\n",
    "            pca.fit(np.concatenate((image_embedding, text_embedding), axis=0))\n",
    "\n",
    "            gap_direction = get_gap_direction(modified_image_features, modified_text_features, pca)\n",
    "\n",
    "            loss_lst.append(avg_val_loss)\n",
    "\n",
    "            # Euclidean distance between mass centers\n",
    "            distance_lst.append(\n",
    "                get_modality_distance(modified_image_features, modified_text_features) * gap_direction\n",
    "            )\n",
    "\n",
    "\n",
    "        orig_distance = get_modality_distance(image_embedding, text_embedding)\n",
    "\n",
    "        fig = go.FigureWidget(data=go.Scatter(x=distance_lst, y=loss_lst, mode='lines+markers', hovertemplate='Distance: %{x} <br>Loss: %{y}'))\n",
    "        fig.add_shape(type=\"line\",\n",
    "            x0=orig_distance, y0=0, x1=orig_distance, y1=max(loss_lst)*1.2,\n",
    "            line=dict(\n",
    "                color=\"Black\",\n",
    "                width=1,\n",
    "                dash=\"dash\",\n",
    "            )\n",
    "        )\n",
    "        fig.update_layout(xaxis_title='Euclidean Distance', yaxis_title='Loss', width=500, title=title)\n",
    "        \n",
    "        self.center = widgets.HBox([fig])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.data import get_dataset\n",
    "# dataset = get_dataset('MSCOCO', '../../../../../Data/mscoco/')\n",
    "# import src.data as data\n",
    "from amumo import data\n",
    "# dataset = data.MSCOCO_Dataset(path='../../../../../Data/mscoco/')\n",
    "dataset = data.DiffusionDB_Dataset(path=\"2m_first_1k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images, all_prompts = dataset.get_data()\n",
    "# dataset = data.RandomAugmentation_Dataset(all_images[0], all_prompts[0])\n",
    "# dataset = data.Rotate_Dataset(all_images[0], all_prompts[0])\n",
    "# dataset = data.Noise_Dataset(all_images[0], all_prompts[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_helper(dataset, filters=[], method=any):\n",
    "    all_images, all_prompts = dataset.get_filtered_data(filters, method=method)\n",
    "    print(len(all_images))\n",
    "\n",
    "    dataset_name = dataset.name\n",
    "    if len(filters) > 0:\n",
    "        dataset_name = dataset_name + '_filter-' + method.__name__ + '_' + '-'.join(filters)\n",
    "    else:\n",
    "        dataset_name = dataset_name + '_size-%i'%len(all_images)\n",
    "\n",
    "    return all_images, all_prompts, dataset_name\n",
    "\n",
    "all_images, all_prompts, dataset_name = get_data_helper(dataset, filters=[], method=any) # filters = [\"dog\"], method=all\n",
    "dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embedding, text_embedding = get_embedding('CLIP', dataset_name, all_images, all_prompts)\n",
    "ModalityGapWidget(image_embedding, text_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clipexplorer = CLIPExplorerWidget(dataset_name, all_images, all_prompts)\n",
    "clipexplorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.available_CLIP_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_comparer = CLIPComparerWidget(dataset_name, all_images, all_prompts, models=['CLIP', 'CyCLIP', 'CLOOB', 'CLOOB_LAION400M'])\n",
    "clip_comparer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do some Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images, all_prompts = dataset.get_data()\n",
    "len(all_images), len(all_prompts)\n",
    "# 3.2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embedding, text_embedding = get_embedding('CLIP', dataset.name, all_images, all_prompts)\n",
    "# 12.2s\n",
    "# 3.1s if cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_texts_images, similarity_all = get_similarity(image_embedding, text_embedding)\n",
    "# 0.0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_,_ = get_cluster_sorting(similarity_texts_images)\n",
    "# 0.0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = get_textual_label_for_cluster([1,2,3,4,5,6,7,8,9,10], all_prompts)\n",
    "# 0.0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_helper(dataset, filters=[], method=any):\n",
    "    all_images, all_prompts = dataset.get_filtered_data(filters, method=method)\n",
    "    print(len(all_images))\n",
    "\n",
    "    dataset_name = dataset.name\n",
    "    if len(filters) > 0:\n",
    "        dataset_name = dataset_name + '_filter-' + method.__name__ + '_' + '-'.join(filters)\n",
    "    else:\n",
    "        dataset_name = dataset_name + '_size-%i'%len(all_images)\n",
    "\n",
    "    return all_images, all_prompts, dataset_name\n",
    "\n",
    "all_images, all_prompts, dataset_name = get_data_helper(dataset, filters=[], method=any) # filters = [\"dog\"], method=all\n",
    "dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embedding, text_embedding = get_embedding('CLIP', dataset_name, all_images, all_prompts)\n",
    "similarity_texts_images, similarity_all = get_similarity(image_embedding, text_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkage = sch.linkage(1-similarity_texts_images, method='complete')\n",
    "cluster_distance_threshold = (1-similarity_texts_images).max()/2\n",
    "idx_to_cluster_array = sch.fcluster(linkage, 10, criterion='maxclust')\n",
    "idx = np.argsort(idx_to_cluster_array)\n",
    "plt.matshow(similarity_texts_images[idx, :][:, idx])\n",
    "idx_to_cluster_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sch.dendrogram(linkage)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inconsistencies = sch.inconsistent(linkage, 100)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(inconsistencies) + 1), inconsistencies[:, 2])\n",
    "plt.xlabel('Number of merges')\n",
    "plt.ylabel('Inconsistency')\n",
    "plt.title('Inconsistency Plot')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inconsistencies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_derivative = np.gradient(np.gradient(inconsistencies[:, 2]))\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(second_derivative) + 1), second_derivative)\n",
    "plt.xlabel('Number of merges')\n",
    "plt.ylabel('Second Derivative')\n",
    "plt.title('Elbow Plot')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN()\n",
    "cluster_labels = clusterer.fit_predict(text_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(similarity_texts_images[np.argsort(cluster_labels), :][:, np.argsort(cluster_labels)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openTSNE import TSNE\n",
    "from umap import UMAP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = data.DiffusionDB_Dataset(path=\"2m_first_1k\", batch_size=100)\n",
    "dataset = data.MSCOCO_Val_Dataset(path='../../../../../Data/mscoco/validation/', batch_size=1000)\n",
    "all_images, all_prompts, dataset_name = get_data_helper(dataset, filters=[], method=any) # filters = [\"dog\"], method=all\n",
    "dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embedding, text_embedding = get_embedding('CLIP', dataset_name, all_images, all_prompts)\n",
    "\n",
    "clip_embeddings = torch.cat([image_embedding, text_embedding], dim=0)\n",
    "clip_embeddings.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_proj = UMAP(\n",
    "    random_state=31415,\n",
    "    metric='cosine'\n",
    ")\n",
    "pre_emb = umap_proj.fit_transform(image_embedding)\n",
    "oos_emb = umap_proj.transform(text_embedding)\n",
    "plt.scatter(pre_emb[:,0], pre_emb[:,1], s=2)\n",
    "plt.scatter(oos_emb[:,0], oos_emb[:,1], s=2)\n",
    "\n",
    "for line_idx in range(len(pre_emb)):\n",
    "    plt.plot((pre_emb[line_idx,0], oos_emb[line_idx,0]), (pre_emb[line_idx,1], oos_emb[line_idx,1]), 'black', linestyle='-', marker='', linewidth=1, alpha=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate pairwise Euclidean distance\n",
    "distances = cdist(clip_embeddings, clip_embeddings, metric='cosine')\n",
    "\n",
    "umap_proj = UMAP(\n",
    "    metric=\"precomputed\",\n",
    "    random_state=31415\n",
    ")\n",
    "pre_emb = umap_proj.fit_transform(distances)\n",
    "plt.scatter(pre_emb[:,0], pre_emb[:,1], c=[0]*100 + [1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate pairwise Euclidean distance\n",
    "distances = cdist(image_embedding, image_embedding, metric='cosine')\n",
    "\n",
    "umap_proj = UMAP(\n",
    "    metric=\"precomputed\",\n",
    "    random_state=31415\n",
    ")\n",
    "cross_emb = umap_proj.fit_transform(distances)\n",
    "plt.scatter(cross_emb[:,0], cross_emb[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = cdist(image_embedding, text_embedding, metric='cosine')\n",
    "inmodal_text_emb = umap_proj.transform(distances)\n",
    "plt.scatter(inmodal_text_emb[:,0], inmodal_text_emb[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = cdist(image_embedding, image_embedding, metric='cosine')\n",
    "inmodal_image_emb = umap_proj.transform(distances)\n",
    "plt.scatter(inmodal_image_emb[:,0], inmodal_image_emb[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.scatter(inmodal_text_emb[:,0], inmodal_text_emb[:,1])\n",
    "plt.scatter(inmodal_image_emb[:,0], inmodal_image_emb[:,1])\n",
    "\n",
    "for line_idx in range(len(inmodal_image_emb)):\n",
    "    plt.plot((inmodal_text_emb[line_idx,0], inmodal_image_emb[line_idx,0]), (inmodal_text_emb[line_idx,1], inmodal_image_emb[line_idx,1]), 'black', linestyle='-', marker='', linewidth=1, alpha=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate pairwise Euclidean distance\n",
    "distances = cdist(image_embedding, text_embedding, metric='euclidean')\n",
    "\n",
    "\n",
    "tsne = TSNE(\n",
    "    perplexity=30,\n",
    "    metric=\"precomputed\",\n",
    "    n_jobs=8,\n",
    "    verbose=True,\n",
    "    random_state=31415\n",
    ")\n",
    "pre_emb = tsne.fit(distances)\n",
    "plt.scatter(pre_emb[:,0], pre_emb[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate pairwise Euclidean distance\n",
    "distances = cdist(clip_embeddings, clip_embeddings, metric='euclidean')\n",
    "\n",
    "\n",
    "tsne = TSNE(\n",
    "    perplexity=30,\n",
    "    metric=\"precomputed\",\n",
    "    n_jobs=8,\n",
    "    verbose=True,\n",
    "    random_state=31415\n",
    ")\n",
    "pre_emb = tsne.fit(distances)\n",
    "plt.scatter(pre_emb[:,0], pre_emb[:,1], c=[0]*100 + [1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tsne = TSNE(\n",
    "    perplexity=30,\n",
    "    metric=\"euclidean\",\n",
    "    n_jobs=8,\n",
    "    verbose=True,\n",
    "    random_state=31415\n",
    ")\n",
    "emb = tsne.fit(clip_embeddings)\n",
    "\n",
    "plt.scatter(emb[:,0], emb[:,1], c=[0]*100 + [1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = UMAP(random_state=31415)\n",
    "image_embedding, text_embedding = get_embedding('CLIP', dataset_name, all_images, all_prompts)\n",
    "proj.fit_transform(image_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = UMAP(random_state=31415)\n",
    "image_embedding, text_embedding = get_embedding('CLIP', dataset_name, all_images, all_prompts)\n",
    "proj.fit_transform(image_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "# ds = load_dataset('poloclub/diffusiondb', '2m_text_only')\n",
    "ds = load_dataset('poloclub/diffusiondb', '2m_first_10k')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = ds['train']['prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_ids = [i for i in range(len(prompts)) if 'kitchen' in prompts[i]]\n",
    "len(subset_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts[subset_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import webdataset as wds\n",
    "url = \"file:../../../../../Data/mscoco/bench/{00000..00591}.tar\" # http://storage.googleapis.com/nvdata-openimages/openimages-train-000000.tar\"\n",
    "        \n",
    "dataset = wds.WebDataset(url).shuffle(10000).decode(\"pil\").rename(text=\"txt\", json=\"json\").to_tuple(\"text\", \"json\").batched(10000)#.map_dict(image=preprocess, text=lambda text: clip.tokenize(text, truncate=True)[0], json=lambda json: json).to_tuple(\"image\", \"text\", \"json\")\n",
    "# dataloader = torch.utils.data.DataLoader(dataset, num_workers=4, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_filter(data):\n",
    "    txt, _ = data\n",
    "    return 'kitchen' in txt.lower()\n",
    "\n",
    "dataset = wds.WebDataset(url).decode(\"pil\").rename(text=\"txt\", json=\"json\").to_tuple(\"text\", \"json\").select(my_filter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from itertools import islice\n",
    "list(islice(dataset, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webdataset as wds\n",
    "url = \"file:../../../../../Data/mscoco/bench/{00000..00591}.tar\" # http://storage.googleapis.com/nvdata-openimages/openimages-train-000000.tar\"\n",
    "      \n",
    "# dataset = wds.WebDataset(url).decode(\"pil\").rename(text=\"txt\", json=\"json\").to_tuple(\"text\", \"json\")#.map_dict(image=preprocess, text=lambda text: clip.tokenize(text, truncate=True)[0], json=lambda json: json).to_tuple(\"image\", \"text\", \"json\")\n",
    "import torch\n",
    "def my_filter(data):\n",
    "    txt, _ = data\n",
    "    return 'young girl' in txt.lower()\n",
    "    # return True\n",
    "dataset = wds.WebDataset(url).decode(\"pil\").rename(text=\"txt\", json=\"json\").to_tuple(\"text\", \"json\").select(my_filter).shuffle(100).batched(10)#.map_dict(image=preprocess, text=lambda text: clip.tokenize(text, truncate=True)[0], json=lambda json: json).to_tuple(\"image\", \"text\", \"json\")\n",
    "# dataloader = torch.utils.data.DataLoader(dataset, num_workers=4, batch_size=None)\n",
    "# all_images, all_prompts = next(iter(dataloader))\n",
    "# all_images, all_prompts\n",
    "# for data in dataset:\n",
    "#     print(data)\n",
    "\n",
    "\n",
    "from itertools import islice\n",
    "# list(islice(dataset, 10000))\n",
    "# next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images, all_prompts = dataset.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define the transformation pipeline\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(degrees=90),\n",
    "    # transforms.RandomCrop(size=(224, 224)),\n",
    "    # transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_level = 1/10\n",
    "image_array = np.array(all_images[0])\n",
    "images = []\n",
    "for i in range(10):\n",
    "    noise = np.random.randint(low=0, high=256, size=image_array.shape,dtype='uint8')\n",
    "    noisy_image_array = np.clip((1-i*noise_level)*image_array + i*noise_level*noise, 0, 255).astype('uint8')\n",
    "    noisy_image = Image.fromarray(noisy_image_array)\n",
    "    display(noisy_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_expanded_button(description, button_style):\n",
    "    return widgets.Button(description=description, button_style=button_style, layout=widgets.Layout(height='auto', width='auto'))\n",
    "\n",
    "\n",
    "grid = widgets.GridspecLayout(4, 2)\n",
    "grid[0,0] = create_expanded_button('One', 'success')\n",
    "grid[0,1] = create_expanded_button('Two', 'info')\n",
    "grid[1,0] = create_expanded_button('Three', 'warning')\n",
    "grid[1,1] = create_expanded_button('Three', 'warning')\n",
    "\n",
    "grid[0,0].layout.height = '500px'\n",
    "grid[0,1].layout.height = '500px'\n",
    "grid[1,1].layout.height = '500px'\n",
    "\n",
    "widgets.AppLayout(header=widgets.Button(description='right', button_style='info', layout=widgets.Layout(height='20px', width='auto')),\n",
    "          left_sidebar=None,\n",
    "          center=grid,\n",
    "          right_sidebar=widgets.Button(description='right', button_style='info', layout=widgets.Layout(height='500px', width='auto')),\n",
    "          footer=None,\n",
    "          height='500px')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
