{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install Amumo\n",
    "\n",
    "# need ipykernel > 6\n",
    "# ! pip install ipykernel==6.23.1\n",
    "# ! pip install \"amumo[clip,cloob,diffusion-db] @ git+https://github.com/ginihumer/Amumo.git\"\n",
    "# for local installation\n",
    "# ! pip install -e ..[clip,cloob,diffusion-db]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import \n",
    "from amumo import model as am_model\n",
    "from amumo import data as am_data\n",
    "from amumo import widgets as am_widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "dataset = am_data.DiffusionDB_Dataset(path=\"2m_first_1k\", batch_size=100) # data helper for the diffusionDB dataset; for the interactive prototype, we only use a random subset of 100 samples\n",
    "all_images, all_prompts = dataset.get_data()\n",
    "cache_name = 'diffusiondb_random_100' # path used to cache the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This widget consists of a similarity heatmap that shows the similarities between all latent embeddings, a 2-dimensional projection of the embeddings, and a hover view of the currently hovered instance.\n",
    "# It also allows user input for choosing the model, an option to manually close the modality gap, an option to cluster the matrix rows by their similarity, and settings for the 2-dimensional projection. \n",
    "# To learn more about the proper use for closing the modality gap, see the interactive article: https://jku-vds-lab.at/amumo.\n",
    "# You can specify a list of models to compare; there is a predefined set of models, but you can also pass your own model class.\n",
    "# The widget takes a list of images and a list of prompts of the same size as input. Images and texts with the same index are considered pairs.\n",
    "am_widgets.CLIPExplorerWidget(cache_name, all_data={\"image\": all_images, \"text\": all_prompts}, models=['CLIP', am_model.CyCLIPModel(), am_model.CLOOB_LAION400M_Model()]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This widget shows similarity heatmaps for all models in the list for a better comparison between models.\n",
    "am_widgets.CLIPComparerWidget(cache_name, all_images, all_prompts, models=['CLIP', am_model.CyCLIPModel(), am_model.CLOOB_LAION400M_Model()]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use a Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "import numpy as np\n",
    "# You can use the CLIPModelInterface to create your own model class wrapper.\n",
    "class CustomModel(am_model.CLIPModelInterface):\n",
    "    available_models = clip.available_models()\n",
    "    model_name = 'MyCLIP'\n",
    "\n",
    "    def __init__(self, name='RN50', device='cpu') -> None:\n",
    "        super().__init__(name, device)\n",
    "        self.model, self.preprocess = clip.load(name, device=device)\n",
    "        self.model.eval()\n",
    "        self.logit_scale = self.model.logit_scale\n",
    "\n",
    "    def encode_image(self, images):\n",
    "        images = [self.preprocess(i) for i in images]\n",
    "        image_input = torch.tensor(np.stack(images)).to(self.device)\n",
    "        return self.model.encode_image(image_input).float().cpu()\n",
    "\n",
    "    def encode_text(self, texts):\n",
    "        text_tokens = clip.tokenize(texts, truncate = True).to(self.device)\n",
    "        return self.model.encode_text(text_tokens).float().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The widget takes any instance of type CLIPModelInterface.\n",
    "am_widgets.CLIPExplorerWidget(cache_name, all_data={\"image\": all_images, \"text\": all_prompts}, models=[CustomModel()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
