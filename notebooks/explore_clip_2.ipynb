{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need ipykernel > 6\n",
    "# ! pip install ipykernel==6.23.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import amumo\n",
    "from importlib import reload\n",
    "# reload(amumo.widgets)\n",
    "# reload(amumo.model)\n",
    "# reload(amumo.data)\n",
    "# reload(amumo.utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amumo import model\n",
    "from amumo import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = data.MSCOCO_Dataset(path='../../../../../Data/mscoco/')\n",
    "# dataset = data.DiffusionDB_Dataset(path=\"2m_first_1k\")\n",
    "dataset = data.MSCOCO_Val_Dataset(path='../../../../../Data/mscoco/validation/')\n",
    "\n",
    "# all_images, all_prompts = dataset.get_data()\n",
    "# dataset = data.RandomAugmentation_Dataset(all_images[0], all_prompts[0])\n",
    "# dataset = data.Rotate_Dataset(all_images[0], all_prompts[0])\n",
    "# dataset = data.Noise_Dataset(all_images[0], all_prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_helper(dataset, filters=[], method=any):\n",
    "    all_images, all_prompts = dataset.get_filtered_data(filters, method=method)\n",
    "    print(len(all_images))\n",
    "\n",
    "    dataset_name = dataset.name\n",
    "    if len(filters) > 0:\n",
    "        dataset_name = dataset_name + '_filter-' + method.__name__ + '_' + '-'.join(filters)\n",
    "    else:\n",
    "        dataset_name = dataset_name + '_size-%i'%len(all_images)\n",
    "\n",
    "    return all_images, all_prompts, dataset_name\n",
    "\n",
    "all_images, all_prompts, dataset_name = get_data_helper(dataset, filters=[], method=any) # filters = [\"dog\"], method=all\n",
    "dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amumo.widgets import CLIPExplorerWidget\n",
    "\n",
    "\n",
    "clipexplorer_widget = CLIPExplorerWidget(dataset_name, all_images, all_prompts)\n",
    "clipexplorer_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clipexplorer_widget.scatter_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amumo import model\n",
    "model.available_CLIP_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amumo.widgets import CLIPComparerWidget\n",
    "clip_comparer = CLIPComparerWidget(dataset_name, all_images, all_prompts, models=['CLIP', 'CyCLIP', 'CLOOB', 'CLOOB_LAION400M'])\n",
    "clip_comparer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amumo.widgets import CLIPComparerWidget\n",
    "clip_comparer = CLIPComparerWidget(dataset_name, all_images, all_prompts, models=['CLIP', 'CyCLIP', 'CLOOB', 'CLOOB_LAION400M'], close_modality_gap=True)\n",
    "clip_comparer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot similarities and gap-closed similarities next to each other\n",
    "from amumo.widgets import CLIPComparerWidget\n",
    "clip_comparer = CLIPComparerWidget(dataset_name, all_images, all_prompts, models=['CLIP', 'CLIP', 'CyCLIP', 'CyCLIP', 'CLOOB', 'CLOOB', 'CLOOB_LAION400M', 'CLOOB_LAION400M'], close_modality_gap=[False, True]*4)\n",
    "clip_comparer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate metrics for larger amounts of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amumo import data\n",
    "from amumo.utils import get_embedding, get_modality_distance, calculate_val_loss, get_closed_modality_gap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = data.MSCOCO_Dataset(path='../../../../../Data/mscoco/')\n",
    "# dataset = data.DiffusionDB_Dataset(path=\"2m_first_10k\", batch_size=None)\n",
    "\n",
    "all_images, all_prompts, dataset_name = get_data_helper(dataset, filters=[], method=any) # filters = [\"dog\"], method=all\n",
    "dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clip_model in ['CLIP', 'OpenCLIP', 'CyCLIP', 'CLOOB', 'CLOOB_LAION400M']:\n",
    "    print('---model: %s---'%clip_model)\n",
    "    image_embedding, text_embedding, logit_scaling = get_embedding(clip_model, dataset_name, all_images, all_prompts)\n",
    "    modality_distance = get_modality_distance(image_embedding, text_embedding)\n",
    "    val_loss = calculate_val_loss(image_embedding, text_embedding, logit_scaling.exp())\n",
    "    print('modality distance: %.2f | validation loss: %.2f'%(modality_distance, val_loss))\n",
    "\n",
    "    image_embedding, text_embedding = get_closed_modality_gap(image_embedding, text_embedding)\n",
    "    modified_modality_distance = get_modality_distance(image_embedding, text_embedding)\n",
    "    modified_val_loss = calculate_val_loss(image_embedding, text_embedding, logit_scaling.exp())\n",
    "    print('modified modality distance: %.2f | modified validation loss: %.2f'%(modified_modality_distance, modified_val_loss))\n",
    "\n",
    "    print('loss difference:', modified_val_loss-val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.MSCOCO_Dataset(path='../../../../../Data/mscoco/', batch_size=10000)\n",
    "# dataset = data.DiffusionDB_Dataset(path=\"2m_first_10k\", batch_size=None)\n",
    "all_images, all_prompts, dataset_name = get_data_helper(dataset, filters=[], method=any) # filters = [\"dog\"], method=all\n",
    "dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clip_model in ['CLIP', 'OpenCLIP', 'CyCLIP', 'CLOOB', 'CLOOB_LAION400M']:\n",
    "    print('---model: %s---'%clip_model)\n",
    "    image_embedding, text_embedding, logit_scaling = get_embedding(clip_model, dataset_name, all_images, all_prompts)\n",
    "    modality_distance = get_modality_distance(image_embedding, text_embedding)\n",
    "    val_loss = calculate_val_loss(image_embedding, text_embedding, logit_scaling.exp())\n",
    "    print('modality distance: %.2f | validation loss: %.2f'%(modality_distance, val_loss))\n",
    "\n",
    "    image_embedding, text_embedding = get_closed_modality_gap(image_embedding, text_embedding)\n",
    "    modified_modality_distance = get_modality_distance(image_embedding, text_embedding)\n",
    "    modified_val_loss = calculate_val_loss(image_embedding, text_embedding, logit_scaling.exp())\n",
    "    print('modified modality distance: %.2f | modified validation loss: %.2f'%(modified_modality_distance, modified_val_loss))\n",
    "\n",
    "    print('loss difference:', modified_val_loss-val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.MSCOCO_Val_Dataset(path='../../../../../Data/mscoco/validation/', batch_size=5000)\n",
    "all_images, all_prompts, dataset_name = get_data_helper(dataset, filters=[], method=any) # filters = [\"dog\"], method=all\n",
    "dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clip_model in ['CLIP', 'OpenCLIP', 'CyCLIP', 'CLOOB', 'CLOOB_LAION400M']:\n",
    "    print('---model: %s---'%clip_model)\n",
    "    image_embedding, text_embedding, logit_scaling = get_embedding(clip_model, dataset_name, all_images, all_prompts)\n",
    "    modality_distance = get_modality_distance(image_embedding, text_embedding)\n",
    "    val_loss = calculate_val_loss(image_embedding, text_embedding, logit_scaling.exp())\n",
    "    print('modality distance: %.2f | validation loss: %.2f'%(modality_distance, val_loss))\n",
    "\n",
    "    image_embedding, text_embedding = get_closed_modality_gap(image_embedding, text_embedding)\n",
    "    modified_modality_distance = get_modality_distance(image_embedding, text_embedding)\n",
    "    modified_val_loss = calculate_val_loss(image_embedding, text_embedding, logit_scaling.exp())\n",
    "    print('modified modality distance: %.2f | modified validation loss: %.2f'%(modified_modality_distance, modified_val_loss))\n",
    "\n",
    "    print('loss difference:', modified_val_loss-val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \n",
    "raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n",
    "\n",
    "# conditional image captioning\n",
    "text = \"a photography of\"\n",
    "inputs = processor(raw_image, text, return_tensors=\"pt\")\n",
    "\n",
    "out = model.generate(**inputs)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))\n",
    "\n",
    "# unconditional image captioning\n",
    "inputs = processor(raw_image, return_tensors=\"pt\")\n",
    "\n",
    "out = model.generate(**inputs)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[raw_image, raw_image]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.MSCOCO_Val_Dataset(path='../../../../../Data/mscoco/validation/', batch_size=2000)\n",
    "all_images, all_prompts = dataset.get_data()\n",
    "\n",
    "weights = models.VGG16_Weights.DEFAULT\n",
    "preprocess = weights.transforms()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.vgg16(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "activations = {}\n",
    "def get_activations(name):\n",
    "    def hook_fn(module, input, output):\n",
    "        if name not in activations.keys():\n",
    "            activations[name] = output.detach()\n",
    "        else:\n",
    "            activations[name] = torch.cat([activations[name], output.detach()], dim=0)\n",
    "    return hook_fn\n",
    "\n",
    "\n",
    "# for module_name, module in model.named_modules():\n",
    "for module_name, module in model.features[20:].named_modules():\n",
    "    if isinstance(module, torch.nn.Conv2d):\n",
    "        module.register_forward_hook(get_activations(module_name))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "import numpy as np\n",
    "\n",
    "with torch.no_grad():\n",
    "    batch_result = []\n",
    "\n",
    "    for i in range(int(len(all_images)/batch_size)):\n",
    "        print(\"batch\", i+1, \"of\", int(len(all_images)/batch_size))\n",
    "        batch_images = all_images[i*batch_size:(i+1)*batch_size]\n",
    "        imgs_transformed = [preprocess(img) for img in batch_images]\n",
    "        output = model(torch.stack(imgs_transformed))\n",
    "        batch_result.append(np.argmax(output.detach(), axis=-1))\n",
    "        \n",
    "    results = np.concatenate(batch_result, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations['21'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "low_dim_map = {}\n",
    "for layer_name in activations.keys():\n",
    "    dr = PCA(1)\n",
    "    low_dim_map[layer_name] = dr.fit_transform(activations[layer_name].reshape(100,-1))[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(low_dim_map)\n",
    "df['pred_class'] = np.argmax(output.detach(), axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.parallel_coordinates(df, color=\"pred_class\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "low_dim_map_umap = {}\n",
    "for layer_name in activations.keys():\n",
    "    dr = UMAP(n_components=1, random_state=31415)\n",
    "    low_dim_map_umap[layer_name] = dr.fit_transform(activations[layer_name].reshape(100,-1))[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_umap = pd.DataFrame(low_dim_map_umap)\n",
    "df_umap['pred_class'] = np.argmax(output.detach(), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.parallel_coordinates(df_umap, color=\"pred_class\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BlipImageProcessor, BlipModel, BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BlipModel.from_pretrained(\"Salesforce/blip-image-captioning-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = BlipImageProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('Salesforce/blip-image-captioning-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = processor(raw_image, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "processor(list(np.array([raw_image, raw_image])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input['pixel_values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(['a photography of a woman and her dog on the beach', \"and her dog on the beach\"], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_image_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_image_features(**input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_text_features(**tokens).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import AutoProcessor, BlipModel\n",
    "\n",
    "model = BlipModel.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "inputs = processor(\n",
    "    text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True\n",
    ")\n",
    "\n",
    "outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.image_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.text_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel = model.get_model('CLIP', 'ViT-B/16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel.model.visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordereddict = mymodel.model.visual.state_dict()\n",
    "ordereddict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordereddict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Your input text goes here.\"\n",
    "tokens = tokenizer.encode_plus(text, add_special_tokens=True, return_tensors='pt')\n",
    "input_ids = tokens['input_ids']\n",
    "attention_mask = tokens['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(input_ids, attention_mask=attention_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_layers = outputs.attentions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {}\n",
    "def get_features(name):\n",
    "    def hook(model, input, output):\n",
    "        features[name] = output.detach()\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_index = 1  # Index of the desired layer\n",
    "head_index = 0  # Index of the desired attention head\n",
    "\n",
    "layer_activations = attention_layers[layer_index]  # Activations for the entire layer\n",
    "head_activations = layer_activations[:, head_index]  # Activations for the attention head\n",
    "keys = head_activations[0]  # Keys activations for the first token\n",
    "values = head_activations[1]  # Values activations for the second token\n",
    "\n",
    "print(keys)\n",
    "print(values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
