<!--
  Copyright 2018 The Distill Template Authors

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!DOCTYPE html>

<head>
    <meta charset="utf8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <script src="scripts/template.v1.js"></script>
    <script src="scripts/d3.v7.min.js"></script>
    <script src="scripts/plotly-2.24.1.min.js" charset="utf-8"></script>
    <link rel="stylesheet" href="styles/katex.min.css">
    <script defer src="scripts/katex.min.js"></script>
    <link rel="icon" type="image/x-icon" href="assets/favicon.ico">

    <style>
        table{
            text-align: left;
        }
    </style>
</head>

<body>

    <script>
        var interpolate = d3.scaleSequential(d3.interpolatePlasma);
        var plasma_colors = [];
        dark = d3.lab(interpolate(0)).l < 50;
        for (let i = 0; i < 100; ++i) {
            plasma_colors.push([i/99, d3.rgb(interpolate(i / (99))).hex()]);
        }

        // load data
        const mscoco_val_prompts_promise = fetch('./exported_data_checkpoints/MSCOCO-Val_size-100/prompts.txt')
            .then(response => response.text())
            .then(data => data.split('\n'))
            .catch(error => console.error(error));

        const mscoco_val_projections_promise = d3.csv("./exported_data_checkpoints/MSCOCO-Val_size-100/projections.csv");
        

        const clip_loss_landscape_promise = fetch('./exported_data_checkpoints/MSCOCO-Val_size-5000/CLIP_loss_landscape.json')
                .then(response => response.json())
                .catch(error => console.error(error));

        const cyclip_loss_landscape_promise = fetch('./exported_data_checkpoints/MSCOCO-Val_size-5000/CyCLIP_loss_landscape.json')
                .then(response => response.json())
                .catch(error => console.error(error));

        const load_similarities_fn = function(dataset, name){
            return fetch('./exported_data_checkpoints/' + dataset + '/similarities/' + name + '.csv')
                .then(response => response.text())
                .then(csvData => {
                    const rows = csvData.split('\n');
                    const data = rows.filter(row => row.length > 1).map(row => row.split(','));
                    return data;
                })
                .catch(error => console.error(error));
        }

        const load_meta_info_fn = function(dataset, name){
            return fetch('./exported_data_checkpoints/' + dataset + '/similarities/' + name + '_meta_info.json')
                .then(response => response.json())
                .catch(error => console.error(error));
        }
            
    </script>

    <script>
        // have to load it again because distill template somehow overrides plotly css
        window.onload = () => {
            var tag = document.createElement("script");
            tag.src = "./scripts/plotly-2.24.1.min.js";
            document.getElementsByTagName("head")[0].appendChild(tag);

        }

        // define widgets
        class ModalityGapWidget{
            constructor(title, data, width=500, height=400){
                this.div = document.createElement('div');
                var trace1 = {
                    name: '',
                    x: data.distances,
                    y: data.losses,
                    type: 'scatter',
                    mode: 'lines+markers',
                    hovertemplate: 'Distance: %{x} <br>Loss: %{y}'
                };

                var layout = {
                    xaxis: {
                        title: 'Euclidean Distance',
                        zeroline: false,
                    },
                    yaxis: {
                        title: 'Loss',
                    },
                    width: width,
                    height: height,
                    title: title,
                    shapes: [
                        {
                            type: 'line',
                            x0: data.original_distance,
                            x1: data.original_distance,
                            y0: 0,
                            y1: Math.max(...data.losses)*1.2,
                            line: {
                                width: 1,
                                dash: 'dash'
                            }
                        }
                    ]
                }

                Plotly.newPlot(this.div, [trace1], layout);
            }
        }

        class HoverWidget{
            constructor(dataset_name, text_list, width=400, height=200){
                this.dataset_name = dataset_name;
                this.text_list = text_list;
                this.width = width;
                this.height = height;

                this.div1 = document.createElement('div');
                this.div1.style.wordWrap = 'break-word';
                this.div1.style.minWidth = '200px';
                this.div1.style.marginBottom = '20px';
                this.div2 = document.createElement('div');
                this.div2.style.wordWrap = 'break-word';
                this.div2.style.minWidth = '200px';

                this.div = document.createElement('div');
                // this.div.style.width = width + 'px';
                // this.div.style.height = height + 'px';
                this.div.appendChild(this.div1);
                this.div.appendChild(this.div2);
            }

            update1(type, idx){
                this.update(this.div1, type, idx);
            }
            update2(type, idx){
                this.update(this.div2, type, idx);
            }

            update(div, type, idx){
                if(type == 'image'){
                    const img = document.createElement('img');
                    img.src = './exported_data_checkpoints/'+this.dataset_name+'/images/'+idx+'.jpg';
                    // img.width = this.width;
                    // img.height = this.height;
                    img.style.maxWidth = this.width + 'px';
                    img.style.maxHeight = this.height + 'px';
                    div.innerHTML = '';
                    div.appendChild(img);
                }else if(type == 'text'){
                    div.innerHTML = this.text_list[idx];
                }else{
                    div.innerHTML = '';
                }
            }
        }


        class ScatterPlotWidget{
            projection_methods = ['PCA', 'TSNE', 'UMAP'];

            constructor(projection_df, model_name, selected_method='PCA', title='', width=400, height=300){
                let margin_top = 30;
                if(title == ''){
                    margin_top = 10;
                }

                this.projection_df = projection_df;
                this.model_name = model_name;
                this.width = width;
                this.height = height;

                // projection method selection
                this.method_select = document.createElement('select');
                for (const key in this.projection_methods) {
                    var opt = new Option(this.projection_methods[key], this.projection_methods[key])
                    this.method_select.appendChild(opt);
                }
                console.log(this.projection_methods.findIndex((value)=>value==selected_method))
                this.method_select.selectedIndex = this.projection_methods.findIndex((value)=>value==selected_method);
                this.method_select.onchange = () => {
                    this.update_scatter();
                }
                

                // scatter plot
                this.scatter_div = document.createElement('div');
                var trace1 = {
                    name: 'image',
                    x: Array(100).fill(0),
                    y: Array(100).fill(0),
                    type: 'scatter',
                    mode: 'markers',
                    hoverinfo: 'text',
                    marker: {
                        color: 'blue'
                    }
                };

                var trace2 = {
                    name: 'text',
                    x: Array(100).fill(0),
                    y: Array(100).fill(0),
                    type: 'scatter',
                    mode: 'markers',
                    hoverinfo: 'text',
                    marker: {
                        color: 'orange'
                    }
                };

                var trace_highlight = {
                    name: 'hover',
                    x: [],
                    y: [],
                    type: 'scatter',
                    mode: 'markers',
                    hoverinfo: 'text',
                    marker: {
                        size: 10,
                        color: 'black'
                    }
                };
                var traces = [trace1, trace2, trace_highlight];
                
                const layout = {
                    width: this.width, 
                    height: this.height, 
                    margin: {l: 10, r: 10, t: margin_top, b: 10},
                    legend: {
                        yanchor: 'top',
                        y: 0.99,
                        xanchor: 'left',
                        x: 0.01
                    },
                    xaxis: {
                        tickmode: 'none',
                        showticklabels: false,
                        // showgrid: false,
                        zeroline: false,
                    },
                    yaxis: {
                        tickmode: 'none',
                        showticklabels: false,
                        // showgrid: false,
                        zeroline: false,
                    },
                    title: title
                }
                Plotly.newPlot(this.scatter_div, traces, layout);

                this.update_scatter();

                this.div = document.createElement('div');
                this.div.appendChild(this.method_select);
                this.div.appendChild(this.scatter_div);
                
                // TODO: when highlighting, the hover events seem to break
                // const this_ = this;
                // this.scatter_div.on('plotly_hover', function(data){
                //     console.log('hover')
                //     const idx = data.points[0].pointIndex;
                //     this_.highlight('text', idx);
                //     this_.highlight('image', idx);
                // });
                // this.scatter_div.on('plotly_unhover', function(data){
                //     this_.highlight('reset', 0);
                // });
            }

            get_XY_coordinates(){
                const x_image = this.projection_df.filter((row) => row.data_type == 'image').map(row => +row[this.model_name + '_' + this.method_select.value + '_x'])
                const y_image = this.projection_df.filter((row) => row.data_type == 'image').map(row => +row[this.model_name + '_' + this.method_select.value + '_y'])
                const x_text = this.projection_df.filter((row) => row.data_type == 'text').map(row => +row[this.model_name + '_' + this.method_select.value + '_x'])
                const y_text = this.projection_df.filter((row) => row.data_type == 'text').map(row => +row[this.model_name + '_' + this.method_select.value + '_y'])
                return {x_image: x_image, y_image: y_image, x_text: x_text, y_text: y_text};
            }

            update_scatter(){
                const coords = this.get_XY_coordinates();

                let lines = []
                for(let line_idx = 0; line_idx < coords.x_image.length; line_idx++){
                    const line = {
                        name: 'connections',
                        type: 'line',
                        x0: coords.x_image[line_idx],
                        y0: coords.y_image[line_idx],
                        x1: coords.x_text[line_idx],
                        y1: coords.y_text[line_idx],
                        line: {
                            width: 1,
                            color: 'grey'
                        }
                    }
                    lines.push(line);
                }
                Plotly.update(this.scatter_div, {'x': [coords.x_image, coords.x_text], 'y': [coords.y_image, coords.y_text]}, {shapes: lines}, [0, 1]);
            }

            // TODO: when highlighting, the hover events seem to break
            // highlight(mode, idx){
            //   console.log('highlight')
            //     const coords = this.get_XY_coordinates();
            //     let coords_x = this.scatter_div.data[2].x;
            //     let coords_y = this.scatter_div.data[2].y;
            //     if(mode == 'image'){
            //         coords_x.push(coords.x_image[idx]);
            //         coords_y.push(coords.y_image[idx]);
            //     }else if(mode == 'text'){
            //         coords_x.push(coords.x_text[idx]);
            //         coords_y.push(coords.y_text[idx]);
            //     }else{
            //         Plotly.restyle(this.scatter_div, {'x': [[]], 'y': [[]]}, {}, [2])
            //         return;
            //     }

            //     Plotly.restyle(this.scatter_div, {'x': [coords_x], 'y': [coords_y]}, {}, [2])
            // }
        }


        class SimilarityHeatmapWidget{
            constructor(do_cluster=false, title='', width=500, height=420, z_min=null, z_max=null){
                this.do_cluster = do_cluster;

                const data = Array.from(Array(200), () => Array(200).fill(0));
                var trace1 = {
                    z: data,
                    type: 'heatmap',
                    hoverinfo: 'text',
                    colorscale: plasma_colors, //'Viridis',//'YlOrRd', 
                    // reversescale: true
                    zmin: z_min,
                    zmax: z_max,
                };
                var traces = [trace1];
                
                var layout = {
                    width: width, 
                    height: height,
                    margin: {l:50, r:10, t:10, b:25},
                }
                this.heatmap_div = document.createElement('div');
                Plotly.newPlot(this.heatmap_div, traces, layout);

                this.meta_info_div = document.createElement('div');
                this.meta_info_div.style.fontSize = '12px';
                this.meta_info_div.style.marginLeft = '50px';

                this.div = document.createElement('div');
                if(title !== ''){
                  const title_el = document.createElement('h4');
                  title_el.innerHTML = title;
                  title_el.style.marginLeft = '50px';
                  title_el.style.marginBottom = '0px';
                  this.div.appendChild(title_el);
                }
                this.div.appendChild(this.meta_info_div);
                this.div.appendChild(this.heatmap_div);

            }

            _set_plotly_event(){
              const this_ = this;
              this.heatmap_div.on('plotly_hover', function(data){
                  const idx = data.points[0].pointIndex;
                  this_.update_hoverIdx([idx]);
              })
            }

            _get_matrix_gridlines(data){
                return [
                    {
                        type: 'line',
                        x0: data.length/2-0.5,
                        x1: data.length/2-0.5,
                        y0: 0-0.5,
                        y1: data.length-0.5,
                        line: {
                            width: 1,
                            color: 'black'
                        }
                    },
                    {
                        type: 'line',
                        y0: data.length/2-0.5,
                        y1: data.length/2-0.5,
                        x0: 0-0.5,
                        x1: data.length-0.5,
                        line: {
                            width: 1,
                            color: 'black'
                        }
                    },
                ]
            }

            async update_heatmap(dataset_name, model_name, show_meta_info=true){
                return load_meta_info_fn(dataset_name, model_name).then((meta_data) => {
                  if(show_meta_info){
                    this.meta_info_div.innerHTML = `Modality distance: ${meta_data.gap_distance.toFixed(2)} | Loss: ${meta_data.loss.toFixed(2)}`;
                  }

                  this.cluster_sort_idcs = meta_data.cluster_sort_idcs;
                  this.cluster_sort_idcs_reverse = meta_data.cluster_sort_idcs_reverse;

                  load_similarities_fn(dataset_name, model_name).then((data) => {
                      if(self.do_cluster){
                        var clustered_data = Array.from(Array(data.length), () => Array(data[0].length).fill(0));

                        for (let i = 0; i < this.cluster_sort_idcs.length; i++) {
                          const i_x = this.cluster_sort_idcs[i];
                          for (let j = 0; j < this.cluster_sort_idcs.length; j++) {
                            const i_y = this.cluster_sort_idcs[j];

                            // sort and add to clustered array for each quadrant
                            clustered_data[i][j] = data[i_x][i_y];
                            clustered_data[i][j+this.cluster_sort_idcs.length] = data[i_x][i_y+this.cluster_sort_idcs.length];
                            clustered_data[i+this.cluster_sort_idcs.length][j] = data[i_x+this.cluster_sort_idcs.length][i_y];
                            clustered_data[i+this.cluster_sort_idcs.length][j+this.cluster_sort_idcs.length] = data[i_x+this.cluster_sort_idcs.length][i_y+this.cluster_sort_idcs.length];
                          }
                        }
                        data = clustered_data;
                      }

                      const shapes = this._get_matrix_gridlines(data);

                      if(this.do_cluster){
                        const cluster_labels = meta_data.cluster_labels;
                        const cluster_sizes = meta_data.cluster_sizes;

                        let offset = 0-0.5 // -0.5 because heatmap rectangles are drawn around [-0.5, 0.5]
                        for (let i = 0; i < cluster_labels.length; i++) {
                          const cluster_label = cluster_labels[i];
                          const cluster_size = cluster_sizes[i];
                          
                          if(cluster_size > 5){
                            let textposition = 'middle right';
                            if(offset < data.length/2/2){
                              textposition = 'middle left';
                            }
                            shapes.push(
                              {
                                type: 'rect',
                                x0: data.length/2+offset,
                                y0: offset,
                                x1: data.length/2+offset+cluster_size,
                                y1: offset+cluster_size,
                                label: {
                                  text: cluster_label, 
                                  textposition: textposition, 
                                  font: {size: 10, color: 'white'},
                                  padding: Math.log(cluster_size)*10
                                },
                                line: {
                                    width: 1,
                                    color: 'white'
                                },
                              }
                            );
                          }
                          offset += cluster_size;
                        }

                      }
                      Plotly.update(this.heatmap_div, {'z': [data]}, {
                          xaxis: {
                              tickmode: 'array',
                              ticktext: ['Image', 'Text'],
                              tickvals: [data.length/4, 3*data.length/4],
                              fixedrange: false
                          },
                          yaxis: {
                              tickmode: 'array',
                              tickvals: [data.length/4, 3*data.length/4],
                              ticktext: ['Image', 'Text'], 
                              fixedrange: false,
                              autorange: 'reversed', 
                          },
                          shapes: shapes
                      }, [0]);

                      this._set_plotly_event();
                  });
                });

            }

            update_hoverIdx(idcs){
                const data = this.heatmap_div.data[0].z;
                let shapes = this.heatmap_div.layout.shapes?.filter((shape) => shape.name !== 'hover_idx');
                for(let i = 0; i < idcs.length; i++){
                    const y_idx = idcs[i][0];
                    const x_idx = idcs[i][1];

                    if(x_idx >= 0 & x_idx < data.length){
                        const shape = {
                            name: 'hover_idx', 
                            type: 'line',
                            x0: x_idx,
                            x1: x_idx,
                            y0: 0-0.5,
                            y1: data.length-0.5,
                            line: {
                                width: 1,
                                color: 'grey'
                            }
                        }
                        shapes.push(shape)
                    }
                    if(y_idx >= 0 & y_idx < data.length){
                        const shape = {
                            name: 'hover_idx', 
                            type: 'line',
                            y0: y_idx,
                            y1: y_idx,
                            x0: 0-0.5,
                            x1: data.length-0.5,
                            line: {
                                width: 1,
                                color: 'grey'
                            }
                        }
                        shapes.push(shape)
                    }
                }
                Plotly.update(this.heatmap_div, {}, {shapes: shapes});
            }
            
        }

    </script>

    <script>
        // helper functions

        function connect_scatter_hover(scatter_widget, hover_widget){
            scatter_widget.scatter_div.on('plotly_hover', function(data){
                const idx = data.points[0].pointIndex;
                hover_widget.update1('text', idx);
                hover_widget.update2('image', idx);
            })
        }

        function connect_heatmap_hover(heatmap_widget, hover_widget){
            heatmap_widget.heatmap_div.on('plotly_hover', function(data){
                const idx = data.points[0].pointIndex;
                let y_idx = idx[0];
                let x_idx = idx[1];
                const z_data = data.points[0].data.z;
                let mode1 = 'text';
                let mode2 = 'text';
                if(x_idx < z_data.length/2){
                    mode1 = 'image';
                }
                if(y_idx < z_data.length/2){
                    mode2 = 'image';
                }
                  x_idx = x_idx%(z_data.length/2);
                  y_idx = y_idx%(z_data.length/2);

                if(heatmap_widget.do_cluster){
                  y_idx = heatmap_widget.cluster_sort_idcs[y_idx];
                  x_idx = heatmap_widget.cluster_sort_idcs[x_idx];
                }

                hover_widget.update1(mode1, x_idx);
                hover_widget.update2(mode2, y_idx);
            })
        }

        function connect_scatter_heatmap(scatter_widget, heatmap_widget){
            scatter_widget.scatter_div.on('plotly_hover', function(data){
                const idx = data.points[0].pointIndex;
                heatmap_idx = idx;
                if(heatmap_widget.do_cluster){
                  heatmap_idx = heatmap_widget.cluster_sort_idcs_reverse[idx];
                }
                heatmap_widget.update_hoverIdx([
                    [heatmap_idx, data.points[0].data.x.length + heatmap_idx], 
                    [data.points[0].data.x.length + heatmap_idx, heatmap_idx]
                ])
            });

            scatter_widget.scatter_div.on('plotly_unhover', function(data){
                heatmap_widget.update_hoverIdx([])
            });

            // TODO: when highlighting, the hover events seem to break
            // heatmap_widget.heatmap_div.on('plotly_hover', function(data){
            //   console.log('hi')
            //     const idx = data.points[0].pointIndex;
            //     let y_idx = idx[0];
            //     let x_idx = idx[1];
            //     const z_data = data.points[0].data.z;
            //     let mode1 = 'text';
            //     let mode2 = 'text';
            //     if(x_idx < z_data.length/2){
            //         mode1 = 'image';
            //     }
            //     if(y_idx < z_data.length/2){
            //         mode2 = 'image';
            //     }
            //     x_idx = x_idx%(z_data.length/2);
            //     y_idx = y_idx%(z_data.length/2);

            //     if(heatmap_widget.do_cluster){
            //       y_idx = heatmap_widget.cluster_sort_idcs[y_idx];
            //       x_idx = heatmap_widget.cluster_sort_idcs[x_idx];
            //     }

            //     scatter_widget.highlight(mode1, x_idx);
            //     scatter_widget.highlight(mode2, y_idx);
            // });

            // heatmap_widget.heatmap_div.on('plotly_unhover', function(data){
            //     scatter_widget.highlight('reset', 0);
            // });
        }

        function clip_explorer_by_model(dataset_name, model_name, el, prompts_promise, projection_promise, projection_method = 'PCA', do_cluster=false){

            if(!(el instanceof Element)){
                el = document.getElementById(el);
            }
            projection_promise.then(function(data){ 
                const heatmap_widget = new SimilarityHeatmapWidget(do_cluster=do_cluster);
                heatmap_widget.update_heatmap(dataset_name, model_name).then(() => {
                  el.appendChild(heatmap_widget.div)

                  const scatter_widget = new ScatterPlotWidget(data, model_name, selected_method=projection_method);
                  el.appendChild(scatter_widget.div)

                  connect_scatter_heatmap(scatter_widget, heatmap_widget);

                  prompts_promise.then(captions => {
                      const hover_widget = new HoverWidget(dataset_name, captions);
                      connect_scatter_hover(scatter_widget, hover_widget)
                      connect_heatmap_hover(heatmap_widget, hover_widget);
                      el.appendChild(hover_widget.div)
                      return captions;
                  });
                });
                return data;
            });
        }


        // TODO: add cluster functionality
        const available_models = ['CLIP', 'CyCLIP', 'CLOOB', 'CLOOB_LAION400M'];
        function clip_explorer_widget(dataset_name, el_id, prompts_promise, projection_promise, projection_method = 'UMAP'){
            // init model select
            const model_select = document.createElement('select');
            for (const key in available_models) {
                var opt = new Option(available_models[key], available_models[key])
                model_select.appendChild(opt);
            }
            model_select.selectedIndex = 0;
            model_select.style.marginLeft = '50px';
            const div = document.getElementById(el_id);
            div.appendChild(model_select);

            // init modality gap checkbox
            const close_gap = document.createElement('input');
            close_gap.type = 'checkbox';
            close_gap.id = el_id + '_close_gap'
            close_gap.style = 'margin-left: 20px;'
            const label = document.createElement('label');
            label.setAttribute('for', close_gap.id);
            label.textContent = 'Close modality gap';
            label.style = 'font-size:15px;';
            div.appendChild(close_gap);
            div.appendChild(label);

            // init cluster checkbox
            const cluster_checkbox = document.createElement('input');
            cluster_checkbox.type = 'checkbox';
            cluster_checkbox.id = el_id + '_cluster'
            cluster_checkbox.style = 'margin-left: 20px;'
            const cluster_label = document.createElement('label');
            cluster_label.setAttribute('for', cluster_checkbox.id);
            cluster_label.textContent = 'Cluster matrix by similarity';
            cluster_label.style = 'font-size:15px;';
            div.appendChild(cluster_checkbox);
            div.appendChild(cluster_label);


            // init explorer div
            const explorer_div = document.createElement('div');
            explorer_div.style = 'display: flex;'
            div.appendChild(explorer_div);

            // handle UI changes
            const update_explorer = () => {
                explorer_div.innerHTML = ''
                let model_name = model_select.value;
                if(close_gap.checked){
                    model_name += '_nogap';
                }
                clip_explorer_by_model(dataset_name, model_name, explorer_div, prompts_promise, projection_promise, projection_method, do_cluster=cluster_checkbox.checked)
            };
            model_select.onchange = update_explorer;
            close_gap.onchange = update_explorer;
            cluster_checkbox.onchange = update_explorer;

            update_explorer();

        }

        clip_comparer = (models, prompts_promise, dataset_name, el_id, z_min=null, z_max=null) => {
            prompts_promise.then(async captions => {
                let heatmap_widgets = {}

                function highlight_hover(data){
                    console.log('highlight hover')
                    for (const key in heatmap_widgets) {
                        if (Object.hasOwnProperty.call(heatmap_widgets, key)) {
                            const element = heatmap_widgets[key];
                            const idx = data.points[0].pointIndex;
                            element.update_hoverIdx([idx]);
                        }
                    }
                }

                const hover_widget = new HoverWidget(dataset_name, captions);

                for (const key in models) {
                    const model = models[key];
                    heatmap_widgets[model] = new SimilarityHeatmapWidget(do_cluster=false, title=model, width=500, height=420, z_min=z_min, z_max=z_max);
                    await heatmap_widgets[model].update_heatmap(dataset_name, model, show_meta_info=false);
                    document.getElementById(el_id + '-' + Math.floor(key/2)).appendChild(heatmap_widgets[model].div)
                    heatmap_widgets[model].heatmap_div.on('plotly_hover', highlight_hover)
                    connect_heatmap_hover(heatmap_widgets[model], hover_widget);
                }

                document.getElementById(el_id + '-hover').appendChild(hover_widget.div)

                return captions;
            })
        }

        
        const augmented_heatmap_comparer = (augmentation, el_id) => {

            const img_thumbnails = [];

            const update_augmented_clip_comparer = (id) => {
            
                img_thumbnails.forEach((img) => {
                    img.style.borderColor = 'transparent';
                    img.style.opacity = 0.3;
                });
                img_thumbnails[id].style.borderColor = 'black';
                img_thumbnails[id].style.opacity = 0.9;

                const dataset_name = augmentation + '-' + id + '_size-100';

                const prompts_promise = fetch('./exported_data_checkpoints/' + dataset_name + '/prompts.txt')
                    .then(response => response.text())
                    .then(data => data.split('\n'))
                    .catch(error => console.error(error));

                document.getElementById(el_id+'-0').innerHTML = '';
                document.getElementById(el_id+'-1').innerHTML = '';
                document.getElementById(el_id+'-hover').innerHTML = '';

                clip_comparer(['CLIP', 'CyCLIP', 'CLOOB', 'CLOOB_LAION400M'], prompts_promise, dataset_name, el_id, z_min=0, z_max=1)
            }

            document.getElementById(el_id+'-picker').innerHTML = '';
            for (let i = 0; i < 9; i++) {
                const img = document.createElement('img');
                img.src = './exported_data_checkpoints/example_images/' + i + '.jpg';
                img.style.cursor = 'pointer';
                img.style.border = 'transparent 2px solid';
                document.getElementById(el_id+'-picker').appendChild(img);
                img_thumbnails.push(img);
                img.addEventListener("click", function() {
                    update_augmented_clip_comparer(i)
                });
            }

            update_augmented_clip_comparer(0);

        }

    </script>

    <script type="text/front-matter">
        title: "Understanding and Comparing Multi-Modal Models"
        description: "Exploring the Latent Space of CLIP-like Models (CLIP, CyCLIP, CLOOB) Using Inter-Modal Pairs"
        authors:
        - Christina Humer: https://jku-vds-lab.at/persons/humer/
        - Marc Streit: https://marc-streit.com
        - Hendrik Strobelt: http://hendrik.strobelt.com/
        affiliations:
        - Johannes Kepler University Linz: https://www.jku.at/
        - Johannes Kepler University Linz: https://www.jku.at/
        - IBM Research: https://www.research.ibm.com/labs/cambridge/
    </script>

    <dt-article>
        <h1>Understanding and Comparing Multi-Modal Models</h1>
        <h2>Exploring the Latent Space of CLIP-like Models (CLIP, CyCLIP, CLOOB) Using Inter-Modal Pairs</h2>
        <dt-byline></dt-byline>
        <p>
            Contrastive Language Image Pre-training (CLIP)<dt-cite key="radford_learning_2021"></dt-cite> and variations of this approach like CyCLIP<dt-cite key="goel_cyclip_2022"></dt-cite>, or CLOOB<dt-cite key="furst_cloob_2022"></dt-cite> are trained on image-text pairs with a contrastive objective. The goal of contrastive loss objectives is to minimize latent-space distances of data points that have the same underlying meaning. We refer to the particular cases of contrastive learning that CLIP-like models perform as multi-modal contrastive learning because they use two (or more<dt-cite key="girdhar_imagebind_2023"></dt-cite>) modes of data (e.g., images and texts) where each mode uses their own encoder to generate a latent embedding space. More specifically, the objective that CLIP is optimized for minimizes the distances between image-text embeddings of pairs that have the same semantic meaning while maximizing the distances to all other combinations of text and image embeddings. 
        </p>
        <p>
            We would expect that such a shared latent space places similar concepts of images and texts close to each other, as demonstrated in the following sketch. However, the reality is a bit more complicated.
        </p>
        <img 
            class="l-body"
            src="assets/idea_figure.png" 
            alt="Example of how we imagined a 2-dimensional projection of CLIP's image and text embeddings. Image and text points are shown in one scatter plot and instants that are semantically similar are plotted close together." 
        >
        <h2>
            CLIP and its Modality Gap
        </h2>
        <p>
            Despite the clear objective that is supposed to bring texts and images to a shared embedding space there is a phenomenon called "Modality Gap"<dt-cite key="liang_mind_2022"></dt-cite> which describes that embeddings of different modalities lie in their own embedding subspaces. The example below visualizes the Modality Gap between images and texts of CLIP<dt-fn>We use the official CLIP implementation by OpenAI: <a src=’https://github.com/openai/CLIP’>https://github.com/openai/CLIP</a> with the RN50 image encoder.</dt-fn> embeddings for a subset of 100 randomly selected images from MSCOCOs<dt-cite key="lin_microsoft_2015"></dt-cite> validation set. The image and text embeddings are projected to a 2-dimensional space and visualized in a scatter plot. We use a gray line to connect image-text pairs that belong together. 
        </p>

        <div id="clip-modalitygap-scatterplot" style="display: inline-flex;"></div>

        <script>
            mscoco_val_projections_promise.then(function(data){ 
                const scatter_widget = new ScatterPlotWidget(data, 'CLIP');
                document.getElementById('clip-modalitygap-scatterplot').appendChild(scatter_widget.div)

                mscoco_val_prompts_promise.then(captions => {
                    const hover_widget = new HoverWidget('MSCOCO-Val_size-100', captions);
                    connect_scatter_hover(scatter_widget, hover_widget)
                    document.getElementById('clip-modalitygap-scatterplot').appendChild(hover_widget.div)
                    return captions;
                })
                return data;
            });
        </script>

        <p>
            The use of dimensionality reduction methods to compute a 2-dimensional view of the data clearly shows the separation between the two modalities. However, dimensionality reduction comes hand in hand with a loss of information and possible distortion of data. We propose a different way to visualize the dimensionality gap. Similarity heatmaps are a simple yet effective way of visualizing latent space embeddings that also helped us to better understand the modality gap. Using this visualization also allowed us to gain interesting insights unrelated to the modality gap, which we could not have found with the scatter plot visualization alone.
        </p>
        <p>
            The similarity heatmap below shows the same subset of 100 image-text pairs previously shown in a scatter plot. However, we show the cosine similarities calculated between all image and text embeddings in this case. This results in a matrix with four quadrants: (i) top-left: in-modal similarities of the 100 image embeddings; (ii) bottom-right: in-modal similarities of the 100 text embeddings; (iii) top-right: cross-modal similarities between the 100 image embeddings and the 100 text embeddings; and (iv) bottom-left: a transposed version of the cross-modal image-text similarities. The diagonal axis of each quadrant represents the matching data points (i.e., for the in-modal similarities, the diagonal shows the similarities of the image or text embedding to itself, while for the cross-modal similarities, the diagonal shows the matching image-text pairs). The modality gap is immediately visible: the in-modal similarities are much higher overall compared to the cross-modal similarities.
        </p>
        <div id="clip-modalitygap-heatmap" style="display: inline-flex;"></div>
        <script>
            const heatmap_widget = new SimilarityHeatmapWidget();
            heatmap_widget.update_heatmap('MSCOCO-Val_size-100', 'CLIP').then(() =>{

              document.getElementById('clip-modalitygap-heatmap').appendChild(heatmap_widget.div)

              mscoco_val_prompts_promise.then(captions => {
                  const hover_widget = new HoverWidget('MSCOCO-Val_size-100', captions);
                  connect_heatmap_hover(heatmap_widget, hover_widget);
                  document.getElementById('clip-modalitygap-heatmap').appendChild(hover_widget.div)

                  return captions;
              });
            });
        </script>

        <p>Now, where does this modality gap even come from?</p>
        <p>
            As analyzed by Liang et al.<dt-cite key="liang_mind_2022"></dt-cite>, the modality gap already appears before models are trained, possibly caused by random weight initialization and different model architectures. The authors also highlight that the gap between modalities persists throughout training, which means that CLIP's objective function cannot overcome this phenomenon. A reason for that could be that the objective function only trains on the alignment of (non-)matching image-text combinations but does not contain any regularization terms with regard to the overall layout of the embedding spaces and in-modality alignment<dt-cite key="goel_cyclip_2022"></dt-cite>. 
            Liang et al.<dt-cite key="liang_mind_2022"></dt-cite> also formally define the modality gap as the Euclidean difference between the centers of each modality:
        </p>

        <div id="modalityGapFormula"></div>
        <!-- https://katex.org/docs/browser.html -->
        <script nomodule defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.js" integrity="sha384-8JJEhHkwY0VI06p7huKe7LqvYgDhSS69qJxWSQGwlB8FmDnAgPKgZGk5Ui0VyuNH" crossorigin="anonymous"></script>
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.css" integrity="sha384-YiVwk+MBV52+yIvvplLwDxm3iGDI7dYb731lqsN9ASGBpIpJ/dte35lBIuR5oGFU" crossorigin="anonymous">
        <script type="module" type="text/javascript">
            import katex from 'https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.mjs';
            katex.render(String.raw`\delta_{gap} = \frac{1}{n} \sum_{i=1}^n{x_i} - \frac{1}{n} \sum_{i=1}^n{y_i}`, document.getElementById('modalityGapFormula'), {
                // \delta_{gap} = \frac{1}{n} \sum_{i=1}^n{x_i} - \frac{1}{n} \sum_{i=1}^n{y_i}
                throwOnError: false
            });
        </script>

        <p>where x<sub>i</sub> and y<sub>i</sub> are the normalized image and text embedding vectors.</p>

        <p>They also experimented with manually reducing the modality gap by moving the embeddings closer together along the gap vector.</p>
        <img class="l-body" src="./exported_data_checkpoints/closethegap-schema.png"/>
        <p>
            However, since the modality subspaces trained by CLIP are not symmetric<dt-fn>Goel et al.<dt-cite key="goel_cyclip_2022"></dt-cite> argue that the CLIP objective does, in fact, symmetrize the spaces in its optimal solution; in practice however this ideal scenario does not happen.</dt-fn> to each other, a modification of the embeddings destroys the complex relationship between images and texts that was derived during training. This naturally results in an increasing loss when changing the distance between modalities that was originally trained, as shown in the visualization below. The x-axis shows the Euclidean distance between the two embedding centers (the black dashed line indicates the original distance). The y-axis shows the loss that results when moving the two embeddings closer together or further away from each other. We calculated these values with the whole 5000 samples MSCOCO validation set. It becomes visible that the global minimum of this manual intervention is at the point of the original (trained) modality gap.
        </p>

        <div id="clip-loss-landscape" style="display: inline-flex;"></div>
        <script>
            clip_loss_landscape_promise.then(data => {
                const modality_gap_widget = new ModalityGapWidget('Loss Landscape CLIP', data);
                document.getElementById('clip-loss-landscape').appendChild(modality_gap_widget.div)

                return data;
            });
        </script>

        <p>
            Now, what do the similarity heatmap and scatter plot look like when we manually close the gap? As shown in the visualization below, the similarity matrix seems more homogeneous (i.e., in-modal similarities and cross-modal similarities are on a similar level), and points in the scatter plot are closer together. However, the scatter plot also shows that the edges between image-text pairs are still long. The many edge crossings between the image-text pairs are also a qualitative indicator that the embeddings are not well aligned. Furthermore, the text embeddings concentrate more on the center compared to the image embeddings.
        </p>

        <div id="clip-nomodalitygap" style="display: flex;" class="l-page-outset"></div>
        <script>
            clip_explorer_by_model('MSCOCO-Val_size-100', 'CLIP_nogap', "clip-nomodalitygap", mscoco_val_prompts_promise, mscoco_val_projections_promise)
        </script>

        <h2>What About Other CLIP-Like Models?</h2>
        <p>
            As mentioned previously, the two modalities live on different embedding spaces, and the two embeddings vary in structure (i.e., they are not symmetric to each other). Recently published papers propose different versions of CLIP, where the objective function has been adjusted to regularize the trained embedding space. We look closer into two promising approaches, namely CyCLIP<dt-cite key="goel_cyclip_2022"></dt-cite> and CLOOB<dt-cite key="furst_cloob_2022"></dt-cite>.
        </p>

        <h3>CyCLIP</h3>
        <p>
            According to Goel et al.<dt-cite key="goel_cyclip_2022"></dt-cite> using CLIP-generated image-text embeddings interchangeably (e.g., for Language-guided image generation) is suboptimal because the embeddings are–in practice–not aligned. They propose an augmentation of the InfoNCE loss used to optimize CLIP that adds two regularization terms that enforce the two embedding spaces to be symmetric: one for in-modal symmetry (LI) and one for cross-modal symmetry (LC) of the similarities. 
        </p>
        <div id="cyCLIPFormula"></div>
        <script type="module" type="text/javascript">
            katex.render(String.raw`L_{CyCLIP} = L_{InfoNCE} + L_I + L_C`, document.getElementById('cyCLIPFormula'), {
                throwOnError: false
            });
        </script>

        <p>
            The visualizations below already hint that the embeddings are symmetric: The in-modal similarity heatmaps of the image and text quadrants look  similar, and the modalities in the image-text points in the PCA projection seem almost parallel. However, it is also clear that the modality gap is still present.
        </p>

        <div id="cyclip-modalitygap" style="display: flex;" class="l-page-outset"></div>
        <script>
            clip_explorer_by_model('MSCOCO-Val_size-100', 'CyCLIP', "cyclip-modalitygap", mscoco_val_prompts_promise, mscoco_val_projections_promise)
        </script>

        <p>
            We can confirm this with a slight modification of the similarity heatmap. For this, we calculate the difference between the in-modal similarities of images and texts, which results in a matrix where values approach zero (i.e., the matrix is a blue rectangle).
        </p>

        <div id="cyclip-showsymmetry" style="display: flex;" class="l-page-outset"></div>
        <script>
            load_similarities_fn('MSCOCO-Val_size-100', 'CyCLIP').then((data) => {
                // return %s.slice(0, 100);

                const inmodal_image_similarities = data.slice(0,data.length/2).map((val_arr) => val_arr.slice(0,data.length/2))
                const inmodal_text_similarities = data.slice(data.length/2).map((val_arr) => val_arr.slice(data.length/2))
                
                const simmilarity_diff =  inmodal_image_similarities.map((arr, index1) => arr.map((val, index2) => val - inmodal_text_similarities[index1][index2]))
                var trace1 = {
                    z: simmilarity_diff,
                    type: 'heatmap',
                    hoverinfo: 'text',
                    colorscale: plasma_colors, //'YlOrRd',
                    // reversescale: true,
                    zmin: 0.35,
                    zmax: 1.0,
                };
                var traces = [trace1];
                
                var layout = {
                    width: 500, 
                    height: 420,
                    margin: {l:50, r:10, t:10, b:25},
                    xaxis: {
                        tickmode: 'array',
                        ticktext: [],
                        tickvals: [],
                        fixedrange: false
                    },
                    yaxis: {
                        tickmode: 'array',
                        tickvals: [],
                        ticktext: [], 
                        fixedrange: false,
                        autorange: 'reversed', 
                    },
                }
                Plotly.newPlot('cyclip-showsymmetry', traces, layout);
            })
        </script>

        <p>
            When looking back to Liang et al.’s<dt-cite key="liang_mind_2022"></dt-cite> experiments that tried to align the embeddings by moving them along the modality gap vector, we saw that this does not work well for CLIP embeddings because the space is not symmetrized. For CyCLIP, however, the symmetrization is enforced with the objective function, and we could see in the previous plots that the spaces are indeed mostly symmetric. Let's look into the loss landscape for different modality distances of CyCLIP embeddings when moving the embeddings along the gap vector. In the visualization below, we can see that CyCLIP’s loss landscape looks very different from CLIP’s landscape: in the distance interval of [-1.5; 1.5] the loss does not change much, which is another indicator that CyCLIP indeed learns nearly symmetric embedding spaces.<dt-fn>Note that the loss outside the [-1.5; 1.5] interval increases. This could be due to the fact that the spaces are not perfectly symmetric. Another explanation could be that this happens due to  normalizing the embeddings to the unit sphere, but the method proposed for closing the gap is done in Euclidean space.</dt-fn>
        </p>

        <div id="cy-clip-loss-landscape" style="display: flex;" class="l-middle-outset"></div>
        <script>
            clip_loss_landscape_promise.then(data => {
                const modality_gap_widget = new ModalityGapWidget('Loss Landscape CLIP', data);
                document.getElementById('cy-clip-loss-landscape').appendChild(modality_gap_widget.div)
                return data;
            });
            cyclip_loss_landscape_promise.then(data => {
                const modality_gap_widget = new ModalityGapWidget('Loss Landscape CyCLIP', data);
                document.getElementById('cy-clip-loss-landscape').appendChild(modality_gap_widget.div)
                return data;
            });
        </script>

        <p>
            Again, when closing the modality gap, the similarity heatmap becomes more homogenous (i.e., the in-modal similarities and cross-modal similarities are similarly strong). Also, the image-text pairs in the scatter plot are closer together, but there are still quite some edge crossings.
        </p>

        <div id="cyclip-nomodalitygap" style="display: flex;" class="l-page-outset"></div>
        <script>
            clip_explorer_by_model('MSCOCO-Val_size-100', 'CyCLIP_nogap', "cyclip-nomodalitygap", mscoco_val_prompts_promise, mscoco_val_projections_promise)
        </script>

        <p>
            Alternatively, we can use UMAP or tSNE projection to better utilize the neighborhoods of similar embeddings instead of linearly determining the axes with the highest variance, as done in PCA. In the case of CyCLIP (scatter plot on the right hand), the use of a neighborhood-based dimensionality reduction technique results in shorter edges and better clustering of similar embeddings. For CLIP (scatter plot on the left hand), edges remain long, and similar images and texts are not clustered together. See the Appendix for examples with 5000 samples.
        </p>

        <div id="cy-clip-nogap-umap" style="display: flex;" class="l-page-outset"></div>
        <script>
            mscoco_val_projections_promise.then(function(data){ 
                const scatter_widget = new ScatterPlotWidget(data, 'CLIP_nogap', selected_method='UMAP', title='CLIP');
                document.getElementById('cy-clip-nogap-umap').appendChild(scatter_widget.div)

                const scatter_widget_cyclip = new ScatterPlotWidget(data, 'CyCLIP_nogap', selected_method='UMAP', title='CyCLIP');
                document.getElementById('cy-clip-nogap-umap').appendChild(scatter_widget_cyclip.div)
                return data;
            });
        </script>

        <p>
            Let us summarize how the loss changes for CLIP and CyCLIP when moving the embeddings together along the modality gap vector. For that, we again use the entire MSCOCO validation dataset of 5000 samples.
        </p>

        <table>
            <tr>
                <th>Model</th>
                <th>Original Distance</th>
                <th>Original Loss</th>
                <th>Closed Distance</th>
                <th>Closed Loss</th>
                <th>Loss Difference</th>
            </tr>
            <tr>
                <td>CLIP</td>
                <td>0.818611</td>
                <td>0.355370</td>
                <td>0.035077</td>
                <td>1.124780</td>
                <td><b>0.769410</b></td>
            </tr>
            <tr>
                <td>CyCLIP</td>
                <td>0.873026</td>
                <td>0.763433</td>
                <td>0.001218</td>
                <td>0.848867</td>
                <td><b>0.085434</b></td>
            </tr>
        </table>

        <p>
            The numbers confirm that CyCLIP's loss changes far less than CLIP's loss when manually closing the gap – it neither gets better nor significantly worse. However, the question arises: 
        </p>
        <h3>
            Why Do We Even Want To Close the Modality Gap, if We Do Not Gain Performance?
        </h3>
        <p>
            From a performance optimization point of view, this is a valid question. What's the point of interfering in a well-performing system? On the other hand, the alignment of embedding spaces can become important for other downstream tasks. For example, using image and text embeddings interchangeably, as done in language-guided image generation, relies on the fact that image and text embeddings are aligned with each other and live in the same space. Another aspect is that an aligned embedding space is closer to how humans expect multi-modal models to see the data. Furthermore, closing the modality gap allows us to actually visualize texts and images in the same space, and develop interactive exploration tools that help to understand multi-modal data (e.g., analyzing pairs of human written captions and machine-generated images to find insights about text-to-image generation models like StableDiffusion<dt-cite key="rombach_high-resolution_2022"></dt-cite>).
        </p>
        <p>
            These example use cases of why closing the modality gap might be helpful should give you an incentive about why the pure "performance optimization" point of view is not the only one. In fact, if we can close the modality gap without significantly losing performance, we have a win-win situation!
        </p>

        <h3>CLOOB</h3>
        <p>
            In the previous section, we established a way to manually close the modality gap of CyCLIP embeddings. However, wouldn’t it be better to already close the gap during training and not rely on post-hoc manipulations? While we did not experiment with further modifying (Cy)CLIP’s objective to close the gap during training, we did stumble upon a different learning approach that naturally closes the gap.
        </p>
        <p>
            Contrastive Leave One Out Boost (short: CLOOB)<dt-cite key="furst_cloob_2022"></dt-cite> is a variation of CLIP that proposes an alternative objective to train the model. The two main components of their objective function are (i) Modern Hopfield Layers and (ii) the InfoLOOB loss instead of the InfoNCE loss as used by CLIP. The authors argue that their modifications solve CLIP's "explaining away" problem (i.e., focusing on a small subset of features while ignoring other relevant features), and InfoNCE's saturation problem.<dt-fn>See Rumetshofer et al.<dt-cite key="furst_cloob_2022"></dt-cite> for more information.</dt-fn>
        </p>
        <p>However, we also observe something else: These modifications of the training objective seem to aid the closure of the modality gap!</p>

        <div id="cloob-explorer" style="display: flex;" class="l-page-outset"></div>
        <script>
            clip_explorer_by_model('MSCOCO-Val_size-100', 'CLOOB', "cloob-explorer", mscoco_val_prompts_promise, mscoco_val_projections_promise, 'UMAP')
        </script>

        <p>
            We believe that CLOOB’s ability to close the gap during training mainly stems from  the modern hopfield layers that are applied before calculating the InfoLOOB loss. In this step, the batch of image embeddings and text embeddings is used as an associative memory to create a weighted average of embeddings for the current instance. Since this is done for each combination of embedding and associative memory (i.e., each image and each text embedding is associated with the entire batch of image embeddings AND the entire batch of text embeddings). This creates a stronger correlation between the image/text instances to their own modality and the opposite modality.<dt-fn>See this blog post for a detailed explanation of modern hopfield layers: <a src=”https://ml-jku.github.io/hopfield-layers/”>https://ml-jku.github.io/hopfield-layers/</a>.</dt-fn>
        </p>

        <h3>Summary of Modality Gap Analysis</h3>
        <p>
            The following visualizations give an overview of the similarity heatmap for CLIP and CyCLIP before (left) and after (right) manually removing the modality gap, as well as the similarity heatmap for CLOOB with the official checkpoints from the paper and a CLOOB version that was trained on the LAION 400M dataset and used a ViT instead of a CNN to encode images<dt-cite key="crowson_cloob-training_2023"></dt-cite>. Note how the overall distribution of similarity values differs between the various embedding spaces. For example, the two CLOOB models seem to discriminate more strictly between matching and non-matching image-text pairs.
        </p>

        <div id="similarity-heatmaps"  class="l-page-outset" style="display: flex;">
            <div>
                <div id="similarity-heatmaps-0" style="display: inline-flex;"></div>
                <div id="similarity-heatmaps-1" style="display: inline-flex;"></div>
                <div id="similarity-heatmaps-2" style="display: inline-flex;"></div>
            </div> 
            <div id="similarity-heatmaps-hover"></div>
        </div>
        <script>
            clip_comparer(['CLIP', 'CLIP_nogap', 'CyCLIP', 'CyCLIP_nogap', 'CLOOB', 'CLOOB_LAION400M'], mscoco_val_prompts_promise, 'MSCOCO-Val_size-100', 'similarity-heatmaps', z_min=-0.25, z_max=1)
        </script>

        <p>
            The previous sections taught us about the modality gap, where it comes from, and ways to close it. We also gave reasons for why closing the gap might be beneficial and now demonstrate how closing the modality gap can help for the use case of visual analytics. We also introduced a handy new way of visualizing latent space embeddings and utilize this again in the following analyses. Finally, we also want to mention that there is a way to visually close the modality gap (i.e., without actually closing the gap in the embedding space), as described in the Appendix.
        </p>

        <h2>Converting the Technique into a Tool</h2>
        <p>
            Using the previously introduced techniques, we implemented an interactive prototype called “CLIP explorer”. Users can switch between models, explore the similarity heatmap and scatter plot visualizations, manually close the modality gap, and try various projection methods.
        </p>

        <div id="clip-explorer-mscoco" class="l-page-outset"></div>
        <script>
            clip_explorer_widget('MSCOCO-Val_size-100', "clip-explorer-mscoco", mscoco_val_prompts_promise, mscoco_val_projections_promise, 'UMAP')
        </script>


        <h3>Identifying Data Subsets</h3>
        <p>
            We can look into semantic subsets of data by filtering instances based on their captions. The following example shows the visualizations for the subset that contains the substring "dog". We notice that some lines in the similarity matrix have a darker color. When hovering over those darker lines, we can see that most of these instances correspond to images and texts about "hot dogs" or other images that do not show a dog or where a dog is in an uncommon setting. To make this even more obvious, we can use the "Cluster matrix by similarity" function that reorders the similarity heatmap such that similar lines are grouped together. One cluster that stands out in all three CLIP-like models is the "hot dog" cluster. However, we can also see clusters for "dog and frisbee", "dog and bed", or "dog and car".
            <!-- clustering could be done differently or made interactive (e.g., allow users to select the number of clusters, or whether they want to cluster by in-modal or cross-modal similarities...) -->
        </p>
        <div id="clip-explorer-filtered" class="l-page-outset"></div>
        <script>
            let dataset_name = 'MSCOCO-Val_filter-any_dog';
            const mscoco_filtered_val_prompts_promise = fetch('./exported_data_checkpoints/' + dataset_name + '/prompts.txt')
                .then(response => response.text())
                .then(data => data.split('\n'))
                .catch(error => console.error(error));

            const mscoco_filtered_val_projections_promise = d3.csv('./exported_data_checkpoints/' + dataset_name + '/projections.csv');
            
            clip_explorer_widget(dataset_name, "clip-explorer-filtered", mscoco_filtered_val_prompts_promise, mscoco_filtered_val_projections_promise, 'UMAP')
        </script>


        <h3>Analyze DiffusionDB Dataset</h3>
        <p>
            We would like to see what the models’ latent-space embeddings look like for a dataset that is not (entirely) procured by humans. To this end, we use DiffusionDB<dt-cite key="wang_diffusiondb_2022"></dt-cite>, a collection of human-written captions and images generated from these captions by Stable Diffusion<dt-cite key="rombach_high-resolution_2022"></dt-cite>. We use a subset of 100 randomly selected samples to qualitatively explore the embedding spaces created by the CLIP models, we previously introduced. You can use the CLIP explorer prototype below to follow along with the analysis described.
        </p>    
        <p>
            With the default settings, we randomly explore the dataset and get a feeling for the data contained in this subset. We can investigate instances that are outliers in the similarity heatmap by hovering rows or cells that have particularly large or low similarity values. For example, there are some particularly bright cells scattered in the image in-modal similarity heatmap. Upon hovering, we see that all of these images are blurry. We know that DiffusionDB added blur filters for images that were detected to show inappropriate content. Interestingly, CLIP seems to create similar latent embeddings for blurry items, causing them to show high similarity in the similarity heatmap.
        </p>
        <p>
            For further analyses, we choose "Cluster matrix by similarity" to order the matrix in a way that groups similar rows in the heatmap and investigate the clusters that are emerging. We can see a cluster for "impressionism and crystal" that seems to have homomorphic similarities over all images (i.e., there is a distinct purple line along all images of this cluster). Upon further investigation, we see that the captions in this cluster are mostly vague texts or single words (e.g., "crystal", "impressionism") that can apply to a lot of images. The same cluster becomes apparent in the text in-modal similarity heatmap, where all captions within the cluster seem to have high similarity.
        </p>
        <p>
            Let’s close the modality gap to investigate clusters in a 2-dimensional scatter plot. We can either do this by switching to the CLOOB model or using CyCLIP in combination with the "Close modality gap" option. We see that the embeddings are aligned and can use the interactive scatter plot to investigate clusters. For example, we can try to find the cluster of blurry images, or we can try to find the cluster with instances of "impressionism". Of course, this would be much more fun on a larger scale :)
        </p>

        <div id="clip-explorer-diffusiondb" class="l-page-outset"></div>
        <script>
            dataset_name = 'DiffusionDB_size-100';
            const diffusiondb_val_prompts_promise = fetch('./exported_data_checkpoints/' + dataset_name + '/prompts.txt')
                .then(response => response.text())
                .then(data => data.split('\n'))
                .catch(error => console.error(error));

            const diffusiondb_val_projections_promise = d3.csv('./exported_data_checkpoints/' + dataset_name + '/projections.csv');
            
            clip_explorer_widget(dataset_name, "clip-explorer-diffusiondb", diffusiondb_val_prompts_promise, diffusiondb_val_projections_promise, 'UMAP')
        </script>


        <h3>Augmentation Analyses</h3>
        <p>
            As previously demonstrated, we can identify patterns in datasets and subsets of datasets using the similarity heatmap visualizations. Now, we would also like to see if we can use the same techniques to find patterns in augmentations of a single data point. For example, we take a single image, generate rotated versions of this image, and use this augmented dataset to compute CLIP embeddings and similarities. The results of this experiment for the three CLIP-like models are shown in the visualization below (note that we again show two variants of the CLOOB model). To generate this dataset, we gradually rotate a selected image by 360 degrees over the course of 100 steps. Each step results in a “new” image and new data point. Note that we only augment the image, but not the text, which results in a completely homogeneous similarity in the in-modal text quadrant of the heatmap and in homogenous stripes along the text dimension in the cross-modal quadrants of the heatmap.
        </p>
        <p>
            When looking at the in-modal image similarity quadrant of the heatmap for each model using augmentations of the first image, we can see an interesting pattern emerge. In addition to the bright yellow diagonal axis that corresponds to the similarities of images to themselves, there is also the perpendicular off-diagonal axis of the matrix sticking out. When hovering along the off-diagonal, we see that the two images along this axis are actually mirrored versions of each other. It seems like all models are invariant to the horizontal flip transformation for this image. We can also see a checkerboard-like pattern emerge for some images emerging in all models except for the CLOOB_LAION400M.  When looking into the darker areas of this heatmap in more detail, we can see that the pattern occurs around multiples of 90-degree rotations. The fact that this pattern occurs mainly for the three models that use a CNN-based image encoder<dt-fn>Note that CLIP and CLOOB_LAION400M (both 400M instances) were trained on a much larger dataset than CLOOB and CyCLIP, which could also be an indicator for varying robustness.<dt-fn> and not for the one with the vision transformer could be an indicator that the two architectures vary in their ability to learn rotation invariant properties. The checkerboard-like pattern seems to be consistent with findings described by Timme et al.<dt-cite key="timme_robustness_2020"></dt-cite> where they tested the rotation robustness of various CNN classifiers by measuring the accuracy. The accuracy of the CNNs showed local maxima at multiples of 90-degree rotations and was lower in-between those angles. 
        </p>
        <p>
            When looking at the overall distribution of similarity values, we also notice that the ViT-based CLOOB model seems to have more patches of low-similarity values compared to its CNN-based counterparts. This might indicate that ViT’s overall robustness to rotation transformations is lower. In further investigations, we might want to directly compare two versions of CLIP: the current version with the CNN-based image encoder and a version with a ViT-based image encoder, and study the phenomenon on a larger dataset.
        </p>
        <p>Use the interactions to explore the heatmaps for different images yourself.</p>
        
        <div id="similarity-heatmaps-rotated-picker" class="l-page"></div>
        <div id="similarity-heatmaps-rotated" class="l-page-outset" style="display: flex;">
            <div>
                <div id="similarity-heatmaps-rotated-0" style="display: inline-flex;"></div>
                <div id="similarity-heatmaps-rotated-1" style="display: inline-flex;"></div>
            </div>
            <div id="similarity-heatmaps-rotated-hover"></div>
        </div>
        <script>
            augmented_heatmap_comparer('Rotated', 'similarity-heatmaps-rotated');
        </script>

        <p>
            In a second experiment, we analyze the heatmaps for an image to which we add an increasingly higher noise level. When looking at the heatmaps for the first image, it seems like there is a certain level of noise for each model, after which the model cannot seem to recognize the content of the image anymore. All images with a higher level of noise than this threshold seem to look (almost) the same (as indicated by a bright yellow rectangle at the lower-right corner of the in-modal image similarity quadrant).
        </p>
        
        <div id="similarity-heatmaps-noisy-picker" class="l-page"></div>
        <div id="similarity-heatmaps-noisy" class="l-page-outset" style="display: flex;">
            <div>
                <div id="similarity-heatmaps-noisy-0" style="display: inline-flex;"></div>
                <div id="similarity-heatmaps-noisy-1" style="display: inline-flex;"></div>
            </div>
            <div id="similarity-heatmaps-noisy-hover"></div>
        </div>
        <script>
            augmented_heatmap_comparer('Noisy', 'similarity-heatmaps-noisy');
        </script>

        <div class="l-page"> 
            <select id="augmentation-picker" style="margin:20px;">
                <option value="Rotated">Rotated</option>
                <option value="Noisy">Noisy</option>
            </select>
        </div>
        <div id="similarity-heatmaps-augmented-picker" class="l-page"></div>
        <div id="similarity-heatmaps-augmented" class="l-page-outset" style="display: flex;">
            <div>
                <div id="similarity-heatmaps-augmented-0" style="display: inline-flex;"></div>
                <div id="similarity-heatmaps-augmented-1" style="display: inline-flex;"></div>
            </div>
            <div id="similarity-heatmaps-augmented-hover"></div>
        </div>
        <script>
            const augmentation_picker = document.getElementById('augmentation-picker');
            augmentation_picker.onchange = () => {
                augmented_heatmap_comparer(augmentation_picker.value, 'similarity-heatmaps-augmented');
            }
            augmented_heatmap_comparer(augmentation_picker.value, 'similarity-heatmaps-augmented');
            
        </script>

        <p>conclusion...</p>

    </dt-article>


      <dt-appendix>
        <h3>Acknowledgements</h3>
        <p>TODO<p>

        <h1>Appendix</h1>
        <h2>Visually closing the modality gap</h2>
        <p>
            In the previous sections, we learned two ways that can help us close the modality gap: 
            <ol>
                <li>
                    post-hoc: post-process embeddings by moving them together along the modality gap vector; this only makes sense if the two embedding spaces are symmetric like in CyCLIP
                </li>
                <li>
                    pre-hoc: define the training objective in a way that aids the closing of the gap
                </li>
            </ol>
        </p>
        <p>
            Let's also recall the reasons why we would like to close the gap:
            <ol>
                <li>can have advantages for downstream tasks (e.g., if embeddings from two modalities need to be interchangeable)</li>
                <li>human expectations of how the embedding space should look like</li>
                <li>aids the development of multi-modal visual analytics tools</li>
            </ol>
        </p>
        <p>
            <!-- The pre-hoc method, where we adopt a training objective that closes the modality gap, supports all three reasons; the post-hoc only reasons 1 and 3... -->
            From a visualization point of view, we might not care about the first two reasons, as long as we can visually close the modality gap. By visually closing the gap, we do not change the embeddings or the model, but map them into a shared low-dimensional space. This can be accomplished in several ways: (i) out-of-sample projection, (ii) concatenating image-text embeddings and treat them as one combined embedding. The first method results in a low-dimensional datapoint for each image and each text embedding; the latter two result in a combined low-dimensional for the image-text pairs. The method you would want to choose depends on the goal of the visualization.
        </p>

        <h3>Out-of-sample projection</h3>
        <p>We can first project embeddings of either images or texts using UMAP. This builds a neighborhood graph using in-modal similarities and results in a low-dimensional projection for one modality. We can utilize UMAP's out-of-sample projection to also project the embeddings of the second modality onto the space of the first modality. Since the out-of-sample projection again tries to map each point to the most similar points in the existing low-dimensional space, you can imagine this as using the cross-modal similarities between images and texts. Since this is what CLIP was trained on (i.e., optimizing distances between texts and images), the mapping should visually remove the modality gap.</p>
        <p>As an example, we take the 5000 sample MSCOCO validation set and first fit and transform the image embeddings. We then transform the text embeddings with the existing UMAP embedding and show the results in a scatter plot. The overall structure seems to align images and texts; while there are a lot of cross-cluster connections that show that image-text pairs are not always close to each other, most of the connections seem to be within clusters. Cross-cluster connections can be indicators of various things. For example, the pairs may not be deemed similar by CLIP, which results in embeddings that are far away from each other in the high-dimensional latent space, which would be reflected in the low-dimensional projection of the embeddings. Another issue might come from the projection method itself. As mentioned previously, projecting data to a low-dimensional space comes with a loss of information that might introduce artifacts. The fact that we use out-of-sample projection might amplify this effect even further. </p>
        <p>Visual analytics tools can be helpful to gain further insights into the data and why certain pairs seem to be far away from each other. Basic interactions like hover information or selection summaries could already be a good start for further investigation. The rich nature of image and text data also allows for more advanced analytic visualizations; for example, texts can be used to extract labels for clusters that carry rich semantic meaning, or example images could be used to summarize clusters. Visually encoding the high-dimensional similarity of embedding pairs (e.g., as saturation of the lines between pairs) could be a helpful indicator that could show whether point pairs are far apart from each other due to artifacts from the projection or due to CLIP not recognizing them to be similar.</p>

        <div>
            <img src="./exported_data_checkpoints/oosExample.png"/>
        </div>
        

        <h3>Concatenating image and text embeddings</h3>
        <p>For a different kind of visual representation of image-text embedding spaces, we can simply concatenate the embedding vectors and transform them into a combined low-dimensional space. The low-dimensional space can be visualized in a scatter plot and visual analytics approaches can be used to explore the data. Note that with this approach, we only have one low-dimensional data point per image-text pair.</p>

        <div>
            <img src="./exported_data_checkpoints/concatExample.png"/>
        </div>


    </dt-appendix>

    <script type="text/bibliography">

        @inproceedings{radford_learning_2021,
            title = {Learning Transferable Visual Models From Natural Language Supervision},
            url = {https://proceedings.mlr.press/v139/radford21a.html},
            language = {en},
            urldate = {2022-12-30},
            booktitle = {Proceedings of the 38th International Conference on Machine Learning},
            publisher = {PMLR},
            author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
            month = jul,
            year = {2021},
            note = {ISSN: 2640-3498},
            pages = {8748--8763},
        }

        @article{liang_mind_2022,
            title = {Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning},
            shorttitle = {Mind the Gap},
            url = {http://arxiv.org/abs/2203.02053},
            urldate = {2023-05-15},
            journal={arXivreprint},
            author = {Liang, Weixin and Zhang, Yuhui and Kwon, Yongchan and Yeung, Serena and Zou, James},
            month = oct,
            year = {2022},
        }

        @inproceedings{goel_cyclip_2022,
            title = {CyCLIP: Cyclic Contrastive Language-Image Pretraining},
            volume = {35},
            url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/2cd36d327f33d47b372d4711edd08de0-Paper-Conference.pdf},
            booktitle = {Advances in Neural Information Processing Systems},
            publisher = {Curran Associates, Inc.},
            author = {Goel, Shashank and Bansal, Hritik and Bhatia, Sumit and Rossi, Ryan and Vinay, Vishwa and Grover, Aditya},
            editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
            year = {2022},
            pages = {6704--6719},
        }

        @inproceedings{furst_cloob_2022,
            title = {CLOOB: Modern Hopfield Networks with InfoLOOB Outperform CLIP},
            volume = {35},
            url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/8078e76f913e31b8467e85b4c0f0d22b-Paper-Conference.pdf},
            booktitle = {Advances in Neural Information Processing Systems},
            publisher = {Curran Associates, Inc.},
            author = {Fürst, Andreas and Rumetshofer, Elisabeth and Lehner, Johannes and Tran, Viet T. and Tang, Fei and Ramsauer, Hubert and Kreil, David and Kopp, Michael and Klambauer, Günter and Bitto, Angela and Hochreiter, Sepp},
            editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
            year = {2022},
            pages = {20450--20468},
        }
        
        @inproceedings{rombach_high-resolution_2022,
            address = {New Orleans, LA, USA},
            title = {High-Resolution Image Synthesis with Latent Diffusion Models},
            isbn = {978-1-66546-946-3},
            url = {https://ieeexplore.ieee.org/document/9878449/},
            doi = {10.1109/CVPR52688.2022.01042},
            language = {en},
            urldate = {2023-04-11},
            booktitle = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
            publisher = {IEEE},
            author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bjorn},
            month = jun,
            year = {2022},
            pages = {10674--10685},
        }


        @article{wang_diffusiondb_2022,
            title = {DiffusionDB: A Large-scale Prompt Gallery Dataset for Text-to-Image Generative Models},
            shorttitle = {DiffusionDB},
            journal={arXivreprint},
            url = {http://arxiv.org/abs/2210.14896},
            abstract = {With recent advancements in diffusion models, users can generate high-quality images by writing text prompts in natural language. However, generating images with desired details requires proper prompts, and it is often unclear how a model reacts to different prompts and what the best prompts are. To help researchers tackle these critical challenges, we introduce DiffusionDB, the first large-scale text-to-image prompt dataset. DiffusionDB contains 14 million images generated by Stable Diffusion using prompts and hyperparameters specified by real users. We analyze prompts in the dataset and discuss key properties of these prompts. The unprecedented scale and diversity of this human-actuated dataset provide exciting research opportunities in understanding the interplay between prompts and generative models, detecting deepfakes, and designing human-AI interaction tools to help users more easily use these models. DiffusionDB is publicly available at: https://poloclub.github.io/diffusiondb.},
            urldate = {2023-04-11},
            publisher = {arXiv},
            author = {Wang, Zijie J. and Montoya, Evan and Munechika, David and Yang, Haoyang and Hoover, Benjamin and Chau, Duen Horng},
            month = nov,
            year = {2022},
        }


        @article{girdhar_imagebind_2023,
            title = {ImageBind: One Embedding Space To Bind Them All},
            shorttitle = {ImageBind},
            url = {http://arxiv.org/abs/2305.05665},
            abstract = {We present ImageBind, an approach to learn a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data. We show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together. ImageBind can leverage recent large scale vision-language models, and extends their zero-shot capabilities to new modalities just by using their natural pairing with images. It enables novel emergent applications 'out-of-the-box' including cross-modal retrieval, composing modalities with arithmetic, cross-modal detection and generation. The emergent capabilities improve with the strength of the image encoder and we set a new state-of-the-art on emergent zero-shot recognition tasks across modalities, outperforming specialist supervised models. Finally, we show strong few-shot recognition results outperforming prior work, and that ImageBind serves as a new way to evaluate vision models for visual and non-visual tasks.},
            urldate = {2023-05-10},
            publisher = {arXiv},
            author = {Girdhar, Rohit and El-Nouby, Alaaeldin and Liu, Zhuang and Singh, Mannat and Alwala, Kalyan Vasudev and Joulin, Armand and Misra, Ishan},
            month = may,
            year = {2023},
            journal={arXivreprint},
        }


        @article{lin_microsoft_2015,
            title = {Microsoft COCO: Common Objects in Context},
            shorttitle = {Microsoft COCO},
            url = {http://arxiv.org/abs/1405.0312},
            urldate = {2023-06-27},
            publisher = {arXiv},
            author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Dollár, Piotr},
            month = feb,
            year = {2015},
            keywords = {Computer Science - Computer Vision and Pattern Recognition},
            journal={arXivreprint},
        }

        @misc{crowson_cloob-training_2023,
            title = {cloob-training},
            copyright = {MIT},
            url = {https://github.com/crowsonkb/cloob-training},
            abstract = {CLOOB training (JAX) and inference (JAX and PyTorch)},
            urldate = {2023-06-27},
            author = {Crowson, Katherine},
            month = may,
            year = {2023},
            note = {original-date: 2022-03-24T22:33:36Z},
        }
        

        @inproceedings{timme_robustness_2020,
            address = {Budapest, Hungary},
            title = {On the Robustness of Convolutional Neural Networks Regarding Transformed Input Images},
            isbn = {978-989-758-475-6},
            shorttitle = {On the {Robustness} of {Convolutional} {Neural} {Networks} {Regarding} {Transformed} {Input} {Images}},
            url = {https://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0010107203960403},
            doi = {10.5220/0010107203960403},
            language = {en},
            urldate = {2023-06-30},
            booktitle = {Proceedings of the 12th {International} {Joint} {Conference} on {Computational} {Intelligence}},
            publisher = {SCITEPRESS - Science and Technology Publications},
            author = {Timme, Frederik and Kerdels, Jochen and Peters, Gabriele},
            year = {2020},
            pages = {396--403},
        }


        @article{eckelt_visual_2022,
            title = {Visual Exploration of Relationships and Structure in Low-Dimensional Embeddings},
            issn = {1941-0506},
            doi = {10.1109/TVCG.2022.3156760},
            journal = {IEEE Transactions on Visualization and Computer Graphics},
            author = {Eckelt, Klaus and Hinterreiter, Andreas and Adelberger, Patrick and Walchshofer, Conny and Dhanoa, Vaishali and Humer, Christina and Heckmann, Moritz and Steinparz, Christian and Streit, Marc},
            year = {2022},
            note = {Conference Name: IEEE Transactions on Visualization and Computer Graphics},
            keywords = {Visualization, Task analysis, aggregation, comparison, Data visualization, Dimensionality reduction, Layout, layout enrichment, projection, Space exploration, Trajectory, visual analytics, Visual analytics},
            pages = {1--1},
        }


        
      </script>


</body>