{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need ipykernel > 6\n",
    "# ! pip install ipykernel==6.23.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clipexplorer\n",
    "from importlib import reload\n",
    "# reload(clipexplorer.widgets)\n",
    "# reload(clipexplorer.model)\n",
    "# reload(clipexplorer.data)\n",
    "# reload(clipexplorer.utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clipexplorer import model\n",
    "from clipexplorer import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset diffusiondb (/Users/christina/.cache/huggingface/datasets/poloclub___diffusiondb/2m_first_1k/0.9.1/b3bc1e64570dc7149af62c4bac49ecfbce16b683dd4fee083292fae1afa95f7c)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0a30969a4644fbf8127ab203780bc53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dataset = data.MSCOCO_Dataset(path='/Users/christina/Data/mscoco/')\n",
    "dataset = data.DiffusionDB_Dataset(path=\"2m_first_1k\")\n",
    "# dataset = data.MSCOCO_Val_Dataset(path='/Users/christina/Data/mscoco/validation/')\n",
    "\n",
    "# all_images, all_prompts = dataset.get_data()\n",
    "# dataset = data.RandomAugmentation_Dataset(all_images[0], all_prompts[0])\n",
    "# dataset = data.Rotate_Dataset(all_images[0], all_prompts[0])\n",
    "# dataset = data.Noise_Dataset(all_images[0], all_prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'DiffusionDB_size-100'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_data_helper(dataset, filters=[], method=any):\n",
    "    all_images, all_prompts = dataset.get_filtered_data(filters, method=method)\n",
    "    print(len(all_images))\n",
    "\n",
    "    dataset_name = dataset.name\n",
    "    if len(filters) > 0:\n",
    "        dataset_name = dataset_name + '_filter-' + method.__name__ + '_' + '-'.join(filters)\n",
    "    else:\n",
    "        dataset_name = dataset_name + '_size-%i'%len(all_images)\n",
    "\n",
    "    return all_images, all_prompts, dataset_name\n",
    "\n",
    "all_images, all_prompts, dataset_name = get_data_helper(dataset, filters=[], method=any) # filters = [\"dog\"], method=all\n",
    "dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "2023-06-09 11:15:53.453025: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of the model checkpoint at Salesforce/blip-image-captioning-base were not used when initializing BlipModel: ['text_decoder.bert.encoder.layer.2.attention.self.value.weight', 'text_decoder.bert.encoder.layer.8.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.9.attention.self.query.weight', 'text_decoder.bert.encoder.layer.9.attention.self.value.weight', 'text_decoder.bert.encoder.layer.5.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.3.attention.self.query.bias', 'text_decoder.bert.encoder.layer.1.attention.self.value.weight', 'text_decoder.bert.encoder.layer.3.output.dense.weight', 'text_decoder.bert.encoder.layer.0.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.10.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.4.output.dense.weight', 'text_decoder.bert.encoder.layer.0.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.10.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.0.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.11.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.4.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.11.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.0.attention.self.key.bias', 'text_decoder.bert.encoder.layer.8.attention.self.value.bias', 'text_decoder.bert.encoder.layer.5.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.9.crossattention.self.query.bias', 'text_decoder.cls.predictions.transform.LayerNorm.weight', 'text_decoder.bert.encoder.layer.8.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.11.attention.self.value.weight', 'text_decoder.bert.encoder.layer.8.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.0.output.dense.weight', 'text_decoder.bert.encoder.layer.10.attention.self.key.bias', 'text_decoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.3.attention.self.value.weight', 'text_decoder.bert.encoder.layer.9.output.dense.weight', 'text_decoder.bert.encoder.layer.4.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.11.attention.self.query.bias', 'text_decoder.bert.encoder.layer.1.attention.self.value.bias', 'text_decoder.bert.encoder.layer.0.output.dense.bias', 'text_decoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.2.attention.self.key.bias', 'text_decoder.bert.encoder.layer.2.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.3.attention.self.query.weight', 'text_decoder.bert.encoder.layer.11.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.0.attention.self.value.bias', 'text_decoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.2.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.8.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.8.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.0.attention.self.query.weight', 'text_decoder.bert.encoder.layer.2.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.10.attention.self.query.bias', 'text_decoder.bert.encoder.layer.8.output.dense.bias', 'text_decoder.bert.encoder.layer.2.output.dense.weight', 'text_decoder.bert.encoder.layer.6.attention.self.query.bias', 'text_decoder.bert.encoder.layer.7.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.2.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.1.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.7.attention.self.query.bias', 'text_decoder.bert.encoder.layer.10.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.5.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.0.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.10.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.6.attention.self.key.bias', 'text_decoder.bert.encoder.layer.9.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.2.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.11.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.3.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.9.attention.self.key.bias', 'text_decoder.bert.encoder.layer.10.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.11.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.0.attention.self.query.bias', 'text_decoder.bert.encoder.layer.7.output.dense.bias', 'text_decoder.bert.encoder.layer.0.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.11.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.4.attention.output.dense.weight', 'text_decoder.cls.predictions.decoder.bias', 'text_decoder.bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.6.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.3.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.11.attention.self.key.weight', 'text_decoder.bert.encoder.layer.5.output.dense.weight', 'text_decoder.bert.encoder.layer.4.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.2.attention.self.key.weight', 'text_decoder.bert.encoder.layer.4.attention.self.value.weight', 'text_decoder.bert.encoder.layer.6.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.7.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.5.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.4.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.7.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.2.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.7.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.3.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'text_decoder.cls.predictions.transform.dense.weight', 'text_decoder.bert.encoder.layer.10.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.11.output.dense.weight', 'text_decoder.bert.encoder.layer.1.crossattention.output.dense.weight', 'text_decoder.bert.embeddings.LayerNorm.bias', 'text_decoder.bert.encoder.layer.4.attention.self.query.bias', 'text_decoder.bert.encoder.layer.1.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.2.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.9.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.7.crossattention.self.query.weight', 'text_decoder.cls.predictions.bias', 'text_decoder.bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.9.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.4.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.9.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.11.attention.self.key.bias', 'text_decoder.bert.encoder.layer.0.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.9.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.6.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.9.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.5.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.1.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.11.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.9.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.11.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.10.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.9.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.6.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.2.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.3.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.3.output.dense.bias', 'text_decoder.bert.encoder.layer.1.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.7.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.0.attention.self.key.weight', 'text_decoder.bert.encoder.layer.8.attention.self.key.weight', 'text_decoder.bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.4.attention.self.query.weight', 'text_decoder.bert.encoder.layer.6.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.5.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.10.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.5.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.1.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.6.attention.self.value.weight', 'text_decoder.bert.encoder.layer.11.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.5.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.10.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.2.attention.self.query.bias', 'text_decoder.bert.encoder.layer.8.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.3.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.10.attention.self.key.weight', 'text_decoder.bert.encoder.layer.5.attention.self.key.weight', 'text_decoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.3.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.3.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.3.attention.self.value.bias', 'text_decoder.bert.encoder.layer.7.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.10.attention.self.value.bias', 'text_decoder.bert.encoder.layer.4.attention.self.key.bias', 'text_decoder.bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.4.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.2.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.11.attention.self.query.weight', 'text_decoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.9.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.1.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.5.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.1.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'text_decoder.cls.predictions.transform.LayerNorm.bias', 'text_decoder.bert.embeddings.LayerNorm.weight', 'text_decoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.1.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'text_decoder.bert.embeddings.position_embeddings.weight', 'text_decoder.bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.4.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.4.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.5.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.7.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.8.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.4.attention.self.value.bias', 'text_decoder.bert.encoder.layer.6.output.dense.bias', 'text_decoder.bert.encoder.layer.5.attention.self.query.weight', 'text_decoder.bert.encoder.layer.2.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.7.attention.self.query.weight', 'text_decoder.bert.encoder.layer.9.attention.self.query.bias', 'text_decoder.bert.encoder.layer.1.output.dense.weight', 'text_decoder.bert.encoder.layer.8.attention.self.key.bias', 'text_decoder.bert.encoder.layer.3.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.6.output.dense.weight', 'text_decoder.bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.7.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.0.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.4.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.6.attention.self.value.bias', 'text_decoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.6.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.1.crossattention.output.dense.bias', 'text_decoder.bert.embeddings.position_ids', 'text_decoder.bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.6.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.9.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.1.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.7.output.dense.weight', 'text_decoder.bert.encoder.layer.6.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.3.attention.self.key.weight', 'text_decoder.bert.encoder.layer.5.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.0.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.6.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.6.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.2.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.3.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.8.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.5.attention.self.value.weight', 'text_decoder.bert.encoder.layer.8.attention.self.query.weight', 'text_decoder.bert.encoder.layer.0.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.9.attention.output.dense.bias', 'text_decoder.cls.predictions.decoder.weight', 'text_decoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.9.attention.self.value.bias', 'text_decoder.bert.encoder.layer.11.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.4.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.7.attention.self.key.bias', 'text_decoder.bert.encoder.layer.5.attention.self.key.bias', 'text_decoder.bert.encoder.layer.6.attention.self.key.weight', 'text_decoder.bert.encoder.layer.5.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.1.attention.self.query.bias', 'text_decoder.bert.encoder.layer.7.attention.self.key.weight', 'text_decoder.bert.encoder.layer.5.attention.self.query.bias', 'text_decoder.bert.encoder.layer.3.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.8.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.7.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.1.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.0.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.2.attention.self.query.weight', 'text_decoder.bert.encoder.layer.1.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.8.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.2.output.dense.bias', 'text_decoder.bert.encoder.layer.3.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.8.attention.self.query.bias', 'text_decoder.bert.encoder.layer.10.attention.self.query.weight', 'text_decoder.bert.encoder.layer.4.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.10.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.1.output.dense.bias', 'text_decoder.bert.encoder.layer.5.attention.self.value.bias', 'text_decoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.11.output.dense.bias', 'text_decoder.bert.encoder.layer.2.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.0.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.11.attention.self.value.bias', 'text_decoder.bert.encoder.layer.9.attention.self.key.weight', 'text_decoder.bert.encoder.layer.3.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.11.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.10.output.dense.bias', 'text_decoder.bert.encoder.layer.2.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.2.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.7.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.0.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.8.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.7.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.4.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.10.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.5.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.9.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.3.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.4.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.7.attention.self.value.weight', 'text_decoder.bert.encoder.layer.1.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.5.output.dense.bias', 'text_decoder.bert.encoder.layer.6.attention.self.query.weight', 'text_decoder.bert.encoder.layer.0.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.6.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.10.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.8.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.7.attention.self.value.bias', 'text_decoder.bert.encoder.layer.10.crossattention.self.value.bias', 'text_decoder.bert.embeddings.word_embeddings.weight', 'text_decoder.bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.3.attention.self.key.bias', 'text_decoder.bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.3.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.7.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.8.attention.self.value.weight', 'text_decoder.bert.encoder.layer.1.attention.self.key.bias', 'text_decoder.bert.encoder.layer.0.attention.self.value.weight', 'text_decoder.bert.encoder.layer.10.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.7.crossattention.self.value.bias', 'text_decoder.cls.predictions.transform.dense.bias', 'text_decoder.bert.encoder.layer.1.attention.self.query.weight', 'text_decoder.bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.11.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.8.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.4.attention.self.key.weight', 'text_decoder.bert.encoder.layer.6.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.6.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.10.output.dense.weight', 'text_decoder.bert.encoder.layer.5.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.11.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.4.output.dense.bias', 'text_decoder.bert.encoder.layer.8.output.dense.weight', 'text_decoder.bert.encoder.layer.0.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.10.attention.self.value.weight', 'text_decoder.bert.encoder.layer.2.attention.self.value.bias', 'text_decoder.bert.encoder.layer.9.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.8.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.1.attention.self.key.weight', 'text_decoder.bert.encoder.layer.11.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.6.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.9.output.dense.bias']\n",
      "- This IS expected if you are initializing BlipModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BlipModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BlipModel were not initialized from the model checkpoint at Salesforce/blip-image-captioning-base and are newly initialized: ['text_model.encoder.layer.11.output.dense.weight', 'text_model.encoder.layer.4.attention.self.key.weight', 'text_model.encoder.layer.9.attention.output.LayerNorm.weight', 'text_model.encoder.layer.9.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.10.attention.self.value.bias', 'text_model.encoder.layer.0.crossattention.self.key.bias', 'text_model.encoder.layer.4.crossattention.output.dense.weight', 'text_model.encoder.layer.4.attention.self.query.bias', 'text_model.encoder.layer.11.attention.self.query.weight', 'text_model.encoder.layer.8.crossattention.self.key.bias', 'text_model.encoder.layer.8.output.LayerNorm.weight', 'text_model.encoder.layer.1.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.2.attention.self.key.bias', 'text_model.encoder.layer.6.output.LayerNorm.bias', 'text_model.encoder.layer.10.attention.self.query.weight', 'text_model.encoder.layer.2.attention.self.query.weight', 'text_model.encoder.layer.3.output.LayerNorm.weight', 'text_model.encoder.layer.6.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.9.attention.self.key.weight', 'text_model.encoder.layer.9.intermediate.dense.bias', 'text_model.encoder.layer.1.crossattention.output.dense.weight', 'text_model.encoder.layer.4.attention.self.value.bias', 'text_model.encoder.layer.2.crossattention.output.dense.bias', 'text_model.encoder.layer.3.crossattention.self.key.weight', 'text_model.encoder.layer.8.attention.output.dense.bias', 'text_model.encoder.layer.4.crossattention.self.key.bias', 'text_model.embeddings.LayerNorm.bias', 'text_model.encoder.layer.4.intermediate.dense.weight', 'text_model.encoder.layer.10.crossattention.output.dense.bias', 'text_model.encoder.layer.5.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.6.attention.output.LayerNorm.weight', 'text_model.encoder.layer.4.attention.output.dense.weight', 'text_model.encoder.layer.0.attention.output.LayerNorm.bias', 'text_model.encoder.layer.3.crossattention.output.dense.weight', 'text_model.encoder.layer.4.output.LayerNorm.weight', 'text_model.encoder.layer.6.attention.self.query.weight', 'text_model.encoder.layer.7.attention.self.query.weight', 'text_model.encoder.layer.7.crossattention.self.value.bias', 'text_model.encoder.layer.6.attention.output.dense.weight', 'text_model.encoder.layer.10.attention.output.dense.bias', 'text_model.encoder.layer.7.intermediate.dense.weight', 'text_model.encoder.layer.1.attention.self.query.bias', 'text_model.encoder.layer.11.attention.output.dense.weight', 'text_model.encoder.layer.3.crossattention.self.query.weight', 'text_model.encoder.layer.0.attention.self.query.bias', 'text_model.encoder.layer.0.attention.self.query.weight', 'text_model.encoder.layer.5.attention.self.query.weight', 'text_model.encoder.layer.3.attention.self.value.weight', 'text_model.encoder.layer.5.attention.self.value.weight', 'text_model.encoder.layer.0.output.LayerNorm.weight', 'text_model.encoder.layer.3.attention.self.query.bias', 'text_model.encoder.layer.10.crossattention.output.dense.weight', 'text_model.encoder.layer.6.attention.output.LayerNorm.bias', 'text_model.encoder.layer.1.intermediate.dense.bias', 'text_model.encoder.layer.5.crossattention.self.query.bias', 'text_model.encoder.layer.2.attention.output.dense.bias', 'text_model.encoder.layer.9.crossattention.self.query.weight', 'text_model.encoder.layer.5.attention.output.dense.weight', 'text_model.encoder.layer.6.crossattention.output.dense.weight', 'text_model.encoder.layer.2.attention.self.key.weight', 'text_model.encoder.layer.4.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.1.crossattention.self.value.bias', 'text_model.encoder.layer.10.attention.self.key.bias', 'text_model.encoder.layer.10.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.1.attention.self.key.weight', 'text_model.encoder.layer.0.crossattention.self.value.bias', 'text_model.encoder.layer.10.intermediate.dense.bias', 'text_model.encoder.layer.6.attention.self.value.weight', 'text_model.encoder.layer.3.attention.self.query.weight', 'text_model.encoder.layer.3.intermediate.dense.bias', 'text_model.encoder.layer.8.crossattention.self.value.bias', 'text_model.encoder.layer.9.attention.self.query.bias', 'text_model.encoder.layer.10.output.dense.weight', 'text_model.encoder.layer.8.attention.output.LayerNorm.bias', 'text_model.encoder.layer.11.crossattention.self.value.weight', 'text_model.encoder.layer.9.crossattention.self.key.bias', 'text_model.encoder.layer.2.crossattention.self.value.weight', 'text_model.encoder.layer.3.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.11.attention.self.query.bias', 'text_model.encoder.layer.11.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.6.output.LayerNorm.weight', 'text_model.encoder.layer.5.output.LayerNorm.weight', 'text_model.encoder.layer.2.intermediate.dense.bias', 'text_model.encoder.layer.3.output.LayerNorm.bias', 'text_model.encoder.layer.8.attention.self.query.weight', 'text_model.encoder.layer.0.attention.self.value.bias', 'text_model.encoder.layer.9.attention.self.query.weight', 'text_model.encoder.layer.4.crossattention.output.dense.bias', 'text_model.encoder.layer.10.crossattention.self.value.bias', 'text_model.encoder.layer.4.output.LayerNorm.bias', 'text_model.encoder.layer.9.attention.self.key.bias', 'text_model.encoder.layer.7.attention.self.value.weight', 'text_model.encoder.layer.7.attention.self.query.bias', 'text_model.encoder.layer.1.attention.output.LayerNorm.weight', 'text_model.encoder.layer.10.crossattention.self.key.bias', 'text_model.encoder.layer.9.output.LayerNorm.weight', 'text_model.encoder.layer.10.output.LayerNorm.bias', 'text_model.encoder.layer.5.crossattention.output.dense.bias', 'text_model.encoder.layer.9.crossattention.self.query.bias', 'text_model.encoder.layer.6.attention.self.key.bias', 'text_model.encoder.layer.2.attention.self.query.bias', 'text_model.encoder.layer.0.attention.self.value.weight', 'text_model.encoder.layer.0.attention.output.dense.bias', 'text_model.encoder.layer.5.intermediate.dense.bias', 'text_model.encoder.layer.7.attention.output.dense.bias', 'text_model.encoder.layer.10.crossattention.self.value.weight', 'text_model.encoder.layer.3.attention.self.key.weight', 'text_model.encoder.layer.6.crossattention.self.value.weight', 'text_model.encoder.layer.5.crossattention.output.dense.weight', 'text_model.encoder.layer.5.output.LayerNorm.bias', 'text_model.encoder.layer.7.crossattention.output.dense.bias', 'text_model.encoder.layer.7.output.dense.bias', 'text_model.encoder.layer.10.output.LayerNorm.weight', 'text_model.encoder.layer.8.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.4.intermediate.dense.bias', 'text_model.encoder.layer.2.attention.self.value.weight', 'text_model.encoder.layer.5.crossattention.self.key.bias', 'text_model.encoder.layer.3.attention.output.LayerNorm.weight', 'text_model.encoder.layer.2.crossattention.self.key.weight', 'text_model.encoder.layer.3.output.dense.bias', 'text_model.encoder.layer.4.crossattention.self.query.weight', 'text_model.encoder.layer.8.attention.self.key.weight', 'text_model.encoder.layer.6.crossattention.self.key.weight', 'text_model.encoder.layer.11.crossattention.output.dense.bias', 'text_model.encoder.layer.2.output.dense.bias', 'text_model.encoder.layer.11.attention.self.value.bias', 'text_model.encoder.layer.10.intermediate.dense.weight', 'text_model.embeddings.LayerNorm.weight', 'text_model.encoder.layer.9.crossattention.self.value.bias', 'text_model.encoder.layer.1.output.LayerNorm.weight', 'text_model.encoder.layer.9.attention.output.dense.bias', 'text_model.encoder.layer.9.crossattention.output.dense.bias', 'text_model.encoder.layer.5.attention.self.key.weight', 'text_model.encoder.layer.8.crossattention.self.key.weight', 'text_model.encoder.layer.2.output.LayerNorm.weight', 'text_model.encoder.layer.4.crossattention.self.query.bias', 'text_model.encoder.layer.6.crossattention.self.key.bias', 'text_model.embeddings.word_embeddings.weight', 'text_model.encoder.layer.10.attention.output.LayerNorm.weight', 'text_model.encoder.layer.7.attention.output.LayerNorm.weight', 'text_model.encoder.layer.11.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.7.intermediate.dense.bias', 'text_model.encoder.layer.4.attention.output.LayerNorm.bias', 'text_model.encoder.layer.7.attention.self.value.bias', 'text_model.encoder.layer.0.attention.output.dense.weight', 'text_model.encoder.layer.1.attention.self.value.bias', 'text_model.encoder.layer.6.attention.self.key.weight', 'text_model.encoder.layer.2.intermediate.dense.weight', 'text_model.encoder.layer.11.attention.self.key.bias', 'text_model.encoder.layer.7.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.1.output.LayerNorm.bias', 'text_model.encoder.layer.8.output.LayerNorm.bias', 'text_model.encoder.layer.0.intermediate.dense.weight', 'text_model.encoder.layer.2.crossattention.self.value.bias', 'text_model.encoder.layer.0.output.dense.weight', 'text_model.encoder.layer.8.output.dense.weight', 'text_model.encoder.layer.4.attention.self.query.weight', 'text_model.encoder.layer.0.crossattention.self.key.weight', 'text_model.encoder.layer.0.crossattention.output.dense.weight', 'text_model.encoder.layer.10.crossattention.self.query.weight', 'text_model.encoder.layer.8.crossattention.self.query.bias', 'text_model.encoder.layer.7.crossattention.self.key.bias', 'text_model.encoder.layer.10.attention.self.key.weight', 'text_model.encoder.layer.3.attention.self.value.bias', 'text_model.encoder.layer.5.output.dense.bias', 'text_model.encoder.layer.10.attention.output.dense.weight', 'text_model.encoder.layer.11.output.dense.bias', 'text_model.encoder.layer.9.crossattention.self.key.weight', 'text_model.encoder.layer.8.intermediate.dense.weight', 'text_model.encoder.layer.9.crossattention.output.dense.weight', 'text_model.encoder.layer.2.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.10.attention.self.value.weight', 'text_model.encoder.layer.11.crossattention.output.dense.weight', 'text_model.encoder.layer.7.attention.output.LayerNorm.bias', 'text_model.encoder.layer.6.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.7.output.LayerNorm.bias', 'text_model.encoder.layer.1.crossattention.output.dense.bias', 'text_model.encoder.layer.6.output.dense.weight', 'text_model.encoder.layer.3.attention.output.dense.weight', 'text_model.encoder.layer.5.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.11.intermediate.dense.bias', 'text_model.encoder.layer.6.intermediate.dense.bias', 'text_model.encoder.layer.8.attention.self.query.bias', 'text_model.encoder.layer.4.attention.self.key.bias', 'text_model.encoder.layer.11.attention.output.dense.bias', 'text_model.encoder.layer.11.attention.self.value.weight', 'text_model.embeddings.position_embeddings.weight', 'text_model.encoder.layer.2.crossattention.output.dense.weight', 'text_model.encoder.layer.3.crossattention.self.value.bias', 'text_model.encoder.layer.3.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.1.attention.output.dense.weight', 'text_model.encoder.layer.6.attention.output.dense.bias', 'text_model.encoder.layer.11.crossattention.self.key.bias', 'text_model.encoder.layer.6.attention.self.value.bias', 'text_model.encoder.layer.5.attention.output.dense.bias', 'text_model.encoder.layer.6.crossattention.self.query.bias', 'text_model.encoder.layer.8.attention.output.LayerNorm.weight', 'text_model.encoder.layer.0.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.0.attention.self.key.weight', 'text_model.encoder.layer.5.output.dense.weight', 'text_model.encoder.layer.3.attention.self.key.bias', 'text_model.encoder.layer.7.attention.self.key.weight', 'text_model.encoder.layer.3.output.dense.weight', 'text_model.encoder.layer.9.intermediate.dense.weight', 'text_model.encoder.layer.5.attention.output.LayerNorm.bias', 'text_model.encoder.layer.5.crossattention.self.value.bias', 'text_model.encoder.layer.2.attention.output.dense.weight', 'text_model.encoder.layer.6.crossattention.self.value.bias', 'text_model.encoder.layer.10.crossattention.self.key.weight', 'text_model.encoder.layer.1.attention.output.LayerNorm.bias', 'text_model.encoder.layer.4.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.0.attention.self.key.bias', 'text_model.encoder.layer.0.crossattention.output.dense.bias', 'text_model.encoder.layer.8.crossattention.self.query.weight', 'text_model.encoder.layer.11.crossattention.self.query.weight', 'text_model.encoder.layer.1.crossattention.self.query.weight', 'text_model.encoder.layer.6.attention.self.query.bias', 'text_model.encoder.layer.2.crossattention.self.key.bias', 'text_model.encoder.layer.5.attention.self.query.bias', 'text_model.encoder.layer.7.attention.output.dense.weight', 'text_model.encoder.layer.0.crossattention.self.value.weight', 'text_model.encoder.layer.9.attention.output.dense.weight', 'text_model.encoder.layer.7.crossattention.self.key.weight', 'text_model.encoder.layer.8.attention.output.dense.weight', 'text_model.encoder.layer.11.crossattention.self.query.bias', 'text_model.encoder.layer.0.crossattention.self.query.weight', 'text_model.encoder.layer.7.crossattention.self.query.weight', 'text_model.encoder.layer.4.output.dense.bias', 'text_model.encoder.layer.9.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.10.attention.output.LayerNorm.bias', 'logit_scale', 'text_model.encoder.layer.9.attention.output.LayerNorm.bias', 'text_model.encoder.layer.11.attention.output.LayerNorm.bias', 'text_model.encoder.layer.7.crossattention.self.value.weight', 'text_model.encoder.layer.1.attention.output.dense.bias', 'text_model.encoder.layer.10.output.dense.bias', 'text_model.encoder.layer.8.attention.self.value.bias', 'text_model.encoder.layer.3.crossattention.self.key.bias', 'text_model.encoder.layer.6.output.dense.bias', 'text_model.encoder.layer.8.attention.self.key.bias', 'text_model.encoder.layer.2.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.10.crossattention.self.query.bias', 'text_model.encoder.layer.5.crossattention.self.query.weight', 'text_model.encoder.layer.8.intermediate.dense.bias', 'text_model.encoder.layer.1.output.dense.bias', 'text_model.encoder.layer.5.crossattention.self.key.weight', 'text_model.encoder.layer.1.crossattention.self.value.weight', 'text_model.encoder.layer.5.intermediate.dense.weight', 'text_model.encoder.layer.7.output.LayerNorm.weight', 'text_model.encoder.layer.8.crossattention.output.LayerNorm.bias', 'text_model.pooler.dense.weight', 'text_model.encoder.layer.11.output.LayerNorm.weight', 'text_model.encoder.layer.9.crossattention.self.value.weight', 'text_model.encoder.layer.0.crossattention.self.query.bias', 'text_model.encoder.layer.6.crossattention.self.query.weight', 'text_model.encoder.layer.11.attention.self.key.weight', 'text_model.encoder.layer.7.crossattention.output.dense.weight', 'text_model.encoder.layer.7.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.9.output.LayerNorm.bias', 'text_model.pooler.dense.bias', 'text_model.encoder.layer.10.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.1.output.dense.weight', 'text_model.encoder.layer.9.output.dense.bias', 'text_model.encoder.layer.0.output.LayerNorm.bias', 'text_model.encoder.layer.8.output.dense.bias', 'text_model.encoder.layer.3.attention.output.dense.bias', 'text_model.encoder.layer.2.crossattention.self.query.weight', 'text_model.encoder.layer.2.attention.output.LayerNorm.bias', 'text_model.encoder.layer.9.attention.self.value.weight', 'text_model.encoder.layer.10.attention.self.query.bias', 'text_model.encoder.layer.4.crossattention.self.key.weight', 'text_model.encoder.layer.5.attention.self.key.bias', 'text_model.encoder.layer.0.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.1.intermediate.dense.weight', 'text_model.encoder.layer.4.attention.self.value.weight', 'text_model.encoder.layer.7.attention.self.key.bias', 'text_model.encoder.layer.3.attention.output.LayerNorm.bias', 'text_model.encoder.layer.3.crossattention.self.value.weight', 'text_model.encoder.layer.2.attention.output.LayerNorm.weight', 'text_model.encoder.layer.0.intermediate.dense.bias', 'text_model.encoder.layer.8.crossattention.output.dense.bias', 'text_projection.weight', 'text_model.encoder.layer.8.crossattention.output.dense.weight', 'text_model.encoder.layer.4.output.dense.weight', 'text_model.encoder.layer.2.output.LayerNorm.bias', 'text_model.encoder.layer.9.attention.self.value.bias', 'text_model.encoder.layer.8.crossattention.self.value.weight', 'text_model.encoder.layer.3.crossattention.self.query.bias', 'text_model.encoder.layer.11.attention.output.LayerNorm.weight', 'text_model.encoder.layer.2.attention.self.value.bias', 'text_model.encoder.layer.7.crossattention.self.query.bias', 'text_model.encoder.layer.6.intermediate.dense.weight', 'text_model.encoder.layer.11.crossattention.self.value.bias', 'text_model.encoder.layer.2.crossattention.self.query.bias', 'text_model.encoder.layer.3.crossattention.output.dense.bias', 'text_model.encoder.layer.8.attention.self.value.weight', 'text_model.encoder.layer.11.output.LayerNorm.bias', 'text_model.encoder.layer.9.output.dense.weight', 'text_model.encoder.layer.1.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.0.output.dense.bias', 'text_model.encoder.layer.1.attention.self.key.bias', 'text_model.encoder.layer.4.crossattention.self.value.weight', 'text_model.encoder.layer.1.crossattention.self.key.weight', 'text_model.encoder.layer.1.attention.self.value.weight', 'text_model.encoder.layer.2.output.dense.weight', 'text_model.encoder.layer.5.attention.output.LayerNorm.weight', 'text_model.encoder.layer.0.attention.output.LayerNorm.weight', 'text_model.encoder.layer.4.attention.output.dense.bias', 'text_model.encoder.layer.5.attention.self.value.bias', 'visual_projection.weight', 'text_model.encoder.layer.1.attention.self.query.weight', 'text_model.encoder.layer.6.crossattention.output.dense.bias', 'text_model.encoder.layer.11.intermediate.dense.weight', 'text_model.encoder.layer.1.crossattention.self.query.bias', 'text_model.encoder.layer.1.crossattention.self.key.bias', 'text_model.encoder.layer.4.attention.output.LayerNorm.weight', 'text_model.encoder.layer.11.crossattention.self.key.weight', 'text_model.encoder.layer.5.crossattention.self.value.weight', 'text_model.encoder.layer.3.intermediate.dense.weight', 'text_model.encoder.layer.4.crossattention.self.value.bias', 'text_model.encoder.layer.7.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages/clipexplorer/CLOOB_local/training/model_configs/RN50.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages/clipexplorer/widgets.py:356: FutureWarning: The input object of type 'PngImageFile' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'PngImageFile', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  self.all_images = np.array(all_images)\n",
      "/Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages/clipexplorer/widgets.py:356: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.all_images = np.array(all_images)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found cached embeddings for DiffusionDB_size-100_CLIP_RN50\n",
      "Modality distance: 0.72 | Loss: 0.36\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa7cf33148ed432aa92b61e899ad72c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CLIPExplorerWidget(children=(VBox(children=(HBox(children=(Dropdown(description='Model: ', options=('CLIP', 'O…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from clipexplorer.widgets import CLIPExplorerWidget\n",
    "\n",
    "\n",
    "clipexplorer_widget = CLIPExplorerWidget(dataset_name, all_images, all_prompts)\n",
    "clipexplorer_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79fa93c85c4246b384109c15879b8969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ScatterPlotWidget(children=(Dropdown(description='Method: ', options=('PCA', 'TSNE', 'UMAP'), value='PCA'), Ch…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clipexplorer_widget.scatter_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clipexplorer import model\n",
    "model.available_CLIP_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clipexplorer.widgets import CLIPComparerWidget\n",
    "clip_comparer = CLIPComparerWidget(dataset_name, all_images, all_prompts, models=['CLIP', 'CyCLIP', 'CLOOB', 'CLOOB_LAION400M'])\n",
    "clip_comparer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clipexplorer.widgets import CLIPComparerWidget\n",
    "clip_comparer = CLIPComparerWidget(dataset_name, all_images, all_prompts, models=['CLIP', 'CyCLIP', 'CLOOB', 'CLOOB_LAION400M'], close_modality_gap=True)\n",
    "clip_comparer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot similarities and gap-closed similarities next to each other\n",
    "from clipexplorer.widgets import CLIPComparerWidget\n",
    "clip_comparer = CLIPComparerWidget(dataset_name, all_images, all_prompts, models=['CLIP', 'CLIP', 'CyCLIP', 'CyCLIP', 'CLOOB', 'CLOOB', 'CLOOB_LAION400M', 'CLOOB_LAION400M'], close_modality_gap=[False, True]*4)\n",
    "clip_comparer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate metrics for larger amounts of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clipexplorer import data\n",
    "from clipexplorer.utils import get_embedding, get_modality_distance, calculate_val_loss, get_closed_modality_gap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = data.MSCOCO_Dataset(path='/Users/christina/Data/mscoco/')\n",
    "# dataset = data.DiffusionDB_Dataset(path=\"2m_first_10k\", batch_size=None)\n",
    "\n",
    "all_images, all_prompts, dataset_name = get_data_helper(dataset, filters=[], method=any) # filters = [\"dog\"], method=all\n",
    "dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clip_model in ['CLIP', 'OpenCLIP', 'CyCLIP', 'CLOOB', 'CLOOB_LAION400M']:\n",
    "    print('---model: %s---'%clip_model)\n",
    "    image_embedding, text_embedding, logit_scaling = get_embedding(clip_model, dataset_name, all_images, all_prompts)\n",
    "    modality_distance = get_modality_distance(image_embedding, text_embedding)\n",
    "    val_loss = calculate_val_loss(image_embedding, text_embedding, logit_scaling.exp())\n",
    "    print('modality distance: %.2f | validation loss: %.2f'%(modality_distance, val_loss))\n",
    "\n",
    "    image_embedding, text_embedding = get_closed_modality_gap(image_embedding, text_embedding)\n",
    "    modified_modality_distance = get_modality_distance(image_embedding, text_embedding)\n",
    "    modified_val_loss = calculate_val_loss(image_embedding, text_embedding, logit_scaling.exp())\n",
    "    print('modified modality distance: %.2f | modified validation loss: %.2f'%(modified_modality_distance, modified_val_loss))\n",
    "\n",
    "    print('loss difference:', modified_val_loss-val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.MSCOCO_Dataset(path='/Users/christina/Data/mscoco/', batch_size=10000)\n",
    "# dataset = data.DiffusionDB_Dataset(path=\"2m_first_10k\", batch_size=None)\n",
    "all_images, all_prompts, dataset_name = get_data_helper(dataset, filters=[], method=any) # filters = [\"dog\"], method=all\n",
    "dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clip_model in ['CLIP', 'OpenCLIP', 'CyCLIP', 'CLOOB', 'CLOOB_LAION400M']:\n",
    "    print('---model: %s---'%clip_model)\n",
    "    image_embedding, text_embedding, logit_scaling = get_embedding(clip_model, dataset_name, all_images, all_prompts)\n",
    "    modality_distance = get_modality_distance(image_embedding, text_embedding)\n",
    "    val_loss = calculate_val_loss(image_embedding, text_embedding, logit_scaling.exp())\n",
    "    print('modality distance: %.2f | validation loss: %.2f'%(modality_distance, val_loss))\n",
    "\n",
    "    image_embedding, text_embedding = get_closed_modality_gap(image_embedding, text_embedding)\n",
    "    modified_modality_distance = get_modality_distance(image_embedding, text_embedding)\n",
    "    modified_val_loss = calculate_val_loss(image_embedding, text_embedding, logit_scaling.exp())\n",
    "    print('modified modality distance: %.2f | modified validation loss: %.2f'%(modified_modality_distance, modified_val_loss))\n",
    "\n",
    "    print('loss difference:', modified_val_loss-val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.MSCOCO_Val_Dataset(path='/Users/christina/Data/mscoco/validation/', batch_size=5000)\n",
    "all_images, all_prompts, dataset_name = get_data_helper(dataset, filters=[], method=any) # filters = [\"dog\"], method=all\n",
    "dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clip_model in ['CLIP', 'OpenCLIP', 'CyCLIP', 'CLOOB', 'CLOOB_LAION400M']:\n",
    "    print('---model: %s---'%clip_model)\n",
    "    image_embedding, text_embedding, logit_scaling = get_embedding(clip_model, dataset_name, all_images, all_prompts)\n",
    "    modality_distance = get_modality_distance(image_embedding, text_embedding)\n",
    "    val_loss = calculate_val_loss(image_embedding, text_embedding, logit_scaling.exp())\n",
    "    print('modality distance: %.2f | validation loss: %.2f'%(modality_distance, val_loss))\n",
    "\n",
    "    image_embedding, text_embedding = get_closed_modality_gap(image_embedding, text_embedding)\n",
    "    modified_modality_distance = get_modality_distance(image_embedding, text_embedding)\n",
    "    modified_val_loss = calculate_val_loss(image_embedding, text_embedding, logit_scaling.exp())\n",
    "    print('modified modality distance: %.2f | modified validation loss: %.2f'%(modified_modality_distance, modified_val_loss))\n",
    "\n",
    "    print('loss difference:', modified_val_loss-val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \n",
    "raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n",
    "\n",
    "# conditional image captioning\n",
    "text = \"a photography of\"\n",
    "inputs = processor(raw_image, text, return_tensors=\"pt\")\n",
    "\n",
    "out = model.generate(**inputs)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))\n",
    "\n",
    "# unconditional image captioning\n",
    "inputs = processor(raw_image, return_tensors=\"pt\")\n",
    "\n",
    "out = model.generate(**inputs)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[raw_image, raw_image]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BlipImageProcessor, BlipModel, BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BlipModel.from_pretrained(\"Salesforce/blip-image-captioning-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = BlipImageProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('Salesforce/blip-image-captioning-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = processor(raw_image, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "processor(list(np.array([raw_image, raw_image])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input['pixel_values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(['a photography of a woman and her dog on the beach', \"and her dog on the beach\"], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_image_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_image_features(**input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_text_features(**tokens).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import AutoProcessor, BlipModel\n",
    "\n",
    "model = BlipModel.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "inputs = processor(\n",
    "    text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True\n",
    ")\n",
    "\n",
    "outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.image_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.text_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
