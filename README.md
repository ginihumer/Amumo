# Understanding and Comparing Multi-Modal Models
## Exploring the Latent Space of CLIP-like Models (CLIP, CyCLIP, CLOOB) Using Inter-Modal Pairs (Featuring Amumo, Your Friendly Neighborhood Mummy)


Interactive article submitted to the [6th Workshop on Visualization for AI Explainability (VISxAI 2023)](https://visxai.io/): https://jku-vds-lab.at/amumo 

### Abstract
(Contrastive Language Image Pre-training (CLIP))[https://proceedings.mlr.press/v139/radford21a.html] and variations of this approach like (CyCLIP)[https://proceedings.neurips.cc/paper_files/paper/2022/file/2cd36d327f33d47b372d4711edd08de0-Paper-Conference.pdf], or (CLOOB)[https://proceedings.neurips.cc/paper_files/paper/2022/file/8078e76f913e31b8467e85b4c0f0d22b-Paper-Conference.pdf] are trained on image-text pairs with a contrastive objective. The goal of contrastive loss objectives is to minimize latent-space distances of data points that have the same underlying meaning. We refer to the particular cases of contrastive learning that CLIP-like models perform as multi-modal contrastive learning because they use two (or (more)[https://arxiv.org/pdf/2305.05665.pdf]) modes of data (e.g., images and texts) where each mode uses their own encoder to generate a latent embedding space. More specifically, the objective that CLIP is optimized for minimizes the distances between image-text embeddings of pairs that have the same semantic meaning while maximizing the distances to all other combinations of text and image embeddings.
We would expect that such a shared latent space places similar concepts of images and texts close to each other. However, the reality is a bit more complicated...