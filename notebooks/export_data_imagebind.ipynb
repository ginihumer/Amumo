{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Data for ImageBind Analysis\n",
    "### This notebook exports data for the ImageBind analysis done after the interactive article was accepted by VISxAI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install git+https://github.com/ginihumer/Amumo.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import amumo\n",
    "from amumo import data as am_data\n",
    "from amumo import utils as am_utils\n",
    "from amumo import model as am_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def create_dir_if_not_exists(dir):\n",
    "    if not os.path.exists(dir):\n",
    "        os.mkdir(dir)\n",
    "    return dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./exported_data_checkpoints/'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export_directory = './exported_data_checkpoints/'\n",
    "create_dir_if_not_exists(export_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text-Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def export_data(dataset_name, images, prompts, models):\n",
    "\n",
    "    # create folder structure\n",
    "    dataset_directory = create_dir_if_not_exists(export_directory + dataset_name)\n",
    "    similarities_dir = create_dir_if_not_exists(dataset_directory + '/similarities')\n",
    "\n",
    "    # export projections and similarities\n",
    "    import torch\n",
    "    from sklearn.decomposition import PCA\n",
    "    from openTSNE import TSNE\n",
    "    from umap import UMAP\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import json\n",
    "\n",
    "    # if there already exists a dataset with projections from prior exports, load it\n",
    "    if not os.path.exists(dataset_directory + '/projections.csv'):\n",
    "        projections_df = pd.DataFrame({'emb_id': list(np.arange(0,len(images),1))+list(np.arange(0,len(prompts),1)), 'data_type':['image']*len(images)+['text']*len(prompts)})\n",
    "    else:\n",
    "        projections_df = pd.read_csv(dataset_directory + '/projections.csv')\n",
    "    \n",
    "\n",
    "    for model in models:\n",
    "        # compute embeddings\n",
    "        image_embedding_gap, text_embedding_gap, logit_scale = am_utils.get_embedding(model, dataset_name, images, prompts)\n",
    "        image_embedding_nogap, text_embedding_nogap = am_utils.get_closed_modality_gap(image_embedding_gap, text_embedding_gap)\n",
    "        \n",
    "        for image_embedding, text_embedding, mode in [(image_embedding_gap, text_embedding_gap, ''), (image_embedding_nogap, text_embedding_nogap, '_nogap')]:\n",
    "            \n",
    "            # compute similarities\n",
    "            similarity_image_text, similarity = am_utils.get_similarity(image_embedding, text_embedding)\n",
    "            np.savetxt('%s/%s%s.csv'%(similarities_dir,model.model_name,mode), similarity, delimiter=',')\n",
    "            \n",
    "            # compute meta information and similarity clustering\n",
    "            meta_info = {}\n",
    "            meta_info['gap_distance'] = float(am_utils.get_modality_distance(image_embedding, text_embedding))\n",
    "            meta_info['loss'] = float(am_utils.calculate_val_loss(image_embedding, text_embedding, logit_scale.exp()))\n",
    "\n",
    "            idcs, clusters, clusters_unsorted = am_utils.get_cluster_sorting(similarity_image_text)\n",
    "            cluster_labels = []\n",
    "            cluster_sizes = []\n",
    "            for c in set(clusters):\n",
    "                cluster_size = int(np.count_nonzero(clusters==c))\n",
    "                cluster_label = am_utils.get_textual_label_for_cluster(np.where(clusters_unsorted==c)[0], prompts)\n",
    "                cluster_labels.append(cluster_label)\n",
    "                cluster_sizes.append(cluster_size)\n",
    "\n",
    "            idcs_reverse = np.argsort(idcs)\n",
    "            meta_info['cluster_sort_idcs'] = idcs.tolist()\n",
    "            meta_info['cluster_sort_idcs_reverse'] = idcs_reverse.tolist()\n",
    "            meta_info['cluster_sizes'] = cluster_sizes\n",
    "            meta_info['cluster_labels'] = cluster_labels\n",
    "            # print(meta_info)\n",
    "\n",
    "            with open(\"%s/%s%s_meta_info.json\"%(similarities_dir, model.model_name, mode), \"w\") as file:\n",
    "                json.dump(meta_info, file)\n",
    "\n",
    "            # compute projections\n",
    "            embedding = np.array(torch.concatenate([image_embedding, text_embedding]))\n",
    "\n",
    "            projection_methods = {\n",
    "                'PCA': PCA,\n",
    "                'UMAP': UMAP,\n",
    "                'TSNE': TSNE\n",
    "            }\n",
    "            for method in projection_methods.keys():\n",
    "                if method == 'PCA':\n",
    "                    proj = projection_methods[method](n_components=2)\n",
    "                else:\n",
    "                    proj = projection_methods[method](n_components=2, metric='cosine', random_state=31415)\n",
    "                \n",
    "                if method == 'TSNE':\n",
    "                    low_dim_data = proj.fit(embedding)\n",
    "                else:\n",
    "                    low_dim_data = proj.fit_transform(embedding)\n",
    "                \n",
    "                projections_df['%s%s_%s_x'%(model.model_name, mode, method)] = low_dim_data[:,0]\n",
    "                projections_df['%s%s_%s_y'%(model.model_name, mode, method)] = low_dim_data[:,1]\n",
    "\n",
    "\n",
    "    projections_df.to_csv(dataset_directory + '/projections.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_31664\\330881050.py:20: FutureWarning: The input object of type 'Image' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Image', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  self.all_images = np.array(all_images)\n",
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_31664\\330881050.py:20: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.all_images = np.array(all_images)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# reuse mscoco subset from previous analysis\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "class Custom_Dataset(am_data.DatasetInterface):\n",
    "    name = 'MSCOCO-Val'\n",
    "\n",
    "    def __init__(self, path, seed=54, batch_size=None):\n",
    "        # create triplet dataset if it does not exist\n",
    "        super().__init__(path, seed, batch_size)\n",
    "        # path: path to the triplet dataset\n",
    "        image_paths = [path + \"images/%i.jpg\"%i for i in range(100)]\n",
    "\n",
    "        all_images = []\n",
    "        for image_path in image_paths:\n",
    "            with open(image_path, \"rb\") as fopen:\n",
    "                image = Image.open(fopen).convert(\"RGB\")\n",
    "                all_images.append(image)\n",
    "\n",
    "        self.all_images = np.array(all_images)\n",
    "        \n",
    "        with open(path + \"/prompts.txt\", \"r\") as file:\n",
    "            self.all_prompts = file.read().splitlines()\n",
    "\n",
    "mscoco_val_dataset_name = \"MSCOCO-Val_size-100\"\n",
    "dataset_mscoco_val = Custom_Dataset(export_directory + mscoco_val_dataset_name + '/')\n",
    "mscoco_val_images, mscoco_val_prompts = dataset_mscoco_val.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found cached embeddings for MSCOCO-Val_size-100_ImageBind_huge\n"
     ]
    }
   ],
   "source": [
    "export_data(mscoco_val_dataset_name, mscoco_val_images, mscoco_val_prompts, [am_model.ImageBind_Model()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text-Image-Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_data(dataset_name, images, prompts, audios, infos, models):\n",
    "\n",
    "    # create folder structure\n",
    "    dataset_directory = create_dir_if_not_exists(export_directory + dataset_name)\n",
    "    images_dir = create_dir_if_not_exists(dataset_directory + '/images')\n",
    "    audios_dir = create_dir_if_not_exists(dataset_directory + '/audios')\n",
    "    similarities_dir = create_dir_if_not_exists(dataset_directory + '/similarities')\n",
    "\n",
    "    # save images\n",
    "    for i in range(len(images)):\n",
    "        im = images[i]\n",
    "        im.resize((400,400))\n",
    "        im.save('%s/%i.jpg'%(images_dir,i))\n",
    "        \n",
    "    # save audios\n",
    "    for i in range(len(audios)):\n",
    "        import soundfile as sf\n",
    "        sf.write('%s/%i.wav'%(audios_dir,i), audios[i][0], audios.sample_rate, format=\"wav\")\n",
    "\n",
    "    # save texts\n",
    "    with open(dataset_directory + \"/prompts.txt\", \"w\") as file:\n",
    "        for prompt in prompts:\n",
    "            file.write(prompt + \"\\n\")\n",
    "\n",
    "    # save infos about youtube source and labels\n",
    "    infos.to_csv(dataset_directory + \"/infos.csv\")\n",
    "\n",
    "    # export projections and similarities\n",
    "    import torch\n",
    "    from sklearn.decomposition import PCA\n",
    "    from openTSNE import TSNE\n",
    "    from umap import UMAP\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import json\n",
    "\n",
    "    projections_df = pd.DataFrame({'emb_id': list(np.arange(0,len(images),1))+list(np.arange(0,len(prompts),1))+list(np.arange(0,len(audios),1)), 'data_type':['image']*len(images)+['text']*len(prompts)+['audio']*len(prompts)})\n",
    "\n",
    "\n",
    "    for model in models:\n",
    "        # compute embeddings\n",
    "        embeddings, logit_scale = am_utils.get_embeddings_per_modality(model, dataset_name, {\"image\": images, \"text\": prompts, \"audio\": audios})\n",
    "        \n",
    "        # compute similarities\n",
    "        similarity = am_utils.get_similarities_all(embeddings)\n",
    "        np.savetxt('%s/%s.csv'%(similarities_dir,model.model_name), similarity, delimiter=',')\n",
    "        \n",
    "        # compute meta information and similarity clustering\n",
    "        # compute gap_distances and losses for each modality pair\n",
    "        gap_distances = {}\n",
    "        losses = {}\n",
    "        for modality_1, modality_2 in [(\"audio\", \"image\"), (\"text\", \"image\"), (\"text\", \"audio\")]:\n",
    "            gap_distances['%s_%s'%(modality_1, modality_2)] = float(am_utils.get_modality_distance(embeddings[modality_1], embeddings[modality_2]))\n",
    "            losses['%s_%s'%(modality_1, modality_2)] = float(am_utils.calculate_val_loss(embeddings[modality_1], embeddings[modality_2], logit_scale.exp()))\n",
    "            \n",
    "        meta_info = {}\n",
    "        meta_info['gap_distance'] = gap_distances\n",
    "        meta_info['loss'] = losses\n",
    "\n",
    "        # compute cluster sorting for modality pair (including in-modal pairs)\n",
    "        all_clusters = {}\n",
    "        import itertools\n",
    "        for modality_1, modality_2 in itertools.product([\"audio\", \"image\", \"text\"], [\"audio\", \"image\", \"text\"]):\n",
    "            similarity_by_modalities = am_utils.get_similarities(torch.from_numpy(embeddings[modality_1]), torch.from_numpy(embeddings[modality_2]))\n",
    "            idcs, clusters, clusters_unsorted = am_utils.get_cluster_sorting(similarity_by_modalities)\n",
    "            cluster_labels = []\n",
    "            cluster_sizes = []\n",
    "            for c in set(clusters):\n",
    "                cluster_size = int(np.count_nonzero(clusters==c))\n",
    "                cluster_label = am_utils.get_textual_label_for_cluster(np.where(clusters_unsorted==c)[0], prompts)\n",
    "                cluster_labels.append(cluster_label)\n",
    "                cluster_sizes.append(cluster_size)\n",
    "\n",
    "            idcs_reverse = np.argsort(idcs)\n",
    "            cluster_dict = {}\n",
    "            cluster_dict['cluster_sort_idcs'] = idcs.tolist()\n",
    "            cluster_dict['cluster_sort_idcs_reverse'] = idcs_reverse.tolist()\n",
    "            cluster_dict['cluster_sizes'] = cluster_sizes\n",
    "            cluster_dict['cluster_labels'] = cluster_labels\n",
    "            all_clusters['%s_%s'%(modality_1, modality_2)] = cluster_dict\n",
    "\n",
    "        meta_info['clusters'] = all_clusters\n",
    "        print(meta_info)\n",
    "\n",
    "        with open(\"%s/%s_meta_info.json\"%(similarities_dir, model.model_name), \"w\") as file:\n",
    "            json.dump(meta_info, file)\n",
    "\n",
    "        # compute projections\n",
    "        embedding = np.concatenate(list(embeddings.values()))\n",
    "\n",
    "        projection_methods = {\n",
    "            'PCA': PCA,\n",
    "            'UMAP': UMAP,\n",
    "            'TSNE': TSNE\n",
    "        }\n",
    "        for method in projection_methods.keys():\n",
    "            if method == 'PCA':\n",
    "                proj = projection_methods[method](n_components=2)\n",
    "            else:\n",
    "                proj = projection_methods[method](n_components=2, metric='cosine', random_state=31415)\n",
    "            \n",
    "            if method == 'TSNE':\n",
    "                low_dim_data = proj.fit(embedding)\n",
    "            else:\n",
    "                low_dim_data = proj.fit_transform(embedding)\n",
    "            \n",
    "            projections_df['%s_%s_x'%(model.model_name, method)] = low_dim_data[:,0]\n",
    "            projections_df['%s_%s_y'%(model.model_name, method)] = low_dim_data[:,1]\n",
    "\n",
    "\n",
    "    projections_df.to_csv(dataset_directory + '/projections.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "\n",
    "class Triplet_Dataset(am_data.DatasetInterface):\n",
    "    name='Triplet'\n",
    "\n",
    "    def __init__(self, path, seed=31415, batch_size=100, sample_rate=16000):\n",
    "        # create triplet dataset if it does not exist\n",
    "        super().__init__(path, seed, batch_size)\n",
    "        # path: path to the triplet dataset\n",
    "        image_paths = glob(path + \"image/*.jpg\", recursive = True)\n",
    "        audio_paths = glob(path + \"audio/*.wav\", recursive = True)\n",
    "\n",
    "        self.sample_rate = sample_rate\n",
    "        \n",
    "        all_images = []\n",
    "        for image_path in image_paths:\n",
    "            with open(image_path, \"rb\") as fopen:\n",
    "                image = Image.open(fopen).convert(\"RGB\")\n",
    "                all_images.append(image)\n",
    "\n",
    "        all_audios = []\n",
    "        for audio_path in audio_paths:\n",
    "            waveform, sr = torchaudio.load(audio_path)\n",
    "            if sample_rate != sr:\n",
    "                waveform = torchaudio.functional.resample(\n",
    "                    waveform, orig_freq=sr, new_freq=sample_rate\n",
    "                )\n",
    "            all_audios.append(waveform)\n",
    "        \n",
    "        self.all_infos = pd.read_csv(path + \"info.csv\", converters={\"labels\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \")})\n",
    "\n",
    "        # TODO... load on demand with a custom loader\n",
    "        self.all_images = np.array(all_images)\n",
    "        self.all_prompts = np.array(self.all_infos[\"labels\"].map(lambda x: \", \".join(x)))\n",
    "        self.all_audios = np.array(all_audios)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def get_data(self):\n",
    "        # create a random batch\n",
    "        batch_idcs = self._get_random_subsample(len(self.all_images))\n",
    "\n",
    "        images = self.MODE1_Type(self.all_images[batch_idcs])\n",
    "        texts = self.MODE2_Type(self.all_prompts[batch_idcs])\n",
    "        audios = am_data.AudioType(self.all_audios[batch_idcs], self.sample_rate)\n",
    "        \n",
    "        return images, texts, audios, self.all_infos.iloc[batch_idcs].reset_index(drop=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_21528\\273827369.py:37: FutureWarning: The input object of type 'Image' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Image', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  self.all_images = np.array(all_images)\n",
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_21528\\273827369.py:37: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.all_images = np.array(all_images)\n",
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_21528\\273827369.py:39: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  self.all_audios = np.array(all_audios)\n",
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_21528\\273827369.py:39: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.all_audios = np.array(all_audios)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100\n",
      "batch 1 of 1\n",
      "batch 1 of 1\n",
      "batch 1 of 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\christina\\repositories\\icg\\researchstay\\amumo\\amumo\\utils.py:126: ClusterWarning: The symmetric non-negative hollow observation matrix looks suspiciously like an uncondensed distance matrix\n",
      "  linkage = sch.linkage(1-similarity, method='complete')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gap_distance': {'audio_image': 0.6844899275524431, 'text_image': 0.876292816992076, 'text_audio': 0.7458446635745786}, 'loss': {'audio_image': 3.7037110545737204, 'text_image': 3.815758442215615, 'text_audio': 3.805993391119964}, 'clusters': {'audio_audio': {'cluster_sort_idcs': [52, 2, 89, 15, 77, 33, 67, 79, 80, 85, 43, 22, 81, 27, 75, 36, 23, 18, 40, 51, 84, 94, 71, 24, 57, 88, 11, 19, 44, 69, 98, 42, 53, 54, 70, 66, 74, 72, 73, 0, 38, 5, 6, 7, 93, 9, 39, 13, 92, 17, 87, 25, 26, 29, 61, 76, 14, 60, 95, 97, 65, 49, 56, 55, 20, 50, 47, 28, 30, 35, 62, 4, 12, 16, 86, 34, 31, 41, 37, 68, 46, 58, 45, 83, 21, 3, 8, 82, 1, 96, 63, 10, 90, 64, 78, 32, 48, 59, 91, 99], 'cluster_sort_idcs_reverse': [39, 88, 1, 85, 71, 41, 42, 43, 86, 45, 91, 26, 72, 47, 56, 3, 73, 49, 17, 27, 64, 84, 11, 16, 23, 51, 52, 13, 67, 53, 68, 76, 95, 5, 75, 69, 15, 78, 40, 46, 18, 77, 31, 10, 28, 82, 80, 66, 96, 61, 65, 19, 0, 32, 33, 63, 62, 24, 81, 97, 57, 54, 70, 90, 93, 60, 35, 6, 79, 29, 34, 22, 37, 38, 36, 14, 55, 4, 94, 7, 8, 12, 87, 83, 20, 9, 74, 50, 25, 2, 92, 98, 48, 44, 21, 58, 89, 59, 30, 99], 'cluster_sizes': [3, 9, 8, 4, 4, 30, 12, 13, 4, 13], 'cluster_labels': ['dog | animal', 'horse | animal', 'horse | bird', 'music | speech animal', 'dog | domestic animals pets', 'domestic animals pets | dog', 'animal | bird', 'bird | bird bird', 'bird | bird bird', 'bird | bird bird']}, 'audio_image': {'cluster_sort_idcs': [24, 23, 94, 88, 60, 58, 84, 42, 67, 65, 52, 48, 43, 89, 35, 79, 96, 2, 10, 1, 64, 63, 27, 21, 78, 56, 85, 40, 51, 90, 22, 32, 80, 15, 18, 36, 77, 81, 75, 50, 99, 91, 71, 33, 4, 82, 55, 86, 97, 59, 68, 46, 45, 83, 8, 37, 31, 62, 70, 69, 74, 16, 3, 14, 20, 95, 12, 92, 26, 61, 28, 41, 9, 87, 7, 98, 5, 34, 47, 93, 0, 73, 72, 57, 54, 53, 11, 39, 38, 76, 29, 25, 19, 17, 13, 6, 30, 66, 44, 49], 'cluster_sort_idcs_reverse': [80, 19, 17, 62, 44, 76, 95, 74, 54, 72, 18, 86, 66, 94, 63, 33, 61, 93, 34, 92, 64, 23, 30, 1, 0, 91, 68, 22, 70, 90, 96, 56, 31, 43, 77, 14, 35, 55, 88, 87, 27, 71, 7, 12, 98, 52, 51, 78, 11, 99, 39, 28, 10, 85, 84, 46, 25, 83, 5, 49, 4, 69, 57, 21, 20, 9, 97, 8, 50, 59, 58, 42, 82, 81, 60, 38, 89, 36, 24, 15, 32, 37, 45, 53, 6, 26, 47, 73, 3, 13, 29, 41, 67, 79, 2, 65, 16, 48, 75, 40], 'cluster_sizes': [3, 5, 10, 7, 14, 5, 14, 21, 16, 5], 'cluster_labels': ['music | horse music neigh whinny speech animal', 'domestic animals pets | cat domestic animals pets meow', 'animal | horse', 'bird | bird bird', 'horse | animal', 'bird | speech', 'bird | bird bird', 'cat | domestic animals pets', 'dog | domestic animals pets', 'dog | animal']}, 'audio_text': {'cluster_sort_idcs': [34, 47, 42, 20, 74, 26, 33, 16, 84, 14, 61, 12, 88, 9, 28, 7, 98, 5, 92, 3, 41, 95, 87, 69, 70, 17, 76, 13, 89, 38, 6, 39, 44, 50, 52, 0, 54, 57, 60, 66, 72, 93, 53, 73, 49, 11, 29, 19, 30, 25, 4, 37, 91, 90, 8, 10, 68, 86, 83, 78, 1, 97, 21, 23, 45, 99, 64, 46, 31, 62, 55, 27, 59, 58, 63, 85, 56, 94, 80, 77, 48, 18, 43, 71, 24, 79, 36, 40, 96, 2, 82, 35, 32, 15, 65, 81, 75, 67, 51, 22], 'cluster_sort_idcs_reverse': [35, 60, 89, 19, 50, 17, 30, 15, 54, 13, 55, 45, 11, 27, 9, 93, 7, 25, 81, 47, 3, 62, 99, 63, 84, 49, 5, 71, 14, 46, 48, 68, 92, 6, 0, 91, 86, 51, 29, 31, 87, 20, 2, 82, 32, 64, 67, 1, 80, 44, 33, 98, 34, 42, 36, 70, 76, 37, 73, 72, 38, 10, 69, 74, 66, 94, 39, 97, 56, 23, 24, 83, 40, 43, 4, 96, 26, 79, 59, 85, 78, 95, 90, 58, 8, 75, 57, 22, 12, 28, 53, 52, 18, 41, 77, 21, 88, 61, 16, 65], 'cluster_sizes': [24, 1, 2, 6, 17, 2, 22, 1, 12, 13], 'cluster_labels': ['cat | domestic animals pets', 'cat purr speech', 'speech dog | domestic animals pets', 'dog | animal', 'dog | domestic animals pets', 'bird | bird bird', 'bird | bird bird', 'bird white noise outside urban manmade', 'horse | speech', 'horse | animal']}, 'image_audio': {'cluster_sort_idcs': [26, 42, 70, 14, 47, 5, 98, 7, 69, 9, 61, 54, 0, 25, 29, 44, 53, 66, 17, 13, 72, 73, 74, 76, 6, 93, 39, 38, 65, 56, 50, 78, 95, 97, 41, 49, 34, 3, 16, 30, 12, 28, 20, 36, 18, 33, 15, 43, 71, 24, 79, 94, 11, 87, 88, 92, 57, 60, 84, 19, 4, 86, 68, 67, 23, 62, 46, 58, 45, 37, 31, 27, 83, 89, 64, 91, 2, 96, 1, 90, 82, 99, 52, 59, 35, 21, 55, 8, 10, 63, 40, 80, 75, 85, 51, 32, 77, 48, 22, 81], 'cluster_sort_idcs_reverse': [12, 78, 76, 37, 60, 5, 24, 7, 87, 9, 88, 52, 40, 19, 3, 46, 38, 18, 44, 59, 42, 85, 98, 64, 49, 13, 0, 71, 41, 14, 39, 70, 95, 45, 36, 84, 43, 69, 27, 26, 90, 34, 1, 47, 15, 68, 66, 4, 97, 35, 30, 94, 82, 16, 11, 86, 29, 56, 67, 83, 57, 10, 65, 89, 74, 28, 17, 63, 62, 8, 2, 48, 20, 21, 22, 92, 23, 96, 31, 50, 91, 99, 80, 72, 58, 93, 61, 53, 54, 73, 79, 75, 55, 25, 51, 32, 77, 33, 6, 81], 'cluster_sizes': [11, 17, 15, 5, 4, 8, 12, 18, 5, 5], 'cluster_labels': ['cat | domestic animals pets', 'dog | domestic animals pets', 'animal | domestic animals pets', 'horse | animal', 'horse | music', 'domestic animals pets | dog', 'bird | bird bird', 'bird | bird bird', 'horse | animal', 'horse | animal']}, 'image_image': {'cluster_sort_idcs': [58, 88, 30, 34, 38, 39, 41, 44, 47, 98, 50, 57, 0, 66, 69, 70, 72, 73, 74, 76, 87, 92, 95, 29, 26, 49, 12, 13, 25, 11, 19, 20, 14, 7, 9, 5, 3, 6, 16, 8, 68, 78, 83, 86, 4, 55, 1, 99, 27, 45, 31, 21, 46, 37, 48, 84, 79, 33, 71, 65, 2, 91, 52, 89, 96, 61, 28, 23, 94, 81, 82, 36, 97, 59, 63, 10, 51, 22, 15, 85, 18, 77, 75, 43, 35, 56, 67, 40, 80, 32, 93, 24, 64, 60, 54, 53, 17, 42, 90, 62], 'cluster_sort_idcs_reverse': [12, 46, 60, 36, 44, 35, 37, 33, 39, 34, 75, 29, 26, 27, 32, 78, 38, 96, 80, 30, 31, 51, 77, 67, 91, 28, 24, 48, 66, 23, 2, 50, 89, 57, 3, 84, 71, 53, 4, 5, 87, 6, 97, 83, 7, 49, 52, 8, 54, 25, 10, 76, 62, 95, 94, 45, 85, 11, 0, 73, 93, 65, 99, 74, 92, 59, 13, 86, 40, 14, 15, 58, 16, 17, 18, 82, 19, 81, 41, 56, 88, 69, 70, 42, 55, 79, 43, 20, 1, 63, 98, 61, 21, 90, 68, 22, 64, 72, 9, 47], 'cluster_sizes': [2, 37, 15, 11, 2, 2, 3, 4, 14, 10], 'cluster_labels': ['bird squawk speech | cat domestic animals pets meow animal', 'domestic animals pets | dog', 'bird | bird bird', 'animal | horse', 'cat purr domestic animals pets animal', 'music | bird music duck', 'horse | horse neigh whinny', 'bird | bird bird', 'horse | animal', 'bird | dog']}, 'image_text': {'cluster_sort_idcs': [99, 68, 64, 62, 83, 59, 55, 46, 45, 37, 90, 91, 27, 21, 78, 1, 8, 4, 10, 97, 58, 63, 86, 23, 31, 20, 5, 14, 41, 3, 7, 70, 74, 87, 88, 98, 47, 92, 16, 95, 42, 61, 12, 28, 84, 34, 33, 69, 26, 9, 79, 2, 75, 65, 81, 80, 15, 51, 18, 43, 96, 22, 32, 56, 85, 94, 24, 71, 67, 82, 36, 77, 40, 48, 35, 11, 44, 39, 89, 0, 49, 72, 60, 57, 54, 13, 52, 6, 17, 19, 93, 25, 73, 30, 76, 50, 53, 66, 29, 38], 'cluster_sort_idcs_reverse': [79, 15, 51, 29, 17, 26, 87, 30, 16, 49, 18, 75, 42, 85, 27, 56, 38, 88, 58, 89, 25, 13, 61, 23, 66, 91, 48, 12, 43, 98, 93, 24, 62, 46, 45, 74, 70, 9, 99, 77, 72, 28, 40, 59, 76, 8, 7, 36, 73, 80, 95, 57, 86, 96, 84, 6, 63, 83, 20, 5, 82, 41, 3, 21, 2, 53, 97, 68, 1, 47, 31, 67, 81, 92, 32, 52, 94, 71, 14, 50, 55, 54, 69, 4, 44, 64, 22, 33, 34, 78, 10, 11, 37, 90, 65, 39, 60, 19, 35, 0], 'cluster_sizes': [20, 2, 3, 7, 18, 14, 11, 3, 9, 13], 'cluster_labels': ['bird | bird bird', 'bird | bird squawk speech', 'bird | duck', 'cat | domestic animals pets meow', 'cat | domestic animals pets', 'horse | animal', 'horse | speech', 'dog | domestic animals pets', 'dog | dog animal', 'dog | domestic animals pets']}, 'text_audio': {'cluster_sort_idcs': [43, 36, 57, 19, 18, 15, 86, 23, 63, 67, 64, 3, 59, 45, 46, 1, 31, 58, 27, 10, 21, 99, 8, 68, 83, 12, 62, 91, 97, 84, 2, 90, 82, 35, 34, 75, 87, 80, 60, 79, 88, 71, 39, 40, 11, 96, 33, 92, 24, 77, 48, 51, 32, 85, 22, 81, 41, 4, 20, 78, 37, 55, 42, 5, 7, 47, 9, 98, 70, 69, 14, 16, 95, 28, 61, 26, 89, 93, 94, 0, 74, 6, 13, 17, 25, 29, 30, 38, 44, 50, 52, 53, 54, 56, 65, 66, 72, 73, 76, 49], 'cluster_sort_idcs_reverse': [79, 15, 30, 11, 57, 63, 81, 64, 22, 66, 19, 44, 25, 82, 70, 5, 71, 83, 4, 3, 58, 20, 54, 7, 48, 84, 75, 18, 73, 85, 86, 16, 52, 46, 34, 33, 1, 60, 87, 42, 43, 56, 62, 0, 88, 13, 14, 65, 50, 99, 89, 51, 90, 91, 92, 61, 93, 2, 17, 12, 38, 74, 26, 8, 10, 94, 95, 9, 23, 69, 68, 41, 96, 97, 80, 35, 98, 49, 59, 39, 37, 55, 32, 24, 29, 53, 6, 36, 40, 76, 31, 27, 47, 77, 78, 72, 45, 28, 67, 21], 'cluster_sizes': [6, 6, 13, 10, 14, 6, 1, 6, 14, 24], 'cluster_labels': ['animal | horse', 'bird | animal', 'bird | bird bird', 'bird | bird bird', 'animal | speech', 'horse | animal', 'horse neigh whinny animal', 'bird | bird bird', 'cat | domestic animals pets', 'dog | animal']}, 'text_image': {'cluster_sort_idcs': [64, 15, 23, 94, 97, 71, 48, 65, 79, 2, 81, 22, 35, 51, 56, 36, 43, 32, 40, 96, 80, 67, 18, 85, 75, 77, 63, 68, 59, 86, 27, 10, 46, 45, 78, 82, 62, 55, 99, 83, 8, 37, 1, 21, 91, 31, 4, 16, 14, 70, 95, 12, 74, 41, 84, 98, 33, 3, 34, 20, 9, 7, 69, 5, 87, 92, 61, 47, 26, 28, 42, 24, 58, 90, 89, 93, 19, 17, 54, 25, 88, 60, 52, 53, 0, 73, 72, 66, 57, 50, 44, 39, 38, 30, 29, 13, 11, 6, 76, 49], 'cluster_sort_idcs_reverse': [84, 42, 9, 57, 46, 63, 97, 61, 40, 60, 31, 96, 51, 95, 48, 1, 47, 77, 22, 76, 59, 43, 11, 2, 71, 79, 68, 30, 69, 94, 93, 45, 17, 56, 58, 12, 15, 41, 92, 91, 18, 53, 70, 16, 90, 33, 32, 67, 6, 99, 89, 13, 82, 83, 78, 37, 14, 88, 72, 28, 81, 66, 36, 26, 0, 7, 87, 21, 27, 62, 49, 5, 86, 85, 52, 24, 98, 25, 34, 8, 20, 10, 35, 39, 54, 23, 29, 64, 80, 74, 73, 44, 65, 75, 3, 50, 19, 4, 55, 38], 'cluster_sizes': [5, 6, 15, 8, 13, 13, 10, 5, 9, 16], 'cluster_labels': ['bird | bird bird', 'horse | animal', 'horse | animal', 'bird | bird bird', 'bird | bird bird', 'cat | domestic animals pets', 'cat | domestic animals pets', 'bird | speech', 'dog | domestic animals pets', 'dog | animal']}, 'text_text': {'cluster_sort_idcs': [58, 63, 31, 23, 99, 59, 46, 45, 62, 64, 37, 68, 27, 78, 83, 21, 55, 4, 1, 97, 90, 91, 10, 8, 86, 7, 95, 41, 42, 88, 61, 3, 47, 98, 5, 9, 69, 34, 87, 20, 84, 14, 12, 16, 26, 28, 74, 33, 73, 60, 72, 66, 93, 57, 0, 53, 25, 29, 50, 30, 52, 11, 92, 44, 13, 39, 17, 38, 19, 6, 54, 76, 89, 49, 70, 71, 48, 85, 77, 36, 79, 2, 96, 56, 94, 51, 75, 43, 15, 40, 18, 32, 35, 22, 24, 82, 81, 80, 65, 67], 'cluster_sort_idcs_reverse': [54, 18, 81, 31, 17, 34, 69, 25, 23, 35, 22, 61, 42, 64, 41, 88, 43, 66, 90, 68, 39, 15, 93, 3, 94, 56, 44, 12, 45, 57, 59, 2, 91, 47, 37, 92, 79, 10, 67, 65, 89, 27, 28, 87, 63, 7, 6, 32, 76, 73, 58, 85, 60, 55, 70, 16, 83, 53, 0, 5, 49, 30, 8, 1, 9, 98, 51, 99, 11, 36, 74, 75, 50, 48, 46, 86, 71, 78, 13, 80, 97, 96, 95, 14, 40, 77, 24, 38, 29, 72, 20, 21, 62, 52, 84, 26, 82, 19, 33, 4], 'cluster_sizes': [4, 20, 1, 23, 14, 10, 2, 1, 5, 20], 'cluster_labels': ['bird | duck', 'bird | bird bird', 'bird pour', 'cat | domestic animals pets', 'dog | domestic animals pets', 'dog | domestic animals pets', 'bow wow dog animal | bark bow wow dog animal', 'cat purr speech', 'horse | music', 'horse | animal']}}}\n"
     ]
    }
   ],
   "source": [
    "data_path = '../../../../../Data/'\n",
    "triplet_dir = data_path + \"imagebind/text-audio-image/\"\n",
    "\n",
    "dataset = Triplet_Dataset(path=triplet_dir, batch_size=100)\n",
    "all_images, all_prompts, all_audios, all_infos = dataset.get_data()\n",
    "print(len(all_images), len(all_prompts), len(all_audios))\n",
    "export_data(\"%s_size-%i\"%(dataset.name, dataset.batch_size), all_images, all_prompts, all_audios, all_infos, [am_model.ImageBind_Model()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
