{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/ginihumer/CLIP-explorer.git\n",
      "  Cloning https://github.com/ginihumer/CLIP-explorer.git to /private/var/folders/46/rz1kpyy146n3kk5pyv0pqdh80000gn/T/pip-req-build-rjptox20\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/ginihumer/CLIP-explorer.git /private/var/folders/46/rz1kpyy146n3kk5pyv0pqdh80000gn/T/pip-req-build-rjptox20\n",
      "  Resolved https://github.com/ginihumer/CLIP-explorer.git to commit e5c305fe7ca509355b85110db8f31fb6f301cf56\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting clip@ git+https://github.com/openai/CLIP.git@a9b1bf5920416aaeaec965c25dd9e8f98c864f16 (from CLIP-explorer==0.1.30)\n",
      "  Using cached clip-1.0-py3-none-any.whl\n",
      "Requirement already satisfied: open-clip-torch==2.20.0 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from CLIP-explorer==0.1.30) (2.20.0)\n",
      "Requirement already satisfied: datasets==2.12.0 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from CLIP-explorer==0.1.30) (2.12.0)\n",
      "Requirement already satisfied: webdataset==0.2.48 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from CLIP-explorer==0.1.30) (0.2.48)\n",
      "Requirement already satisfied: plotly in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from CLIP-explorer==0.1.30) (5.14.1)\n",
      "Collecting ipywidgets==8.0.6 (from CLIP-explorer==0.1.30)\n",
      "  Using cached ipywidgets-8.0.6-py3-none-any.whl (138 kB)\n",
      "Requirement already satisfied: ipykernel==6.23.1 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from CLIP-explorer==0.1.30) (6.23.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from CLIP-explorer==0.1.30) (1.2.2)\n",
      "Requirement already satisfied: openTSNE in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from CLIP-explorer==0.1.30) (0.7.1)\n",
      "Requirement already satisfied: umap-learn in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from CLIP-explorer==0.1.30) (0.5.3)\n",
      "Requirement already satisfied: numpy==1.23.5 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from CLIP-explorer==0.1.30) (1.23.5)\n",
      "Requirement already satisfied: pycocotools in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from CLIP-explorer==0.1.30) (2.0.6)\n",
      "Requirement already satisfied: transformers in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from CLIP-explorer==0.1.30) (4.29.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from datasets==2.12.0->CLIP-explorer==0.1.30) (12.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from datasets==2.12.0->CLIP-explorer==0.1.30) (0.3.6)\n",
      "Requirement already satisfied: pandas in /Users/christina/.local/lib/python3.9/site-packages (from datasets==2.12.0->CLIP-explorer==0.1.30) (1.3.5)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from datasets==2.12.0->CLIP-explorer==0.1.30) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from datasets==2.12.0->CLIP-explorer==0.1.30) (4.65.0)\n",
      "Requirement already satisfied: xxhash in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from datasets==2.12.0->CLIP-explorer==0.1.30) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from datasets==2.12.0->CLIP-explorer==0.1.30) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from datasets==2.12.0->CLIP-explorer==0.1.30) (2023.5.0)\n",
      "Requirement already satisfied: aiohttp in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from datasets==2.12.0->CLIP-explorer==0.1.30) (3.8.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from datasets==2.12.0->CLIP-explorer==0.1.30) (0.14.1)\n",
      "Requirement already satisfied: packaging in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from datasets==2.12.0->CLIP-explorer==0.1.30) (23.1)\n",
      "Requirement already satisfied: responses<0.19 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from datasets==2.12.0->CLIP-explorer==0.1.30) (0.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from datasets==2.12.0->CLIP-explorer==0.1.30) (6.0)\n",
      "Requirement already satisfied: appnope in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from ipykernel==6.23.1->CLIP-explorer==0.1.30) (0.1.3)\n",
      "Requirement already satisfied: comm>=0.1.1 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from ipykernel==6.23.1->CLIP-explorer==0.1.30) (0.1.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from ipykernel==6.23.1->CLIP-explorer==0.1.30) (1.6.7)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from ipykernel==6.23.1->CLIP-explorer==0.1.30) (8.13.2)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from ipykernel==6.23.1->CLIP-explorer==0.1.30) (8.2.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from ipykernel==6.23.1->CLIP-explorer==0.1.30) (5.3.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from ipykernel==6.23.1->CLIP-explorer==0.1.30) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from ipykernel==6.23.1->CLIP-explorer==0.1.30) (1.5.6)\n",
      "Requirement already satisfied: psutil in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from ipykernel==6.23.1->CLIP-explorer==0.1.30) (5.9.5)\n",
      "Requirement already satisfied: pyzmq>=20 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from ipykernel==6.23.1->CLIP-explorer==0.1.30) (25.0.2)\n",
      "Requirement already satisfied: tornado>=6.1 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from ipykernel==6.23.1->CLIP-explorer==0.1.30) (6.3.2)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from ipykernel==6.23.1->CLIP-explorer==0.1.30) (5.7.1)\n",
      "Collecting widgetsnbextension~=4.0.7 (from ipywidgets==8.0.6->CLIP-explorer==0.1.30)\n",
      "  Using cached widgetsnbextension-4.0.7-py3-none-any.whl (2.1 MB)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.7 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from ipywidgets==8.0.6->CLIP-explorer==0.1.30) (3.0.7)\n",
      "Requirement already satisfied: torch>=1.9.0 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from open-clip-torch==2.20.0->CLIP-explorer==0.1.30) (2.0.1)\n",
      "Requirement already satisfied: torchvision in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from open-clip-torch==2.20.0->CLIP-explorer==0.1.30) (0.15.2)\n",
      "Requirement already satisfied: regex in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from open-clip-torch==2.20.0->CLIP-explorer==0.1.30) (2023.5.5)\n",
      "Requirement already satisfied: ftfy in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from open-clip-torch==2.20.0->CLIP-explorer==0.1.30) (6.1.1)\n",
      "Requirement already satisfied: sentencepiece in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from open-clip-torch==2.20.0->CLIP-explorer==0.1.30) (0.1.99)\n",
      "Requirement already satisfied: protobuf<4 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from open-clip-torch==2.20.0->CLIP-explorer==0.1.30) (3.20.3)\n",
      "Requirement already satisfied: timm in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from open-clip-torch==2.20.0->CLIP-explorer==0.1.30) (0.9.2)\n",
      "Requirement already satisfied: braceexpand in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from webdataset==0.2.48->CLIP-explorer==0.1.30) (0.1.7)\n",
      "Requirement already satisfied: scipy in /Users/christina/.local/lib/python3.9/site-packages (from openTSNE->CLIP-explorer==0.1.30) (1.9.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from scikit-learn->CLIP-explorer==0.1.30) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from scikit-learn->CLIP-explorer==0.1.30) (3.1.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from plotly->CLIP-explorer==0.1.30) (8.2.2)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in /Users/christina/.local/lib/python3.9/site-packages (from pycocotools->CLIP-explorer==0.1.30) (3.7.1)\n",
      "Requirement already satisfied: filelock in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from transformers->CLIP-explorer==0.1.30) (3.12.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from transformers->CLIP-explorer==0.1.30) (0.13.3)\n",
      "Requirement already satisfied: numba>=0.49 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from umap-learn->CLIP-explorer==0.1.30) (0.57.0)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from umap-learn->CLIP-explorer==0.1.30) (0.5.10)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from aiohttp->datasets==2.12.0->CLIP-explorer==0.1.30) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from aiohttp->datasets==2.12.0->CLIP-explorer==0.1.30) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from aiohttp->datasets==2.12.0->CLIP-explorer==0.1.30) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from aiohttp->datasets==2.12.0->CLIP-explorer==0.1.30) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from aiohttp->datasets==2.12.0->CLIP-explorer==0.1.30) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from aiohttp->datasets==2.12.0->CLIP-explorer==0.1.30) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from aiohttp->datasets==2.12.0->CLIP-explorer==0.1.30) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets==2.12.0->CLIP-explorer==0.1.30) (4.6.0)\n",
      "Requirement already satisfied: backcall in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel==6.23.1->CLIP-explorer==0.1.30) (0.2.0)\n",
      "Requirement already satisfied: decorator in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel==6.23.1->CLIP-explorer==0.1.30) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel==6.23.1->CLIP-explorer==0.1.30) (0.18.2)\n",
      "Requirement already satisfied: pickleshare in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel==6.23.1->CLIP-explorer==0.1.30) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel==6.23.1->CLIP-explorer==0.1.30) (3.0.38)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel==6.23.1->CLIP-explorer==0.1.30) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel==6.23.1->CLIP-explorer==0.1.30) (0.6.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel==6.23.1->CLIP-explorer==0.1.30) (4.8.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.3 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from jupyter-client>=6.1.12->ipykernel==6.23.1->CLIP-explorer==0.1.30) (6.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from jupyter-client>=6.1.12->ipykernel==6.23.1->CLIP-explorer==0.1.30) (2.8.2)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel==6.23.1->CLIP-explorer==0.1.30) (3.5.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/christina/.local/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools->CLIP-explorer==0.1.30) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/christina/.local/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools->CLIP-explorer==0.1.30) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/christina/.local/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools->CLIP-explorer==0.1.30) (4.39.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/christina/.local/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools->CLIP-explorer==0.1.30) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools->CLIP-explorer==0.1.30) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/christina/.local/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools->CLIP-explorer==0.1.30) (3.0.9)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/christina/.local/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools->CLIP-explorer==0.1.30) (5.12.0)\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from numba>=0.49->umap-learn->CLIP-explorer==0.1.30) (0.40.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests>=2.19.0->datasets==2.12.0->CLIP-explorer==0.1.30) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests>=2.19.0->datasets==2.12.0->CLIP-explorer==0.1.30) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests>=2.19.0->datasets==2.12.0->CLIP-explorer==0.1.30) (2023.5.7)\n",
      "Requirement already satisfied: sympy in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from torch>=1.9.0->open-clip-torch==2.20.0->CLIP-explorer==0.1.30) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from torch>=1.9.0->open-clip-torch==2.20.0->CLIP-explorer==0.1.30) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from torch>=1.9.0->open-clip-torch==2.20.0->CLIP-explorer==0.1.30) (3.1.2)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from ftfy->open-clip-torch==2.20.0->CLIP-explorer==0.1.30) (0.2.6)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from pandas->datasets==2.12.0->CLIP-explorer==0.1.30) (2023.3)\n",
      "Requirement already satisfied: safetensors in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from timm->open-clip-torch==2.20.0->CLIP-explorer==0.1.30) (0.3.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from importlib-metadata>=4.8.3->jupyter-client>=6.1.12->ipykernel==6.23.1->CLIP-explorer==0.1.30) (3.15.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel==6.23.1->CLIP-explorer==0.1.30) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel==6.23.1->CLIP-explorer==0.1.30) (0.7.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel==6.23.1->CLIP-explorer==0.1.30) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from jinja2->torch>=1.9.0->open-clip-torch==2.20.0->CLIP-explorer==0.1.30) (2.1.2)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from stack-data->ipython>=7.23.1->ipykernel==6.23.1->CLIP-explorer==0.1.30) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from stack-data->ipython>=7.23.1->ipykernel==6.23.1->CLIP-explorer==0.1.30) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from stack-data->ipython>=7.23.1->ipykernel==6.23.1->CLIP-explorer==0.1.30) (0.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from sympy->torch>=1.9.0->open-clip-torch==2.20.0->CLIP-explorer==0.1.30) (1.3.0)\n",
      "Building wheels for collected packages: CLIP-explorer\n",
      "  Building wheel for CLIP-explorer (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for CLIP-explorer: filename=CLIP_explorer-0.1.30-py3-none-any.whl size=1428968 sha256=04bfaa7ebee9a3c1b648da53d8bf77c3b901e7f9e57b15d9eec17c5f86cbed6a\n",
      "  Stored in directory: /private/var/folders/46/rz1kpyy146n3kk5pyv0pqdh80000gn/T/pip-ephem-wheel-cache-s42frk7o/wheels/09/fc/8b/3b8a1f10143be547929a0768142222f15b2860f13226a18139\n",
      "Successfully built CLIP-explorer\n",
      "Installing collected packages: widgetsnbextension, ipywidgets, CLIP-explorer\n",
      "  Attempting uninstall: widgetsnbextension\n",
      "    Found existing installation: widgetsnbextension 3.6.4\n",
      "    Uninstalling widgetsnbextension-3.6.4:\n",
      "      Successfully uninstalled widgetsnbextension-3.6.4\n",
      "  Attempting uninstall: ipywidgets\n",
      "    Found existing installation: ipywidgets 7.7.1\n",
      "    Uninstalling ipywidgets-7.7.1:\n",
      "      Successfully uninstalled ipywidgets-7.7.1\n",
      "  Attempting uninstall: CLIP-explorer\n",
      "    Found existing installation: CLIP-explorer 0.1.29\n",
      "    Uninstalling CLIP-explorer-0.1.29:\n",
      "      Successfully uninstalled CLIP-explorer-0.1.29\n",
      "Successfully installed CLIP-explorer-0.1.30 ipywidgets-8.0.6 widgetsnbextension-4.0.7\n"
     ]
    }
   ],
   "source": [
    "# need ipykernel > 6\n",
    "# ! pip install ipykernel==6.23.1\n",
    "! pip install git+https://github.com/ginihumer/amumo.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amumo import model\n",
    "from amumo import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset diffusiondb (/Users/christina/.cache/huggingface/datasets/poloclub___diffusiondb/2m_first_1k/0.9.1/b3bc1e64570dc7149af62c4bac49ecfbce16b683dd4fee083292fae1afa95f7c)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb81f2df4ffb463cbd2bb1fafd6c080a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'DiffusionDB_size-100'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = data.DiffusionDB_Dataset(path=\"2m_first_1k\")\n",
    "def get_data_helper(dataset, filters=[], method=any):\n",
    "    all_images, all_prompts = dataset.get_filtered_data(filters, method=method)\n",
    "    print(len(all_images))\n",
    "\n",
    "    dataset_name = dataset.name\n",
    "    if len(filters) > 0:\n",
    "        dataset_name = dataset_name + '_filter-' + method.__name__ + '_' + '-'.join(filters)\n",
    "    else:\n",
    "        dataset_name = dataset_name + '_size-%i'%len(all_images)\n",
    "\n",
    "    return all_images, all_prompts, dataset_name\n",
    "\n",
    "all_images, all_prompts, dataset_name = get_data_helper(dataset, filters=[], method=any) # filters = [\"dog\"], method=all\n",
    "dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(model.CLIPModelInterface):\n",
    "    available_models = clip.available_models()\n",
    "    model_name = 'CLIP'\n",
    "\n",
    "    def __init__(self, name='RN50', device='cpu') -> None:\n",
    "        super().__init__(name, device)\n",
    "        self.model, self.preprocess = clip.load(name, device=device)\n",
    "        self.model.eval()\n",
    "        self.logit_scale = self.model.logit_scale\n",
    "\n",
    "    def encode_image(self, images):\n",
    "        images = [self.preprocess(i) for i in images]\n",
    "        image_input = torch.tensor(np.stack(images)).to(self.device)\n",
    "        return self.model.encode_image(image_input).float().cpu()\n",
    "\n",
    "    def encode_text(self, texts):\n",
    "        text_tokens = clip.tokenize(texts, truncate = True).to(self.device)\n",
    "        return self.model.encode_text(text_tokens).float().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "2023-06-08 13:14:24.188821: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of the model checkpoint at Salesforce/blip-image-captioning-base were not used when initializing BlipModel: ['text_decoder.bert.encoder.layer.1.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.7.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.5.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.3.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.5.attention.self.query.bias', 'text_decoder.bert.encoder.layer.2.output.dense.weight', 'text_decoder.bert.encoder.layer.3.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.11.attention.self.key.weight', 'text_decoder.bert.encoder.layer.9.attention.self.value.weight', 'text_decoder.bert.encoder.layer.11.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.9.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.2.attention.self.query.bias', 'text_decoder.bert.encoder.layer.5.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.0.output.dense.weight', 'text_decoder.bert.encoder.layer.8.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.7.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.7.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.0.attention.self.value.bias', 'text_decoder.cls.predictions.transform.dense.bias', 'text_decoder.bert.encoder.layer.6.output.LayerNorm.bias', 'text_decoder.cls.predictions.transform.LayerNorm.bias', 'text_decoder.bert.encoder.layer.10.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.1.attention.self.value.weight', 'text_decoder.bert.encoder.layer.8.attention.self.key.bias', 'text_decoder.bert.encoder.layer.8.output.dense.bias', 'text_decoder.bert.encoder.layer.9.attention.self.query.weight', 'text_decoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.8.attention.self.value.weight', 'text_decoder.bert.encoder.layer.11.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.2.output.dense.bias', 'text_decoder.bert.encoder.layer.4.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.1.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.10.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.5.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.1.crossattention.self.value.bias', 'text_decoder.cls.predictions.transform.LayerNorm.weight', 'text_decoder.bert.encoder.layer.9.output.dense.weight', 'text_decoder.bert.encoder.layer.3.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.0.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.6.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.8.attention.self.key.weight', 'text_decoder.bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.5.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.10.output.dense.bias', 'text_decoder.bert.encoder.layer.7.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.8.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.0.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.0.output.dense.bias', 'text_decoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.7.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.7.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.11.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.6.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.8.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.10.attention.self.key.weight', 'text_decoder.bert.encoder.layer.7.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.8.attention.self.query.weight', 'text_decoder.bert.encoder.layer.8.attention.self.query.bias', 'text_decoder.bert.encoder.layer.10.attention.self.query.weight', 'text_decoder.bert.encoder.layer.5.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.6.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.1.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.3.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.4.output.dense.bias', 'text_decoder.cls.predictions.decoder.bias', 'text_decoder.bert.encoder.layer.8.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.3.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.4.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.10.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.7.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.8.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.5.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.2.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.6.output.dense.weight', 'text_decoder.bert.embeddings.position_ids', 'text_decoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.10.attention.self.value.bias', 'text_decoder.bert.encoder.layer.3.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.2.attention.self.query.weight', 'text_decoder.bert.encoder.layer.4.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.3.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.11.crossattention.self.value.weight', 'text_decoder.cls.predictions.bias', 'text_decoder.bert.encoder.layer.7.attention.self.key.weight', 'text_decoder.bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.2.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.6.attention.self.key.weight', 'text_decoder.bert.encoder.layer.5.attention.self.key.weight', 'text_decoder.bert.encoder.layer.5.attention.self.value.bias', 'text_decoder.bert.encoder.layer.8.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.11.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.11.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.0.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.1.output.dense.bias', 'text_decoder.bert.encoder.layer.9.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.2.attention.self.key.weight', 'text_decoder.bert.encoder.layer.4.attention.self.query.weight', 'text_decoder.bert.encoder.layer.5.attention.self.key.bias', 'text_decoder.bert.encoder.layer.8.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.11.output.dense.bias', 'text_decoder.bert.encoder.layer.1.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.5.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.10.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.2.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.7.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.3.attention.self.query.weight', 'text_decoder.bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.10.attention.self.value.weight', 'text_decoder.bert.encoder.layer.9.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.4.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.9.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.9.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.10.attention.self.key.bias', 'text_decoder.bert.encoder.layer.5.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.0.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.3.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.2.attention.self.value.weight', 'text_decoder.bert.encoder.layer.4.attention.self.key.weight', 'text_decoder.bert.encoder.layer.7.attention.self.query.weight', 'text_decoder.bert.encoder.layer.10.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.8.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.9.attention.self.key.weight', 'text_decoder.bert.encoder.layer.7.attention.self.key.bias', 'text_decoder.bert.encoder.layer.6.attention.self.value.weight', 'text_decoder.bert.encoder.layer.7.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.10.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.8.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.4.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.11.attention.self.value.bias', 'text_decoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.4.attention.self.key.bias', 'text_decoder.bert.encoder.layer.2.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.5.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.3.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.3.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.0.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.11.attention.self.query.weight', 'text_decoder.bert.encoder.layer.8.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.5.attention.self.query.weight', 'text_decoder.bert.encoder.layer.6.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.0.attention.self.key.bias', 'text_decoder.bert.encoder.layer.2.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.3.output.dense.bias', 'text_decoder.bert.encoder.layer.11.attention.self.value.weight', 'text_decoder.bert.encoder.layer.1.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.0.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.6.attention.self.value.bias', 'text_decoder.bert.encoder.layer.3.attention.self.key.weight', 'text_decoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.10.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.3.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.4.attention.self.value.weight', 'text_decoder.bert.encoder.layer.2.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.6.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.4.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.11.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.4.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.6.crossattention.output.dense.bias', 'text_decoder.bert.embeddings.LayerNorm.bias', 'text_decoder.bert.encoder.layer.5.attention.self.value.weight', 'text_decoder.bert.encoder.layer.9.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.4.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.6.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.2.attention.self.value.bias', 'text_decoder.bert.encoder.layer.2.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.11.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.5.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.0.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.6.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.0.attention.self.key.weight', 'text_decoder.bert.encoder.layer.9.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.4.output.dense.weight', 'text_decoder.bert.encoder.layer.0.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.5.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.9.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.6.attention.self.query.weight', 'text_decoder.bert.encoder.layer.11.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.10.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.9.attention.self.key.bias', 'text_decoder.bert.encoder.layer.10.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.7.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.9.attention.self.query.bias', 'text_decoder.bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'text_decoder.bert.embeddings.word_embeddings.weight', 'text_decoder.bert.encoder.layer.6.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.8.attention.self.value.bias', 'text_decoder.bert.encoder.layer.0.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.11.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.0.attention.self.query.bias', 'text_decoder.bert.encoder.layer.0.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.3.attention.self.query.bias', 'text_decoder.bert.encoder.layer.1.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.6.attention.self.key.bias', 'text_decoder.bert.encoder.layer.4.attention.self.query.bias', 'text_decoder.bert.encoder.layer.9.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.0.attention.self.value.weight', 'text_decoder.bert.encoder.layer.0.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.3.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.5.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.10.output.LayerNorm.weight', 'text_decoder.cls.predictions.decoder.weight', 'text_decoder.bert.encoder.layer.5.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.10.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.7.output.dense.bias', 'text_decoder.bert.encoder.layer.2.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.3.attention.self.value.bias', 'text_decoder.cls.predictions.transform.dense.weight', 'text_decoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.9.attention.self.value.bias', 'text_decoder.bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.6.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.6.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.2.attention.self.key.bias', 'text_decoder.bert.encoder.layer.5.output.dense.bias', 'text_decoder.bert.encoder.layer.11.output.dense.weight', 'text_decoder.bert.encoder.layer.2.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.1.attention.self.value.bias', 'text_decoder.bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.7.output.dense.weight', 'text_decoder.bert.encoder.layer.9.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.11.attention.self.query.bias', 'text_decoder.bert.encoder.layer.8.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.2.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.6.output.dense.bias', 'text_decoder.bert.encoder.layer.11.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.1.attention.self.key.bias', 'text_decoder.bert.encoder.layer.1.attention.self.key.weight', 'text_decoder.bert.encoder.layer.3.output.dense.weight', 'text_decoder.bert.encoder.layer.1.attention.self.query.bias', 'text_decoder.bert.encoder.layer.9.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.4.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.0.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.1.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.0.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.1.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.1.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.2.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.0.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.6.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.11.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.3.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.10.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.1.output.dense.weight', 'text_decoder.bert.encoder.layer.3.attention.self.value.weight', 'text_decoder.bert.encoder.layer.7.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.4.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.4.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.1.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.7.attention.self.query.bias', 'text_decoder.bert.encoder.layer.7.attention.self.value.weight', 'text_decoder.bert.encoder.layer.7.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.1.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.11.attention.self.key.bias', 'text_decoder.bert.encoder.layer.11.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.8.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.9.output.dense.bias', 'text_decoder.bert.encoder.layer.4.attention.self.value.bias', 'text_decoder.bert.encoder.layer.4.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.4.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.4.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.8.attention.output.dense.bias', 'text_decoder.bert.embeddings.LayerNorm.weight', 'text_decoder.bert.encoder.layer.9.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.8.output.dense.weight', 'text_decoder.bert.encoder.layer.1.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.1.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.2.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.8.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.0.attention.self.query.weight', 'text_decoder.bert.encoder.layer.2.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.5.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.9.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.2.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.10.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.6.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.3.crossattention.self.key.weight', 'text_decoder.bert.embeddings.position_embeddings.weight', 'text_decoder.bert.encoder.layer.10.output.dense.weight', 'text_decoder.bert.encoder.layer.6.attention.self.query.bias', 'text_decoder.bert.encoder.layer.9.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.11.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.3.attention.self.key.bias', 'text_decoder.bert.encoder.layer.7.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.7.attention.self.value.bias', 'text_decoder.bert.encoder.layer.10.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.1.attention.self.query.weight', 'text_decoder.bert.encoder.layer.5.output.dense.weight', 'text_decoder.bert.encoder.layer.10.attention.self.query.bias']\n",
      "- This IS expected if you are initializing BlipModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BlipModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BlipModel were not initialized from the model checkpoint at Salesforce/blip-image-captioning-base and are newly initialized: ['text_model.encoder.layer.0.output.LayerNorm.weight', 'text_model.encoder.layer.2.crossattention.self.value.bias', 'text_model.encoder.layer.10.crossattention.self.query.weight', 'text_model.encoder.layer.1.output.LayerNorm.bias', 'text_model.encoder.layer.7.output.dense.bias', 'text_model.encoder.layer.3.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.5.crossattention.self.key.bias', 'text_model.encoder.layer.9.output.dense.bias', 'text_model.embeddings.LayerNorm.bias', 'text_model.encoder.layer.7.crossattention.output.dense.weight', 'text_model.encoder.layer.6.intermediate.dense.bias', 'text_model.encoder.layer.3.attention.self.key.bias', 'text_model.encoder.layer.2.attention.output.dense.weight', 'text_model.encoder.layer.11.attention.self.key.bias', 'text_model.encoder.layer.1.crossattention.self.query.weight', 'text_model.encoder.layer.0.attention.self.query.weight', 'text_model.encoder.layer.5.attention.self.value.bias', 'text_model.encoder.layer.9.crossattention.self.key.weight', 'text_model.encoder.layer.2.output.dense.bias', 'text_model.encoder.layer.0.attention.self.query.bias', 'text_model.encoder.layer.5.crossattention.output.dense.bias', 'text_model.encoder.layer.7.intermediate.dense.weight', 'text_model.encoder.layer.11.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.1.crossattention.self.key.weight', 'text_model.encoder.layer.2.attention.self.value.weight', 'text_model.encoder.layer.0.intermediate.dense.weight', 'text_model.encoder.layer.0.crossattention.self.value.weight', 'text_model.encoder.layer.2.intermediate.dense.weight', 'text_model.encoder.layer.10.attention.self.key.weight', 'text_model.encoder.layer.8.crossattention.output.dense.bias', 'text_model.encoder.layer.10.crossattention.self.key.weight', 'text_model.encoder.layer.4.attention.output.LayerNorm.bias', 'text_model.encoder.layer.3.crossattention.self.key.bias', 'text_model.encoder.layer.6.output.LayerNorm.bias', 'text_model.encoder.layer.4.intermediate.dense.weight', 'text_model.encoder.layer.10.crossattention.self.value.weight', 'text_model.encoder.layer.10.intermediate.dense.bias', 'text_model.encoder.layer.9.output.dense.weight', 'text_model.encoder.layer.11.output.LayerNorm.bias', 'text_model.encoder.layer.2.attention.self.query.weight', 'text_model.encoder.layer.2.output.dense.weight', 'text_model.encoder.layer.1.crossattention.self.value.weight', 'text_model.encoder.layer.1.output.LayerNorm.weight', 'text_model.encoder.layer.3.attention.self.query.bias', 'text_model.encoder.layer.7.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.1.attention.self.value.weight', 'text_model.encoder.layer.8.attention.self.query.weight', 'text_model.encoder.layer.5.attention.self.key.bias', 'text_model.encoder.layer.5.attention.output.dense.weight', 'text_model.encoder.layer.0.attention.output.LayerNorm.bias', 'text_model.encoder.layer.2.crossattention.output.dense.weight', 'text_model.encoder.layer.3.attention.output.dense.weight', 'text_model.encoder.layer.1.attention.self.key.weight', 'text_model.encoder.layer.3.crossattention.self.key.weight', 'text_model.encoder.layer.7.attention.output.dense.weight', 'text_model.encoder.layer.11.attention.output.dense.weight', 'text_model.encoder.layer.9.attention.self.key.weight', 'text_model.encoder.layer.6.output.dense.weight', 'text_model.encoder.layer.10.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.7.crossattention.self.query.weight', 'text_model.encoder.layer.9.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.5.intermediate.dense.bias', 'text_model.encoder.layer.8.crossattention.output.dense.weight', 'text_model.encoder.layer.6.attention.output.dense.weight', 'text_model.encoder.layer.9.attention.output.LayerNorm.bias', 'text_model.encoder.layer.10.crossattention.self.query.bias', 'text_model.encoder.layer.8.attention.output.dense.bias', 'text_model.encoder.layer.9.attention.self.key.bias', 'text_model.encoder.layer.11.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.9.crossattention.self.value.bias', 'text_model.encoder.layer.8.output.LayerNorm.weight', 'text_model.encoder.layer.11.attention.output.LayerNorm.bias', 'text_model.encoder.layer.2.crossattention.self.query.weight', 'text_model.encoder.layer.11.crossattention.output.dense.weight', 'text_model.encoder.layer.10.crossattention.self.key.bias', 'text_model.encoder.layer.1.attention.output.LayerNorm.weight', 'text_model.encoder.layer.7.attention.self.query.bias', 'text_model.encoder.layer.9.attention.self.query.weight', 'text_model.encoder.layer.1.crossattention.output.dense.weight', 'text_model.encoder.layer.3.crossattention.output.dense.weight', 'text_model.encoder.layer.9.intermediate.dense.bias', 'text_model.encoder.layer.1.attention.self.value.bias', 'text_model.encoder.layer.3.crossattention.self.value.weight', 'text_model.encoder.layer.9.crossattention.self.query.weight', 'text_model.encoder.layer.2.attention.output.LayerNorm.weight', 'text_model.encoder.layer.4.attention.output.dense.weight', 'text_model.encoder.layer.5.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.2.crossattention.output.dense.bias', 'text_model.encoder.layer.9.output.LayerNorm.weight', 'text_model.encoder.layer.2.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.4.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.1.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.7.crossattention.self.value.weight', 'text_model.encoder.layer.3.intermediate.dense.bias', 'text_model.encoder.layer.10.attention.self.key.bias', 'text_model.encoder.layer.7.attention.self.query.weight', 'text_model.encoder.layer.9.attention.output.dense.weight', 'text_model.encoder.layer.11.attention.output.dense.bias', 'text_model.encoder.layer.6.crossattention.self.key.bias', 'text_model.encoder.layer.3.attention.output.LayerNorm.weight', 'text_model.encoder.layer.10.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.8.attention.output.LayerNorm.weight', 'text_model.encoder.layer.3.output.dense.bias', 'text_model.encoder.layer.8.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.3.output.dense.weight', 'text_model.encoder.layer.5.crossattention.self.query.weight', 'text_model.encoder.layer.10.crossattention.output.dense.bias', 'text_model.encoder.layer.10.attention.output.LayerNorm.bias', 'text_model.encoder.layer.3.crossattention.self.query.bias', 'text_model.encoder.layer.8.attention.output.dense.weight', 'text_model.encoder.layer.6.crossattention.self.value.bias', 'text_model.encoder.layer.9.crossattention.self.key.bias', 'visual_projection.weight', 'text_model.encoder.layer.1.attention.output.LayerNorm.bias', 'text_model.encoder.layer.11.attention.self.query.bias', 'text_model.encoder.layer.4.output.dense.bias', 'text_model.encoder.layer.0.intermediate.dense.bias', 'text_model.encoder.layer.4.attention.self.query.bias', 'text_model.encoder.layer.0.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.1.output.dense.bias', 'text_model.encoder.layer.1.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.8.attention.output.LayerNorm.bias', 'text_model.encoder.layer.10.attention.output.dense.weight', 'text_model.encoder.layer.8.attention.self.query.bias', 'text_model.encoder.layer.9.output.LayerNorm.bias', 'text_model.encoder.layer.6.crossattention.self.query.weight', 'text_model.encoder.layer.4.attention.self.value.bias', 'text_model.encoder.layer.8.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.1.attention.output.dense.weight', 'text_model.encoder.layer.6.attention.self.query.bias', 'text_model.encoder.layer.0.crossattention.self.query.bias', 'text_model.encoder.layer.11.crossattention.self.query.bias', 'text_model.encoder.layer.5.attention.self.value.weight', 'text_model.encoder.layer.4.crossattention.self.value.weight', 'text_model.encoder.layer.3.output.LayerNorm.weight', 'text_model.encoder.layer.10.attention.output.dense.bias', 'text_model.encoder.layer.11.attention.self.query.weight', 'text_model.encoder.layer.8.attention.self.value.bias', 'text_model.encoder.layer.11.output.LayerNorm.weight', 'text_model.encoder.layer.6.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.9.attention.output.dense.bias', 'text_model.encoder.layer.7.attention.output.LayerNorm.weight', 'text_model.encoder.layer.8.intermediate.dense.weight', 'text_model.encoder.layer.11.intermediate.dense.weight', 'text_model.encoder.layer.5.crossattention.self.key.weight', 'text_model.encoder.layer.0.output.LayerNorm.bias', 'text_model.encoder.layer.11.crossattention.self.query.weight', 'text_model.encoder.layer.1.attention.self.key.bias', 'text_model.encoder.layer.4.crossattention.self.query.weight', 'text_model.encoder.layer.5.attention.self.key.weight', 'text_model.encoder.layer.1.crossattention.self.value.bias', 'text_model.encoder.layer.5.crossattention.self.value.weight', 'text_model.encoder.layer.9.crossattention.output.dense.bias', 'text_model.encoder.layer.7.crossattention.output.dense.bias', 'text_model.encoder.layer.6.intermediate.dense.weight', 'text_model.encoder.layer.11.attention.self.value.weight', 'text_model.encoder.layer.9.crossattention.self.value.weight', 'text_model.encoder.layer.4.attention.self.query.weight', 'text_model.encoder.layer.5.crossattention.self.value.bias', 'text_model.encoder.layer.2.output.LayerNorm.weight', 'text_model.encoder.layer.10.output.dense.weight', 'text_model.encoder.layer.7.output.dense.weight', 'text_model.encoder.layer.5.output.dense.weight', 'text_projection.weight', 'text_model.encoder.layer.3.output.LayerNorm.bias', 'text_model.encoder.layer.8.crossattention.self.query.weight', 'text_model.encoder.layer.7.attention.self.value.weight', 'text_model.encoder.layer.8.crossattention.self.value.weight', 'text_model.encoder.layer.4.crossattention.output.dense.bias', 'text_model.embeddings.position_embeddings.weight', 'text_model.encoder.layer.3.attention.self.value.bias', 'text_model.encoder.layer.5.attention.output.LayerNorm.weight', 'text_model.encoder.layer.9.intermediate.dense.weight', 'text_model.encoder.layer.6.attention.self.key.weight', 'text_model.encoder.layer.9.attention.output.LayerNorm.weight', 'text_model.encoder.layer.8.attention.self.key.bias', 'text_model.encoder.layer.0.attention.self.value.weight', 'text_model.encoder.layer.11.attention.self.value.bias', 'text_model.encoder.layer.11.crossattention.output.dense.bias', 'text_model.encoder.layer.7.crossattention.self.key.weight', 'text_model.encoder.layer.10.attention.self.value.weight', 'text_model.encoder.layer.0.crossattention.self.value.bias', 'text_model.encoder.layer.10.intermediate.dense.weight', 'text_model.encoder.layer.10.output.LayerNorm.weight', 'text_model.encoder.layer.11.intermediate.dense.bias', 'text_model.encoder.layer.0.attention.self.key.weight', 'text_model.encoder.layer.0.crossattention.self.query.weight', 'text_model.encoder.layer.2.attention.output.LayerNorm.bias', 'text_model.encoder.layer.0.crossattention.self.key.bias', 'text_model.encoder.layer.4.crossattention.self.value.bias', 'text_model.encoder.layer.10.output.dense.bias', 'text_model.encoder.layer.0.attention.output.LayerNorm.weight', 'text_model.encoder.layer.9.crossattention.self.query.bias', 'text_model.encoder.layer.8.intermediate.dense.bias', 'text_model.embeddings.word_embeddings.weight', 'text_model.encoder.layer.7.crossattention.self.key.bias', 'text_model.encoder.layer.11.crossattention.self.key.weight', 'text_model.encoder.layer.7.crossattention.self.query.bias', 'text_model.encoder.layer.9.attention.self.query.bias', 'text_model.encoder.layer.2.crossattention.self.value.weight', 'text_model.encoder.layer.6.attention.self.value.weight', 'text_model.encoder.layer.6.attention.self.value.bias', 'text_model.encoder.layer.7.output.LayerNorm.bias', 'text_model.encoder.layer.11.attention.output.LayerNorm.weight', 'text_model.encoder.layer.5.output.LayerNorm.bias', 'text_model.encoder.layer.8.crossattention.self.key.bias', 'text_model.encoder.layer.1.crossattention.output.dense.bias', 'text_model.encoder.layer.4.attention.self.key.bias', 'text_model.encoder.layer.4.crossattention.output.dense.weight', 'text_model.encoder.layer.10.attention.output.LayerNorm.weight', 'text_model.encoder.layer.2.attention.self.query.bias', 'text_model.encoder.layer.0.crossattention.output.dense.bias', 'text_model.encoder.layer.8.output.LayerNorm.bias', 'text_model.encoder.layer.2.crossattention.self.key.bias', 'text_model.encoder.layer.11.output.dense.bias', 'text_model.encoder.layer.9.crossattention.output.dense.weight', 'text_model.encoder.layer.0.crossattention.self.key.weight', 'text_model.pooler.dense.bias', 'text_model.encoder.layer.0.output.dense.weight', 'text_model.encoder.layer.4.attention.self.key.weight', 'text_model.encoder.layer.10.crossattention.output.dense.weight', 'text_model.encoder.layer.2.attention.self.key.weight', 'text_model.encoder.layer.6.crossattention.self.key.weight', 'text_model.encoder.layer.7.attention.output.LayerNorm.bias', 'text_model.encoder.layer.8.output.dense.weight', 'text_model.encoder.layer.11.attention.self.key.weight', 'text_model.encoder.layer.2.attention.output.dense.bias', 'logit_scale', 'text_model.encoder.layer.0.attention.self.value.bias', 'text_model.encoder.layer.7.output.LayerNorm.weight', 'text_model.encoder.layer.5.intermediate.dense.weight', 'text_model.encoder.layer.4.crossattention.self.query.bias', 'text_model.encoder.layer.2.attention.self.value.bias', 'text_model.encoder.layer.3.intermediate.dense.weight', 'text_model.encoder.layer.5.attention.output.dense.bias', 'text_model.encoder.layer.6.crossattention.output.dense.bias', 'text_model.encoder.layer.4.output.LayerNorm.weight', 'text_model.encoder.layer.5.crossattention.self.query.bias', 'text_model.encoder.layer.6.output.dense.bias', 'text_model.pooler.dense.weight', 'text_model.encoder.layer.1.crossattention.self.query.bias', 'text_model.embeddings.LayerNorm.weight', 'text_model.encoder.layer.2.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.6.output.LayerNorm.weight', 'text_model.encoder.layer.10.attention.self.query.weight', 'text_model.encoder.layer.9.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.6.attention.self.query.weight', 'text_model.encoder.layer.1.output.dense.weight', 'text_model.encoder.layer.4.output.dense.weight', 'text_model.encoder.layer.0.crossattention.output.dense.weight', 'text_model.encoder.layer.3.attention.self.key.weight', 'text_model.encoder.layer.3.attention.self.value.weight', 'text_model.encoder.layer.6.crossattention.output.dense.weight', 'text_model.encoder.layer.10.attention.self.query.bias', 'text_model.encoder.layer.1.attention.self.query.weight', 'text_model.encoder.layer.6.attention.output.LayerNorm.bias', 'text_model.encoder.layer.5.output.dense.bias', 'text_model.encoder.layer.5.crossattention.output.dense.weight', 'text_model.encoder.layer.5.output.LayerNorm.weight', 'text_model.encoder.layer.10.crossattention.self.value.bias', 'text_model.encoder.layer.1.intermediate.dense.bias', 'text_model.encoder.layer.6.crossattention.self.value.weight', 'text_model.encoder.layer.1.intermediate.dense.weight', 'text_model.encoder.layer.11.crossattention.self.value.weight', 'text_model.encoder.layer.5.attention.self.query.bias', 'text_model.encoder.layer.4.attention.self.value.weight', 'text_model.encoder.layer.1.crossattention.self.key.bias', 'text_model.encoder.layer.7.attention.self.key.bias', 'text_model.encoder.layer.8.crossattention.self.query.bias', 'text_model.encoder.layer.7.attention.self.key.weight', 'text_model.encoder.layer.4.output.LayerNorm.bias', 'text_model.encoder.layer.4.attention.output.dense.bias', 'text_model.encoder.layer.7.crossattention.self.value.bias', 'text_model.encoder.layer.11.crossattention.self.value.bias', 'text_model.encoder.layer.3.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.6.attention.self.key.bias', 'text_model.encoder.layer.2.crossattention.self.query.bias', 'text_model.encoder.layer.9.attention.self.value.weight', 'text_model.encoder.layer.8.attention.self.value.weight', 'text_model.encoder.layer.7.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.1.attention.output.dense.bias', 'text_model.encoder.layer.1.attention.self.query.bias', 'text_model.encoder.layer.8.output.dense.bias', 'text_model.encoder.layer.10.output.LayerNorm.bias', 'text_model.encoder.layer.10.attention.self.value.bias', 'text_model.encoder.layer.11.crossattention.self.key.bias', 'text_model.encoder.layer.3.attention.output.dense.bias', 'text_model.encoder.layer.5.attention.output.LayerNorm.bias', 'text_model.encoder.layer.4.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.2.output.LayerNorm.bias', 'text_model.encoder.layer.0.attention.output.dense.weight', 'text_model.encoder.layer.6.crossattention.self.query.bias', 'text_model.encoder.layer.4.intermediate.dense.bias', 'text_model.encoder.layer.6.attention.output.LayerNorm.weight', 'text_model.encoder.layer.0.attention.self.key.bias', 'text_model.encoder.layer.4.attention.output.LayerNorm.weight', 'text_model.encoder.layer.2.attention.self.key.bias', 'text_model.encoder.layer.7.attention.output.dense.bias', 'text_model.encoder.layer.4.crossattention.self.key.bias', 'text_model.encoder.layer.5.attention.self.query.weight', 'text_model.encoder.layer.6.attention.output.dense.bias', 'text_model.encoder.layer.2.crossattention.self.key.weight', 'text_model.encoder.layer.0.attention.output.dense.bias', 'text_model.encoder.layer.3.crossattention.self.query.weight', 'text_model.encoder.layer.5.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.0.output.dense.bias', 'text_model.encoder.layer.4.crossattention.self.key.weight', 'text_model.encoder.layer.2.intermediate.dense.bias', 'text_model.encoder.layer.7.attention.self.value.bias', 'text_model.encoder.layer.3.crossattention.output.dense.bias', 'text_model.encoder.layer.7.intermediate.dense.bias', 'text_model.encoder.layer.3.attention.output.LayerNorm.bias', 'text_model.encoder.layer.6.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.9.attention.self.value.bias', 'text_model.encoder.layer.8.crossattention.self.key.weight', 'text_model.encoder.layer.8.crossattention.self.value.bias', 'text_model.encoder.layer.3.crossattention.self.value.bias', 'text_model.encoder.layer.3.attention.self.query.weight', 'text_model.encoder.layer.11.output.dense.weight', 'text_model.encoder.layer.0.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.8.attention.self.key.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages/clipexplorer/CLOOB_local/training/model_configs/RN50.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages/clipexplorer/widgets.py:356: FutureWarning: The input object of type 'PngImageFile' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'PngImageFile', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  self.all_images = np.array(all_images)\n",
      "/Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages/clipexplorer/widgets.py:356: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.all_images = np.array(all_images)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found cached embeddings for DiffusionDB_size-100_CLIP_RN50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b8fc6ba3dc14f50b825ad38559657e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CLIPExplorerWidget(children=(VBox(children=(HBox(children=(Dropdown(description='Model: ', options=('CLIP', 'O"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from amumo.widgets import CLIPExplorerWidget\n",
    "\n",
    "\n",
    "clipexplorer_widget = CLIPExplorerWidget(dataset_name, all_images, all_prompts, models=[CustomModel()])\n",
    "clipexplorer_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
