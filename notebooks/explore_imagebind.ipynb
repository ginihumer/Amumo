{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting soundfile\n",
      "  Using cached soundfile-0.12.1-py2.py3-none-win_amd64.whl (1.0 MB)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\christina\\appdata\\roaming\\python\\python39\\site-packages (from soundfile) (1.15.1)\n",
      "Collecting pycparser (from cffi>=1.0->soundfile)\n",
      "  Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
      "Installing collected packages: pycparser, soundfile\n",
      "Successfully installed pycparser-2.21 soundfile-0.12.1\n"
     ]
    }
   ],
   "source": [
    "# ! pip install git+https://github.com/facebookresearch/ImageBind\n",
    "! pip install soundfile\n",
    "! pip install librosa\n",
    "! pip install \"imagebind @ git+https://github.com/facebookresearch/ImageBind@c6a47d6dc2b53eced51d398c181d57049ca59286\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d2c64d1867448b87724548bdda9b66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Christina\\anaconda3\\envs\\amumo_test\\lib\\site-packages\\datasets\\table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "# create triplet dataset\n",
    "from datasets import load_dataset\n",
    "import soundfile as sf\n",
    "import requests\n",
    "import pandas as pd\n",
    "# https://huggingface.co/datasets/agkphysics/AudioSet\n",
    "# dataset = load_dataset(\"agkphysics/AudioSet\", \"bal\", split=\"test\")\n",
    "dataset = load_dataset(\"arrow\", split=\"test\", data_files={\"test\":\"C:/Users/Christina/.cache/huggingface/datasets/agkphysics___audio_set/bal/audio_set-test-000**-of-00018.arrow\"})#, split=\"test[0:10]\")\n",
    "# https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.filter\n",
    "dataset_it = dataset.to_iterable_dataset()\n",
    "\n",
    "data_dir = \"C:\\\\Users\\\\Christina\\\\Data\\\\imagebind\\\\text-audio-image\\\\\"\n",
    "animals = [\"Bird\", \"Cat\", \"Dog\", \"Horse\"]\n",
    "\n",
    "infos = []\n",
    "idx = 0\n",
    "for animal in animals:\n",
    "    for example in dataset_it.filter(lambda x: animal in x[\"human_labels\"]).take(25):\n",
    "        # save audiofile\n",
    "        sf.write(data_dir + \"audio\\\\%03d.wav\"%idx, example[\"audio\"][\"array\"], example[\"audio\"][\"sampling_rate\"], format=\"wav\")\n",
    "\n",
    "        # save image\n",
    "        # https://stackoverflow.com/questions/2068344/how-do-i-get-a-youtube-video-thumbnail-from-the-youtube-api\n",
    "        url = 'https://img.youtube.com/vi/%s/hqdefault.jpg'%example[\"video_id\"]\n",
    "        data = requests.get(url).content \n",
    "        with open(data_dir + \"image\\\\%03d.jpg\"%idx,'wb') as f:\n",
    "            f.write(data) \n",
    "\n",
    "        # save info\n",
    "        infos.append({\"id\": idx, \"video_id\":example[\"video_id\"], \"labels\":example[\"human_labels\"], \"animal\": animal})\n",
    "\n",
    "        idx += 1\n",
    "\n",
    "pd.DataFrame(infos).to_csv(data_dir + \"info.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61a6e5092bd843aea7d2235064690b54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Christina\\anaconda3\\envs\\amumo_test\\lib\\site-packages\\datasets\\table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "# # https://huggingface.co/docs/datasets/loading#slice-splits\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# # dataset = load_dataset(\"agkphysics/AudioSet\", \"bal\", split=\"test\") #, streaming=True\n",
    "# dataset = load_dataset(\"arrow\", split=\"test\", data_files={\"test\":\"C:/Users/Christina/.cache/huggingface/datasets/agkphysics___audio_set/bal/audio_set-test-000**-of-00018.arrow\"})#, split=\"test[0:10]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image-Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mamumo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data \u001b[38;5;28;01mas\u001b[39;00m am_data\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mamumo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils \u001b[38;5;28;01mas\u001b[39;00m am_utils\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mamumo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m widgets \u001b[38;5;28;01mas\u001b[39;00m am_widgets\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# load dataset\u001b[39;00m\n\u001b[0;32m      7\u001b[0m dataset \u001b[38;5;241m=\u001b[39m am_data\u001b[38;5;241m.\u001b[39mDiffusionDB_Dataset(path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2m_first_1k\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m) \u001b[38;5;66;03m# data helper for the diffusionDB dataset; for the interactive prototype, we only use a random subset of 100 samples\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Christina\\anaconda3\\envs\\amumo_test\\lib\\site-packages\\amumo\\widgets.py:10\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PCA\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenTSNE\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TSNE\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mumap\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UMAP\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\decomposition\\__init__.py:9\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mThe :mod:`sklearn.decomposition` module includes matrix decomposition\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03malgorithms, including among others PCA, NMF or ICA. Most of the algorithms of\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03mthis module can be regarded as dimensionality reduction techniques.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextmath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m randomized_svd\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dict_learning\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     DictionaryLearning,\n\u001b[0;32m     11\u001b[0m     MiniBatchDictionaryLearning,\n\u001b[0;32m     12\u001b[0m     SparseCoder,\n\u001b[0;32m     13\u001b[0m     dict_learning,\n\u001b[0;32m     14\u001b[0m     dict_learning_online,\n\u001b[0;32m     15\u001b[0m     sparse_encode,\n\u001b[0;32m     16\u001b[0m )\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_factor_analysis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FactorAnalysis\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_fastica\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastICA, fastica\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\decomposition\\_dict_learning.py:23\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m linalg\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     18\u001b[0m     BaseEstimator,\n\u001b[0;32m     19\u001b[0m     ClassNamePrefixFeaturesOutMixin,\n\u001b[0;32m     20\u001b[0m     TransformerMixin,\n\u001b[0;32m     21\u001b[0m     _fit_context,\n\u001b[0;32m     22\u001b[0m )\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Lars, Lasso, LassoLars, orthogonal_mp_gram\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_array, check_random_state, gen_batches, gen_even_slices\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Hidden, Interval, StrOptions, validate_params\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\__init__.py:11\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearRegression\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bayes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ARDRegression, BayesianRidge\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_coordinate_descent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     12\u001b[0m     ElasticNet,\n\u001b[0;32m     13\u001b[0m     ElasticNetCV,\n\u001b[0;32m     14\u001b[0m     Lasso,\n\u001b[0;32m     15\u001b[0m     LassoCV,\n\u001b[0;32m     16\u001b[0m     MultiTaskElasticNet,\n\u001b[0;32m     17\u001b[0m     MultiTaskElasticNetCV,\n\u001b[0;32m     18\u001b[0m     MultiTaskLasso,\n\u001b[0;32m     19\u001b[0m     MultiTaskLassoCV,\n\u001b[0;32m     20\u001b[0m     enet_path,\n\u001b[0;32m     21\u001b[0m     lasso_path,\n\u001b[0;32m     22\u001b[0m )\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_glm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GammaRegressor, PoissonRegressor, TweedieRegressor\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_huber\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuberRegressor\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:20\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sparse\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiOutputMixin, RegressorMixin, _fit_context\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_cv\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_array, check_scalar\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval, StrOptions\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\__init__.py:3\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_plot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LearningCurveDisplay, ValidationCurveDisplay\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_search\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GridSearchCV, ParameterGrid, ParameterSampler, RandomizedSearchCV\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_split\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      6\u001b[0m     BaseCrossValidator,\n\u001b[0;32m      7\u001b[0m     BaseShuffleSplit,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m     train_test_split,\n\u001b[0;32m     25\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_plot.py:7\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_matplotlib_support\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_plotting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _interval_max_min_ratio, _validate_score_name\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m learning_curve, validation_curve\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01m_BaseCurveDisplay\u001b[39;00m:\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_plot_curve\u001b[39m(\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     13\u001b[0m         x_data,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m         errorbar_kw\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     24\u001b[0m     ):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py:29\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone, is_classifier\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FitFailedWarning\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_scoring, get_scorer_names\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_scorer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _check_multimetric_scoring, _MultimetricScorer\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\__init__.py:7\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mThe :mod:`sklearn.metrics` module includes score functions, performance metrics\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mand pairwise metrics and distance computations.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cluster\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_classification\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      9\u001b[0m     accuracy_score,\n\u001b[0;32m     10\u001b[0m     balanced_accuracy_score,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m     zero_one_loss,\n\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dist_metrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DistanceMetric\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\cluster\\__init__.py:9\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mThe :mod:`sklearn.metrics.cluster` submodule contains evaluation metrics for\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mcluster analysis results. There are two forms of evaluation:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m- unsupervised, which does not and measures the 'quality' of the model itself.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bicluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m consensus_score\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_supervised\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     adjusted_mutual_info_score,\n\u001b[0;32m     11\u001b[0m     adjusted_rand_score,\n\u001b[0;32m     12\u001b[0m     completeness_score,\n\u001b[0;32m     13\u001b[0m     contingency_matrix,\n\u001b[0;32m     14\u001b[0m     entropy,\n\u001b[0;32m     15\u001b[0m     expected_mutual_information,\n\u001b[0;32m     16\u001b[0m     fowlkes_mallows_score,\n\u001b[0;32m     17\u001b[0m     homogeneity_completeness_v_measure,\n\u001b[0;32m     18\u001b[0m     homogeneity_score,\n\u001b[0;32m     19\u001b[0m     mutual_info_score,\n\u001b[0;32m     20\u001b[0m     normalized_mutual_info_score,\n\u001b[0;32m     21\u001b[0m     pair_confusion_matrix,\n\u001b[0;32m     22\u001b[0m     rand_score,\n\u001b[0;32m     23\u001b[0m     v_measure_score,\n\u001b[0;32m     24\u001b[0m )\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_unsupervised\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     26\u001b[0m     calinski_harabasz_score,\n\u001b[0;32m     27\u001b[0m     davies_bouldin_score,\n\u001b[0;32m     28\u001b[0m     silhouette_samples,\n\u001b[0;32m     29\u001b[0m     silhouette_score,\n\u001b[0;32m     30\u001b[0m )\n\u001b[0;32m     32\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madjusted_mutual_info_score\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnormalized_mutual_info_score\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconsensus_score\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     52\u001b[0m ]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:29\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulticlass\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m type_of_target\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_array, check_consistent_length\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_expected_mutual_info_fast\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m expected_mutual_information\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_clusterings\u001b[39m(labels_true, labels_pred):\n\u001b[0;32m     33\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that the labels arrays are 1D and of same dimension.\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;124;03m        The predicted labels.\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:398\u001b[0m, in \u001b[0;36mparent\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from amumo import model as am_model\n",
    "from amumo import data as am_data\n",
    "from amumo import utils as am_utils\n",
    "from amumo import widgets as am_widgets\n",
    "\n",
    "# load dataset\n",
    "dataset = am_data.DiffusionDB_Dataset(path=\"2m_first_1k\", batch_size=100) # data helper for the diffusionDB dataset; for the interactive prototype, we only use a random subset of 100 samples\n",
    "all_images, all_prompts = dataset.get_data()\n",
    "cache_name = 'diffusiondb_random_100' # path used to cache the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 244M/244M [01:27<00:00, 2.92MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found cached embeddings for diffusiondb_random_100_ImageBind_huge image\n",
      "found cached embeddings for diffusiondb_random_100_ImageBind_huge text\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "019294e3992a433eb1b83e3af890c8e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CLIPExplorerWidget(children=(VBox(children=(HBox(children=(Dropdown(description='Model: ', options=('ImageBind…"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found cached embeddings for diffusiondb_random_100_CLIP_RN50 image\n",
      "found cached embeddings for diffusiondb_random_100_CLIP_RN50 text\n",
      "found cached embeddings for diffusiondb_random_100_ImageBind_huge image\n",
      "found cached embeddings for diffusiondb_random_100_ImageBind_huge text\n"
     ]
    }
   ],
   "source": [
    "am_widgets.CLIPExplorerWidget(cache_name, all_data={\"image\": all_images, \"text\": all_prompts}, models=[am_model.ImageBind_Model(), \"CLIP\"]) # {\"image\": all_images, \"depth\": all_depths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True [False, False] {'ImageBind': <amumo.model.ImageBind_Model object at 0x000001B878F032E0>, 'CLIP': <amumo.model.CLIPModel object at 0x000001B767795E20>}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\christina\\repositories\\icg\\researchstay\\amumo\\amumo\\widgets.py:940: FutureWarning:\n",
      "\n",
      "The input object of type 'PngImageFile' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'PngImageFile', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "\n",
      "c:\\users\\christina\\repositories\\icg\\researchstay\\amumo\\amumo\\widgets.py:940: VisibleDeprecationWarning:\n",
      "\n",
      "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found cached embeddings for diffusiondb_random_100_ImageBind_huge\n",
      "found cached embeddings for diffusiondb_random_100_CLIP_RN50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e29ae00ffa9f41edaaf7549652f884de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CLIPComparerWidget(children=(HoverWidget(children=(VBox(children=(HTML(value='', layout=Layout(width='300px'))…"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "am_widgets.CLIPComparerWidget(cache_name, all_images=all_images, all_prompts=all_prompts, models=[am_model.ImageBind_Model(), \"CLIP\"]) # {\"image\": all_images, \"depth\": all_depths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image-Text-Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christina\\AppData\\Roaming\\Python\\Python39\\site-packages\\umap\\distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "C:\\Users\\Christina\\AppData\\Roaming\\Python\\Python39\\site-packages\\umap\\distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "C:\\Users\\Christina\\AppData\\Roaming\\Python\\Python39\\site-packages\\umap\\distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "C:\\Users\\Christina\\AppData\\Roaming\\Python\\Python39\\site-packages\\umap\\umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "from amumo import model as am_model\n",
    "from amumo import data as am_data\n",
    "from amumo import utils as am_utils\n",
    "from amumo import widgets as am_widgets\n",
    "# ! pip install torch\n",
    "# ! pip install torchaudio\n",
    "import torch\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amumo import model as am_model\n",
    "from amumo import data as am_data\n",
    "from amumo import utils as am_utils\n",
    "from amumo import widgets as am_widgets\n",
    "# ! pip install torch\n",
    "# ! pip install torchaudio\n",
    "import torch\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import io\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class AudioType(am_data.DataTypeInterface):\n",
    "    name = \"Audio\"\n",
    "\n",
    "    def __init__(self, data, sample_rate) -> None:\n",
    "        super().__init__(data)\n",
    "        self.sample_rate = sample_rate\n",
    "    \n",
    "    def getVisItem(self, idx):\n",
    "        import soundfile as sf\n",
    "        buffer = io.BytesIO()\n",
    "        sf.write(buffer, self.data[idx][0], self.sample_rate, format=\"wav\")\n",
    "        audio_bytes = buffer.getvalue() # wav bytes\n",
    "        return {\"displayType\": am_data.DisplayTypes.AUDIO, \"value\": audio_bytes}\n",
    "\n",
    "\n",
    "class Triplet_Dataset(am_data.DatasetInterface):\n",
    "    name='Triplet'\n",
    "\n",
    "    def __init__(self, path, seed=31415, batch_size=100, sample_rate=16000):\n",
    "        # create triplet dataset if it does not exist\n",
    "        super().__init__(path, seed, batch_size)\n",
    "        # path: path to the triplet dataset\n",
    "        image_paths = glob(path + \"image\\\\*.jpg\", recursive = True)\n",
    "        audio_paths = glob(path + \"audio\\\\*.wav\", recursive = True)\n",
    "\n",
    "        self.sample_rate = sample_rate\n",
    "        \n",
    "        all_images = []\n",
    "        for image_path in image_paths:\n",
    "            with open(image_path, \"rb\") as fopen:\n",
    "                image = Image.open(fopen).convert(\"RGB\")\n",
    "                all_images.append(image)\n",
    "\n",
    "        all_audios = []\n",
    "        for audio_path in audio_paths:\n",
    "            waveform, sr = torchaudio.load(audio_path)\n",
    "            if sample_rate != sr:\n",
    "                waveform = torchaudio.functional.resample(\n",
    "                    waveform, orig_freq=sr, new_freq=sample_rate\n",
    "                )\n",
    "            all_audios.append(waveform)\n",
    "        \n",
    "        self.all_infos = pd.read_csv(path + \"info.csv\", converters={\"labels\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \")})\n",
    "\n",
    "        # TODO... load on demand with a custom loader\n",
    "        self.all_images = np.array(all_images)\n",
    "        self.all_prompts = np.array(self.all_infos[\"labels\"].map(lambda x: \", \".join(x)))\n",
    "        self.all_audios = np.array(all_audios)\n",
    "    \n",
    "    \n",
    "    def get_data(self):\n",
    "        \n",
    "        if self.batch_size is None:\n",
    "            images = self.MODE1_Type(self.all_images)\n",
    "            texts = self.MODE2_Type(self.all_prompts)\n",
    "            audios = AudioType(self.all_audios, self.sample_rate)\n",
    "        \n",
    "            return images, texts, audios\n",
    "\n",
    "        # create a random batch\n",
    "        batch_idcs = self._get_random_subsample(len(self.all_images))\n",
    "\n",
    "        images = self.MODE1_Type(self.all_images[batch_idcs])\n",
    "        texts = self.MODE2_Type(self.all_prompts[batch_idcs])\n",
    "        audios = AudioType(self.all_audios[batch_idcs], self.sample_rate)\n",
    "        \n",
    "        return images, texts, audios\n",
    "    \n",
    "        \n",
    "    def get_filtered_data(self, filter_list, method=any):\n",
    "        # filter_list: a list of strings that are used for filtering\n",
    "        # method: any -> any substring given in filter_list is present; all -> all substrings must be contained in the string\n",
    "        if filter_list is None or len(filter_list) <= 0:\n",
    "            return self.get_data()\n",
    "\n",
    "        subset_ids = np.array([i for i in range(len(self.all_prompts)) if method(substring in self.all_prompts[i].lower() for substring in filter_list)])\n",
    "        if len(subset_ids) <= 0:\n",
    "            print(\"no filter matches found\")\n",
    "            return [], [], []\n",
    "        \n",
    "        # create a random batch\n",
    "        batch_idcs = self._get_random_subsample(len(subset_ids))\n",
    "        subset_ids = subset_ids[batch_idcs]\n",
    "        \n",
    "        images = self.MODE1_Type(self.all_images[subset_ids])\n",
    "        texts = self.MODE2_Type(self.all_prompts[subset_ids])\n",
    "        audios = AudioType(self.all_audios[subset_ids], self.sample_rate)\n",
    "        return images, texts, audios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amumo.utils import get_textual_label_for_cluster, get_embeddings_per_modality, get_similarities, get_similarities_all, get_cluster_sorting, get_modality_distance, calculate_val_loss, get_closed_modality_gap, get_modality_gap_normed, l2_norm, get_gap_direction\n",
    "\n",
    "\n",
    "import traitlets\n",
    "from ipywidgets import widgets\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class SimilarityHeatmapClusteringWidget(widgets.VBox):\n",
    "    \n",
    "    embedding = traitlets.Dict().tag(sync=True)\n",
    "    value = traitlets.Any(np.zeros((6,6))).tag(sync=True)\n",
    "    cluster = traitlets.Any().tag(sync=True)\n",
    "    modality_labels = traitlets.List([]).tag(sync=True)\n",
    "    cluster_label_data = None\n",
    "\n",
    "    hover_idx = traitlets.List([]).tag(sync=True)\n",
    "\n",
    "\n",
    "    def __init__(self, zmin=None, zmax=None, cluster_label_data=None, hover_callback=None):\n",
    "        super(SimilarityHeatmapClusteringWidget, self).__init__()\n",
    "\n",
    "        self.cluster_label_data = cluster_label_data\n",
    "        self.size = 6\n",
    "        self.hover_callback = hover_callback\n",
    "\n",
    "        self.cluster_similarity_matrix_widget = widgets.Checkbox(\n",
    "            value=False,\n",
    "            description='Cluster matrix by similarity',\n",
    "            disabled=False,\n",
    "            indent=False\n",
    "        )\n",
    "        self.cluster_similarity_matrix_by_widget1 = widgets.Dropdown(\n",
    "            options=self.modality_labels,\n",
    "            description='between',\n",
    "            style = {\"description_width\": \"initial\"},\n",
    "            layout=widgets.Layout(width=\"150px\")\n",
    "        )\n",
    "        self.cluster_similarity_matrix_by_widget2 = widgets.Dropdown(\n",
    "            options=self.modality_labels,\n",
    "            description='and',\n",
    "            style = {\"description_width\": \"initial\"},\n",
    "            layout=widgets.Layout(width=\"150px\")\n",
    "        )\n",
    "        \n",
    "        self.fig_widget = go.FigureWidget(data=[go.Heatmap(z=self.value, zmin=zmin, zmax=zmax)])\n",
    "        self.heatmap = self.fig_widget.data[0]\n",
    "        self.heatmap.hoverinfo = \"text\"\n",
    "        self.fig_widget.update_layout(width=500, height=420,\n",
    "            margin=dict(l=10, r=10, t=10, b=10),\n",
    "        )\n",
    "        self.fig_widget.update_yaxes(autorange='reversed', fixedrange=False)\n",
    "        self.fig_widget.update_xaxes(fixedrange=False)\n",
    "        self.fig_widget.layout.shapes = self._get_matrix_gridlines()\n",
    "\n",
    "        self.heatmap.on_hover(self._hover_fn)\n",
    "        \n",
    "        self.cluster_similarity_matrix_widget.observe(self.onUpdateEmbedding, names='value')\n",
    "        self.cluster_similarity_matrix_by_widget1.observe(self.onUpdateClusterBy, names='value')\n",
    "        self.cluster_similarity_matrix_by_widget2.observe(self.onUpdateClusterBy, names='value')\n",
    "\n",
    "        settings = widgets.HBox([self.cluster_similarity_matrix_widget, \n",
    "                                 self.cluster_similarity_matrix_by_widget1, \n",
    "                                 self.cluster_similarity_matrix_by_widget2], \n",
    "                                 layout=widgets.Layout(width=\"430px\"))\n",
    "        self.children = [settings, widgets.HBox([self.fig_widget])]\n",
    "\n",
    "\n",
    "\n",
    "    def _hover_fn(self, trace, points, state):\n",
    "        x_idx = points.xs[0]\n",
    "        y_idx = points.ys[0]\n",
    "            \n",
    "        # show hover lines in similarity heatmap\n",
    "        self.hover_idx = [(x_idx, y_idx)]\n",
    "\n",
    "        # extract original x and y index and modalities for x and y; then call the callback function\n",
    "        if self.hover_callback is not None:\n",
    "            x_modality_idx = math.floor(x_idx/self.size)\n",
    "            y_modality_idx = math.floor(y_idx/self.size)\n",
    "\n",
    "            # x_modality = 'modality2'\n",
    "            # if x_idx < self.size:\n",
    "            #     x_modality = 'modality1'\n",
    "\n",
    "            # y_modality = 'modality2'\n",
    "            # if y_idx < len(self.idcs):\n",
    "            #     y_modality = 'modality1'\n",
    "\n",
    "            x_idx = self.idcs[x_idx%len(self.idcs)]\n",
    "            y_idx = self.idcs[y_idx%len(self.idcs)]\n",
    "            # self.hover_callback(x_idx, y_idx, x_modality, y_modality)\n",
    "            self.hover_callback(x_idx, y_idx, self.modality_labels[x_modality_idx], self.modality_labels[y_modality_idx])\n",
    "\n",
    "    def _get_matrix_gridlines(self):\n",
    "        no_modalities = len(self.modality_labels)  \n",
    "\n",
    "        line_style = dict(color=\"black\", width=1)\n",
    "        gridlines = []\n",
    "        for i in range(no_modalities-1):\n",
    "            horizontal_line = go.layout.Shape(type='line', x0=len(self.value)*(1+i)/no_modalities-0.5, y0=0-0.5, x1=len(self.value)*(1+i)/no_modalities-0.5, y1=len(self.value)-0.5, line=line_style)\n",
    "            gridlines.append(horizontal_line)\n",
    "            vertical_line = go.layout.Shape(type='line', y0=len(self.value)*(1+i)/no_modalities-0.5, x0=0-0.5, y1=len(self.value)*(1+i)/no_modalities-0.5, x1=len(self.value)-0.5, line=line_style)\n",
    "            gridlines.append(vertical_line)\n",
    "\n",
    "        return gridlines\n",
    "\n",
    "    @traitlets.observe(\"modality_labels\")\n",
    "    def onUpdateModalityLabel(self, change):\n",
    "        no_modalities = len(self.modality_labels)\n",
    "        self.fig_widget.update_layout(\n",
    "            xaxis = dict(\n",
    "                tickmode = 'array',\n",
    "                tickvals = [(i*2+1)/2 * self.size for i in range(no_modalities)], #[1*len(self.value)/4, 3*len(self.value)/4]\n",
    "                ticktext = self.modality_labels\n",
    "            ),\n",
    "            yaxis = dict(\n",
    "                tickmode = 'array',\n",
    "                tickvals = [(i*2+1)/2 * self.size for i in range(no_modalities)], #[1*len(self.value)/4, 3*len(self.value)/4]\n",
    "                ticktext = self.modality_labels\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.cluster_similarity_matrix_by_widget1.options = self.modality_labels\n",
    "        if self.cluster_similarity_matrix_by_widget1.value is None:\n",
    "            self.cluster_similarity_matrix_by_widget1.value = self.modality_labels[0]\n",
    "        self.cluster_similarity_matrix_by_widget2.options = self.modality_labels\n",
    "        if self.cluster_similarity_matrix_by_widget2.value is None:\n",
    "            self.cluster_similarity_matrix_by_widget2.value = self.modality_labels[0]\n",
    "    \n",
    "\n",
    "    def onUpdateClusterBy(self, change):\n",
    "        if self.cluster_similarity_matrix_by_widget1.value is not None and self.cluster_similarity_matrix_by_widget2.value is not None and self.cluster_similarity_matrix_widget.value:\n",
    "            self.onUpdateEmbedding(change)\n",
    "\n",
    "\n",
    "    @traitlets.validate(\"value\")\n",
    "    def _validate_value(self, proposal):\n",
    "        # print(\"TODO: validate value\")\n",
    "        return proposal.value\n",
    "\n",
    "    @traitlets.observe(\"value\")\n",
    "    def onUpdateValue(self, change):\n",
    "        self.fig_widget.data[0].z = self.value\n",
    "        self.fig_widget.layout.shapes = self._get_matrix_gridlines()\n",
    "\n",
    "    @traitlets.validate(\"embedding\")\n",
    "    def _validate_embedding(self, proposal):\n",
    "        # print(\"TODO: validate embedding\")\n",
    "        # for backwards compatibility map array to dict\n",
    "        if isinstance(proposal.value, tuple):\n",
    "            embeddings = {}\n",
    "            embeddings[\"Modality1\"] = proposal.value[0]\n",
    "            embeddings[\"Modality2\"] = proposal.value[1]\n",
    "            return embeddings\n",
    "        return proposal.value\n",
    "\n",
    "    @traitlets.observe(\"embedding\")\n",
    "    def onUpdateEmbedding(self, change):\n",
    "        self.size=len(list(self.embedding.values())[0])\n",
    "        self.modality_labels = list(self.embedding.keys())\n",
    "        no_modalities = len(self.embedding.keys())\n",
    "        similarity_all = get_similarities_all(self.embedding)\n",
    "\n",
    "        cluster_labels = []\n",
    "        cluster_sizes = []\n",
    "\n",
    "        if self.cluster_similarity_matrix_widget.value:\n",
    "            similarity_between = self.embedding[self.cluster_similarity_matrix_by_widget1.value]\n",
    "            similarity_and = self.embedding[self.cluster_similarity_matrix_by_widget2.value]\n",
    "            similarity_cross_modal = get_similarities(torch.from_numpy(similarity_between), torch.from_numpy(similarity_and))\n",
    "            self.idcs, clusters, clusters_unsorted = get_cluster_sorting(similarity_cross_modal)\n",
    "            for c in set(clusters):\n",
    "                cluster_size = np.count_nonzero(clusters==c)\n",
    "                cluster_label = \"\"\n",
    "                if self.cluster_label_data is not None:\n",
    "                    cluster_label = self.cluster_label_data.getMinSummary(np.where(clusters_unsorted==c)[0])\n",
    "                cluster_labels.append(cluster_label)\n",
    "                cluster_sizes.append(cluster_size)\n",
    "        else:\n",
    "            self.idcs = np.arange(self.size) # TODO: use reverse idcs to get original order for interaction with other widgets\n",
    "\n",
    "        # with heatmap_widget.batch_update():\n",
    "        matrix_sort_idcs = np.concatenate([self.idcs + i*self.size for i in range(no_modalities)], axis=0) # [self.idcs, self.idcs+self.size] # need to do double index because we combined images and texts\n",
    "        \n",
    "        self.value = similarity_all[matrix_sort_idcs, :][:, matrix_sort_idcs]\n",
    "        self.cluster = (cluster_labels, cluster_sizes)\n",
    "\n",
    "        \n",
    "    # @traitlets.validate(\"cluster\")\n",
    "    # def _validate_cluster(self, proposal):\n",
    "        # takes a list of cluster labels + sizes\n",
    "    #     print(\"TODO: validate cluster\")\n",
    "    #     return proposal.value\n",
    "\n",
    "    @traitlets.observe(\"cluster\")\n",
    "    def onUpdateCluster(self, change):\n",
    "        cluster_modality1_idx = self.modality_labels.index(self.cluster_similarity_matrix_by_widget1.value)\n",
    "        cluster_modality2_idx = self.modality_labels.index(self.cluster_similarity_matrix_by_widget2.value)\n",
    "        cluster_shapes = self._get_matrix_gridlines()\n",
    "        labels, sizes = self.cluster\n",
    "        offset = 0-0.5 # -0.5 because heatmap rectangles are drawn around [-0.5, 0.5]\n",
    "        for (cluster_label, cluster_size) in zip(labels, sizes):\n",
    "            if cluster_size > 5:\n",
    "                textposition = 'middle left' if offset < len(self.value)/2/2 else 'middle right'\n",
    "\n",
    "                # see https://plotly.com/python/shapes/\n",
    "                cluster_shapes += [go.layout.Shape(\n",
    "                    type='rect', \n",
    "                    x0=cluster_modality1_idx*self.size+offset, \n",
    "                    y0=cluster_modality2_idx*self.size+offset, \n",
    "                    x1=cluster_modality1_idx*self.size+offset+cluster_size, \n",
    "                    y1=cluster_modality2_idx*self.size+offset+cluster_size, \n",
    "                    label=dict(text=cluster_label, textposition=textposition, font=dict(size=10, color=\"white\"), padding=np.log(cluster_size)*10), \n",
    "                    line=dict(width=1, color='white')\n",
    "                )]\n",
    "\n",
    "            offset += cluster_size\n",
    "            \n",
    "        self.fig_widget.layout.shapes = cluster_shapes\n",
    "\n",
    "\n",
    "    @traitlets.observe(\"hover_idx\")\n",
    "    def onUpdateHoverIdx(self, change):\n",
    "        shapes = [sh for sh in self.fig_widget.layout.shapes if sh.name != 'hover_idx' and sh.name != 'hover_idx']\n",
    "\n",
    "        for (x_idx, y_idx) in self.hover_idx:\n",
    "            if x_idx >= 0 and x_idx < len(self.value):\n",
    "                shapes.append(go.layout.Shape(name='hover_idx', type='line', x0=x_idx, y0=0-0.5, x1=x_idx, y1=len(self.value)-0.5, line=dict(color=\"grey\", width=1)))\n",
    "            if y_idx >= 0 and y_idx < len(self.value):\n",
    "                shapes.append(go.layout.Shape(name='hover_idx', type='line', y0=y_idx, x0=0-0.5, y1=y_idx, x1=len(self.value)-0.5, line=dict(color=\"grey\", width=1)))\n",
    "        \n",
    "        self.fig_widget.layout.shapes = shapes\n",
    "\n",
    "\n",
    "class HoverWidget(widgets.VBox):\n",
    "    \n",
    "    valueX = traitlets.Any().tag(sync=True)\n",
    "    valueY = traitlets.Any().tag(sync=True)\n",
    "\n",
    "\n",
    "    def __init__(self, width=300):\n",
    "        super(HoverWidget, self).__init__()\n",
    "\n",
    "        self.width = width\n",
    "\n",
    "        output_dummy_img = io.BytesIO()\n",
    "        Image.new('RGB', (self.width,self.width)).save(output_dummy_img, format=\"JPEG\")\n",
    "        self.img_widgets = {'valueX': widgets.Image(value=output_dummy_img.getvalue(), width=0, height=0), \n",
    "                            'valueY': widgets.Image(value=output_dummy_img.getvalue(), width=0)} #, height=0)}\n",
    "        self.txt_widgets = {'valueX': widgets.HTML(value='', layout=widgets.Layout(width=\"%ipx\"%self.width)), \n",
    "                            'valueY': widgets.HTML(value='', layout=widgets.Layout(width=\"%ipx\"%self.width)), }\n",
    "        self.wav_widgets = {'valueX': widgets.Audio(autoplay=True, layout=widgets.Layout(width=\"0px\", height=\"0px\")),\n",
    "                            'valueY': widgets.Audio(autoplay=True, layout=widgets.Layout(width=\"0px\", height=\"0px\"))}\n",
    "\n",
    "        self.children = [widgets.VBox(list(self.wav_widgets.values())), widgets.VBox(list(self.txt_widgets.values())), widgets.VBox(list(self.img_widgets.values()))]\n",
    "\n",
    "        self.layout = widgets.Layout(width=\"%ipx\"%(self.width+10), height=\"inherit\")\n",
    "\n",
    "\n",
    "    @traitlets.validate(\"value1\", \"value2\")\n",
    "    def _validate_value(self, proposal):\n",
    "        # print(\"TODO: validate value1\")\n",
    "        return proposal.value\n",
    "    \n",
    "    def set_text(self, name, value):\n",
    "        cur_img_widget = self.img_widgets[name]\n",
    "        cur_txt_widget = self.txt_widgets[name]\n",
    "        cur_wav_widget = self.wav_widgets[name]\n",
    "\n",
    "        cur_txt_widget.value = \"<div style='word-wrap: break-word;'>{}</div>\".format(value)\n",
    "        cur_img_widget.width = 0\n",
    "        # cur_img_widget.height = 0\n",
    "        cur_wav_widget.value = b'RIFF$\\xe2\\x04\\x00WAVEfmt'\n",
    "        cur_wav_widget.layout = widgets.Layout(width=\"0px\", height=\"0px\")\n",
    "\n",
    "    def set_img(self, name, value):\n",
    "        cur_img_widget = self.img_widgets[name]\n",
    "        cur_txt_widget = self.txt_widgets[name]\n",
    "        cur_wav_widget = self.wav_widgets[name]\n",
    "\n",
    "        cur_img_widget.value = value\n",
    "        cur_img_widget.width = self.width\n",
    "        # cur_img_widget.height = self.width\n",
    "        cur_txt_widget.value = \"\"\n",
    "        cur_wav_widget.value = b'RIFF$\\xe2\\x04\\x00WAVEfmt'\n",
    "        cur_wav_widget.layout = widgets.Layout(width=\"0px\", height=\"0px\")\n",
    "\n",
    "    def set_wav(self, name, value):\n",
    "        cur_img_widget = self.img_widgets[name]\n",
    "        cur_txt_widget = self.txt_widgets[name]\n",
    "        cur_wav_widget = self.wav_widgets[name]\n",
    "\n",
    "        cur_img_widget.width = 0\n",
    "        cur_txt_widget.value = \"\"\n",
    "        cur_wav_widget.value = value\n",
    "        cur_wav_widget.layout = widgets.Layout(width=\"%ipx\"%self.width, height=\"20px\")\n",
    "\n",
    "\n",
    "    @traitlets.observe(\"valueX\", \"valueY\")\n",
    "    def onUpdateValue(self, change):\n",
    "        if type(change.new) is io.BytesIO:\n",
    "            self.set_img(change.name, change.new.getvalue())\n",
    "\n",
    "        elif type(change.new) is dict:\n",
    "            if \"displayType\" in change.new:\n",
    "                if change.new[\"displayType\"] == am_data.DisplayTypes.IMAGE:\n",
    "                    self.set_img(change.name, change.new[\"value\"])\n",
    "                elif change.new[\"displayType\"] == am_data.DisplayTypes.AUDIO:\n",
    "                    self.set_wav(change.name, change.new[\"value\"])\n",
    "                else:\n",
    "                    self.set_text(change.name, change.new[\"value\"])\n",
    "\n",
    "            elif \"value\" in change.new:\n",
    "                self.set_text(change.name, change.new[\"value\"])\n",
    "        else:\n",
    "            self.set_text(change.name, change.new)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from umap import UMAP\n",
    "available_projection_methods = {\n",
    "    'PCA': {'module': PCA, 'OOS':False}, # OOS: flag to signal whether or not out of sample is possible\n",
    "    'UMAP': {'module': UMAP, 'OOS':True},\n",
    "}\n",
    "try: \n",
    "    from openTSNE import TSNE\n",
    "    available_projection_methods[\"TSNE\"] = {'module': TSNE, 'OOS':False}\n",
    "except ImportError:\n",
    "    print(\"To support TSNE dataset, please install 'openTSNE': 'pip install openTSNE'.\")\n",
    "\n",
    "\n",
    "class ScatterPlotWidget(widgets.VBox):\n",
    "    \n",
    "    embedding = traitlets.Any().tag(sync=True)\n",
    "    cluster = traitlets.Any().tag(sync=True)\n",
    "    hover_idcs = traitlets.Any().tag(sync=True)\n",
    "    # modality1_label = traitlets.Unicode().tag(sync=True)\n",
    "    # modality2_label = traitlets.Unicode().tag(sync=True)\n",
    "\n",
    "    mark_colors = [\"#e41a1c\", \"#377eb8\", \"#4daf4a\", \"#984ea3\", \"#ff7f00\", \"#ffff33\", \"#a65628\", \"#f781bf\", \"#999999\"]\n",
    "\n",
    "    def __init__(self, seed=31415, modality1_label='Image', modality2_label='Text', hover_callback=None, unhover_callback=None):\n",
    "        super(ScatterPlotWidget, self).__init__()\n",
    "\n",
    "        self.hover_callback = hover_callback\n",
    "        self.unhover_callback = unhover_callback\n",
    "        self.seed=seed\n",
    "\n",
    "        self.nr_components_widget = widgets.BoundedIntText(\n",
    "            value=2,\n",
    "            min=2,\n",
    "            max=10,\n",
    "            step=1,\n",
    "            description='Nr Components:',\n",
    "            disabled=False,\n",
    "            layout=widgets.Layout(width=\"150px\")\n",
    "        )\n",
    "        self.nr_components_widget.observe(self.onUpdateValue, 'value')\n",
    "\n",
    "        self.x_component_widget = widgets.BoundedIntText(\n",
    "            value=1,\n",
    "            min=1,\n",
    "            max=10,\n",
    "            step=1,\n",
    "            description='X component:',\n",
    "            disabled=False,\n",
    "            layout=widgets.Layout(width=\"150px\")\n",
    "        )\n",
    "        self.x_component_widget.observe(self.update_scatter, 'value')\n",
    "\n",
    "        self.y_component_widget = widgets.BoundedIntText(\n",
    "            value=2,\n",
    "            min=1,\n",
    "            max=10,\n",
    "            step=1,\n",
    "            description='Y component:',\n",
    "            disabled=False,\n",
    "            layout=widgets.Layout(width=\"150px\")\n",
    "        )\n",
    "        self.y_component_widget.observe(self.update_scatter, 'value')\n",
    "\n",
    "        traitlets.dlink((self.nr_components_widget, 'value'), (self.x_component_widget, 'max'))\n",
    "        traitlets.dlink((self.nr_components_widget, 'value'), (self.y_component_widget, 'max'))\n",
    "\n",
    "\n",
    "        self.fig_widget = go.FigureWidget()\n",
    "        # self.fig_widget.add_trace(go.Scatter(name = modality1_label, x=[0,1,2,3], y=[0,1,2,3], mode=\"markers\", marker_color=self.mark_colors[0]))\n",
    "        # self.fig_widget.add_trace(go.Scatter(name = modality2_label, x=[3,2,1,0], y=[0,1,2,3], mode=\"markers\", marker_color=self.mark_colors[1]))\n",
    "        self.fig_widget.update_layout(width=400, \n",
    "                                      height=300, \n",
    "                                      margin=dict(l=10, r=10, t=10, b=10),\n",
    "                                      legend=dict(\n",
    "                                            yanchor=\"top\",\n",
    "                                            y=0.99,\n",
    "                                            xanchor=\"left\",\n",
    "                                            x=0.01  \n",
    "                                            )\n",
    "                                      )\n",
    "\n",
    "        # self.scatter_image = self.fig_widget.data[0]\n",
    "        # self.scatter_text = self.fig_widget.data[1]\n",
    "        \n",
    "        # self.modality1_label = modality1_label\n",
    "        # self.modality2_label = modality2_label\n",
    "\n",
    "        self.select_projection_method = widgets.Dropdown(\n",
    "            description='Method: ',\n",
    "            value='UMAP',\n",
    "            options=list(available_projection_methods),\n",
    "        )\n",
    "        self.select_projection_method.observe(self._update_projection_method, 'value')\n",
    "\n",
    "        self.use_oos_projection = widgets.Checkbox(\n",
    "            value=False,\n",
    "            description='Use out of sample projection',\n",
    "            disabled=not available_projection_methods[self.select_projection_method.value]['OOS'],\n",
    "            indent=False\n",
    "        )\n",
    "        self.use_oos_projection.observe(self._update_projection_method, 'value')\n",
    "\n",
    "        \n",
    "        self.children = [self.select_projection_method, self.use_oos_projection, self.nr_components_widget, widgets.HBox([self.x_component_widget, self.y_component_widget]), self.fig_widget]\n",
    "        \n",
    "\n",
    "    def _update_projection_method(self, change):\n",
    "        print('', available_projection_methods[self.select_projection_method.value]['OOS'])\n",
    "        self.use_oos_projection.disabled = not available_projection_methods[self.select_projection_method.value]['OOS']\n",
    "        self.onUpdateValue(change)\n",
    "\n",
    "    # @traitlets.validate(\"modality1_label\", \"modality2_label\")\n",
    "    # def _validate_modality_label(self, proposal):\n",
    "    #     # TODO: validate modality label\n",
    "    #     return proposal.value\n",
    "    \n",
    "    # @traitlets.observe(\"modality1_label\", \"modality2_label\")\n",
    "    # def onUpdateModalityLabel(self, change):\n",
    "    #     self.scatter_image.name = self.modality1_label\n",
    "    #     self.scatter_text.name = self.modality2_label\n",
    "        # self.fig_widget.update_layout(legend=dict(\n",
    "        #                                     yanchor=\"top\",\n",
    "        #                                     y=0.99,\n",
    "        #                                     xanchor=\"left\",\n",
    "        #                                     x=0.01  \n",
    "        #                                     )\n",
    "        #                               )\n",
    "\n",
    "    @traitlets.validate(\"embedding\")\n",
    "    def _validate_value(self, proposal):\n",
    "        # print(\"TODO: validate embedding\")\n",
    "        # for backwards compatibility map array to dict\n",
    "        if isinstance(proposal.value, np.ndarray):\n",
    "            embeddings = {}\n",
    "            embeddings[\"Modality1\"] = proposal.value[:int(len(proposal.value)/2),:]\n",
    "            embeddings[\"Modality2\"] = proposal.value[int(len(proposal.value)/2):,:]\n",
    "            return embeddings\n",
    "        return proposal.value\n",
    "\n",
    "    @traitlets.observe(\"embedding\")\n",
    "    def onUpdateValue(self, change):\n",
    "        projection_method = available_projection_methods[self.select_projection_method.value]\n",
    "        # TODO: add UI for distance metric\n",
    "        if self.select_projection_method.value == 'PCA':\n",
    "            projection = projection_method['module'](n_components=self.nr_components_widget.value, random_state=self.seed)\n",
    "        else:\n",
    "            projection = projection_method['module'](n_components=self.nr_components_widget.value, metric=\"cosine\", random_state=self.seed)\n",
    "            \n",
    "        if not self.use_oos_projection.disabled and self.use_oos_projection.value:\n",
    "            # only possible with umap; fit by one modality only and project other modalities out of sample\n",
    "            projection.fit(self.embedding[list(self.embedding.keys())[0]])\n",
    "\n",
    "            # project_by = 'image' # TODO: add user select for this\n",
    "            # if project_by == \"image\":\n",
    "            #     self.image_embedding_projection = projection.fit_transform(self.embedding[:int(len(self.embedding)/2),:])\n",
    "            #     self.text_embedding_projection = projection.transform(self.embedding[int(len(self.embedding)/2):,:])\n",
    "            # elif project_by == \"text\":\n",
    "            #     self.text_embedding_projection = projection.fit_transform(self.embedding[int(len(self.embedding)/2):,:])\n",
    "            #     self.image_embedding_projection = projection.transform(self.embedding[:int(len(self.embedding)/2),:])\n",
    "\n",
    "            # self.embedding_projection = np.concatenate((self.image_embedding_projection, self.text_embedding_projection))\n",
    "            \n",
    "        else:\n",
    "            projection.fit(np.concatenate(list(self.embedding.values())))\n",
    "            # self.embedding_projection = projection.fit_transform(self.embedding)\n",
    "            # self.image_embedding_projection = self.embedding_projection[:int(len(self.embedding)/2),:]\n",
    "            # self.text_embedding_projection = self.embedding_projection[int(len(self.embedding)/2):,:]\n",
    "\n",
    "        embedding_projection = {}\n",
    "        for modality in self.embedding.keys():\n",
    "            embedding_projection[modality] = projection.transform(self.embedding[modality])\n",
    "        self.embedding_projection = embedding_projection\n",
    "\n",
    "        self.update_scatter(change)\n",
    "\n",
    "    @traitlets.observe(\"hover_idcs\")\n",
    "    def hover_changed(self, change):\n",
    "        shapes = [line for line in self.fig_widget.layout.shapes if line.name == 'pair_connections']\n",
    "\n",
    "        modalities = list(self.embedding_projection.keys())\n",
    "\n",
    "        if type(self.hover_idcs) is list:\n",
    "            # if a list of line indices is given, we highlight the circles and draw lines to the centroid of all modalities of this line\n",
    "            for line_idx in self.hover_idcs:\n",
    "                centroid = np.stack(list(self.embedding_projection.values()))[:,line_idx,:].mean(axis=0)\n",
    "                for i in range(len(modalities)):\n",
    "                    modality = modalities[i]\n",
    "                    shapes.append(go.layout.Shape(name='hover_line', \n",
    "                                                    type='line', \n",
    "                                                    x0=self.embedding_projection[modality][line_idx,self.x_component_widget.value-1], \n",
    "                                                    y0=self.embedding_projection[modality][line_idx,self.y_component_widget.value-1], \n",
    "                                                    x1=centroid[0], \n",
    "                                                    y1=centroid[1], \n",
    "                                                    line=dict(color=\"yellow\", width=1)))\n",
    "                    \n",
    "                    shapes.append(go.layout.Shape(name=\"hover_circle\",\n",
    "                                                type=\"circle\",\n",
    "                                                xref=\"x\",\n",
    "                                                yref=\"y\",\n",
    "                                                x0=self.embedding_projection[modality][line_idx,self.x_component_widget.value-1]-0.01,\n",
    "                                                x1=self.embedding_projection[modality][line_idx,self.x_component_widget.value-1]+0.01,\n",
    "                                                y0=self.embedding_projection[modality][line_idx,self.y_component_widget.value-1]-0.01,\n",
    "                                                y1=self.embedding_projection[modality][line_idx,self.y_component_widget.value-1]+0.01,\n",
    "                                                fillcolor=\"yellow\",\n",
    "                                                line_color=\"yellow\"))\n",
    "        elif type(self.hover_idcs) is dict:\n",
    "            # if a dict of modalities and line indices is given, we highlight the circles of the given modality and line index\n",
    "            for modality in self.hover_idcs.keys():\n",
    "                line_idx = self.hover_idcs[modality]\n",
    "                shapes.append(go.layout.Shape(name=\"hover_circle\",\n",
    "                                            type=\"circle\",\n",
    "                                            xref=\"x\",\n",
    "                                            yref=\"y\",\n",
    "                                            x0=self.embedding_projection[modality][line_idx,self.x_component_widget.value-1]-0.01,\n",
    "                                            x1=self.embedding_projection[modality][line_idx,self.x_component_widget.value-1]+0.01,\n",
    "                                            y0=self.embedding_projection[modality][line_idx,self.y_component_widget.value-1]-0.01,\n",
    "                                            y1=self.embedding_projection[modality][line_idx,self.y_component_widget.value-1]+0.01,\n",
    "                                            # fillcolor=\"yellow\",\n",
    "                                            line_color=\"yellow\"))\n",
    "\n",
    "\n",
    "        self.fig_widget.layout.shapes = shapes\n",
    "\n",
    "    def _on_hover(self, trace, points, state):\n",
    "        if len(points.point_inds) < 1:\n",
    "            return\n",
    "        line_idx = points.point_inds[0]\n",
    "\n",
    "        self.hover_idcs = [line_idx]\n",
    "\n",
    "        if self.hover_callback is not None:\n",
    "            self.hover_callback(trace, points, state)\n",
    "\n",
    "    def _on_unhover(self, trace, points, state):\n",
    "        # self.fig_widget.layout.shapes = [line for line in self.fig_widget.layout.shapes if line.name == 'pair_connections']\n",
    "        if self.unhover_callback is not None:\n",
    "            self.unhover_callback(trace, points, state)\n",
    "\n",
    "    def update_scatter(self, change):\n",
    "        modalities = list(self.embedding_projection.keys())\n",
    "        for i, modality in enumerate(modalities):\n",
    "            x_data = self.embedding_projection[modality][:,self.x_component_widget.value-1]\n",
    "            y_data = self.embedding_projection[modality][:,self.y_component_widget.value-1]\n",
    "            if len(self.fig_widget.data) <= i: # add a new trace\n",
    "                trace = go.Scatter(name=modality, x=x_data, y=y_data, mode=\"markers\", marker_color=self.mark_colors[i], hoverinfo=\"text\")\n",
    "                self.fig_widget.add_trace(trace)\n",
    "                self.fig_widget.data[i].on_hover(self._on_hover)\n",
    "                self.fig_widget.data[i].on_unhover(self._on_unhover)\n",
    "            else:\n",
    "                self.fig_widget.data[i].x = x_data\n",
    "                self.fig_widget.data[i].y = y_data\n",
    "                self.fig_widget.data[i].name = modality\n",
    "\n",
    "        # self.scatter_image.x = self.image_embedding_projection[:,self.x_component_widget.value-1]\n",
    "        # self.scatter_image.y = self.image_embedding_projection[:,self.y_component_widget.value-1]\n",
    "\n",
    "        # self.scatter_text.x = self.text_embedding_projection[:,self.x_component_widget.value-1]\n",
    "        # self.scatter_text.y = self.text_embedding_projection[:,self.y_component_widget.value-1]\n",
    "\n",
    "        lines = []\n",
    "        for line_idx in range(len(list(self.embedding_projection.values())[0])):\n",
    "            \n",
    "            # for i in range(len(modalities)-1):\n",
    "            #     modality1 = modalities[i]\n",
    "            #     modality2 = modalities[i+1]\n",
    "            #     lines.append(go.layout.Shape(name='pair_connections', \n",
    "            #                                  type='line', \n",
    "            #                                  x0=self.embedding_projection[modality1][line_idx,self.x_component_widget.value-1], \n",
    "            #                                  y0=self.embedding_projection[modality1][line_idx,self.y_component_widget.value-1], \n",
    "            #                                  x1=self.embedding_projection[modality2][line_idx,self.x_component_widget.value-1], \n",
    "            #                                  y1=self.embedding_projection[modality2][line_idx,self.y_component_widget.value-1], \n",
    "            #                                  line=dict(color=\"grey\", width=1)))\n",
    "\n",
    "            centroid = np.stack(list(self.embedding_projection.values()))[:,line_idx,:].mean(axis=0)\n",
    "            for i in range(len(modalities)):\n",
    "                modality = modalities[i]\n",
    "                lines.append(go.layout.Shape(name='pair_connections', \n",
    "                                             type='line', \n",
    "                                             x0=self.embedding_projection[modality][line_idx,self.x_component_widget.value-1], \n",
    "                                             y0=self.embedding_projection[modality][line_idx,self.y_component_widget.value-1], \n",
    "                                             x1=centroid[0], \n",
    "                                             y1=centroid[1], \n",
    "                                             line=dict(color=\"grey\", width=1),\n",
    "                                             opacity=0.4,))\n",
    "            \n",
    "        self.fig_widget.layout.shapes = lines\n",
    "\n",
    "\n",
    "    @traitlets.validate(\"cluster\")\n",
    "    def _validate_cluster(self, proposal):\n",
    "        # takes a list of cluster labels + sizes\n",
    "        # print(\"TODO: validate cluster\")\n",
    "        return proposal.value\n",
    "\n",
    "    @traitlets.observe(\"cluster\")\n",
    "    def onUpdateCluster(self, change):\n",
    "        print(change)\n",
    "\n",
    "\n",
    "\n",
    "class CLIPExplorerWidget(widgets.AppLayout):\n",
    "    def __init__(self, dataset_name, all_images=None, all_prompts=None, all_data=None, models=None):\n",
    "        # using \"all_images\" and \"all_prompts\" is deprecated; use the \"all_data\" dict instead; e.g. {\"image\": all_images, \"text\": all_prompts}\n",
    "        ### models... list of strings or instances that inherit from CLIPModelInterface \n",
    "        super(CLIPExplorerWidget, self).__init__()\n",
    "        \n",
    "        # for backwards compatibility\n",
    "        if all_data is None:\n",
    "            all_data = {}\n",
    "            print('WARNING: using all_images and all_prompts is deprecated; use the \"all_data\" dict instead; e.g. {\"image\": all_images, \"text\": all_prompts}')\n",
    "            if all_images is not None:\n",
    "                all_data[\"image\"] = all_images\n",
    "            if all_prompts is not None:\n",
    "                all_data[\"text\"] = all_prompts\n",
    "\n",
    "        if models is None:\n",
    "            models = am_model.available_CLIP_models\n",
    "\n",
    "        self.models = {}\n",
    "        for m in models:\n",
    "            if type(m) == str:\n",
    "                self.models[m] = am_model.get_model(m)\n",
    "            elif issubclass(type(m), am_model.CLIPModelInterface):\n",
    "                self.models[m.model_name] = m\n",
    "            else:\n",
    "                print('skipped', m, 'because it is not string or of type CLIPModelInterface')\n",
    "\n",
    "        \n",
    "        self.dataset_name = dataset_name\n",
    "        self.all_data = all_data\n",
    "        self.size = len(all_data[list(all_data.keys())[0]])\n",
    "\n",
    "        # ui select widgets\n",
    "        self.model_select_widget = widgets.Dropdown(\n",
    "            description='Model: ',\n",
    "            value=list(self.models.keys())[0],\n",
    "            options=list(self.models.keys()),\n",
    "        )\n",
    "\n",
    "        m = self.models[self.model_select_widget.value]\n",
    "        self.available_modalities = set(list(self.all_data.keys())).intersection(m.encoding_functions.keys())\n",
    "        \n",
    "\n",
    "        self.close_modality_gap_widget = widgets.Checkbox(\n",
    "            value=False,\n",
    "            description='Close modality gap',\n",
    "            disabled=False,\n",
    "            indent=False\n",
    "        )\n",
    "\n",
    "        self.modality1_select_widget = widgets.Dropdown(\n",
    "            description='Modality 1: ',\n",
    "            value=list(self.available_modalities)[0],\n",
    "            options=list(self.available_modalities),\n",
    "        )\n",
    "        \n",
    "        self.modality2_select_widget = widgets.Dropdown(\n",
    "            description='Modality 2: ',\n",
    "            value=list(self.available_modalities)[1],\n",
    "            options=list(self.available_modalities),\n",
    "        )\n",
    "        \n",
    "        # TODO: make dynamic -> users should be able to add/remove modalities dynamically\n",
    "        if len(self.available_modalities) != 2:\n",
    "            self.close_modality_gap_widget.layout.visibility = 'hidden'\n",
    "        \n",
    "            modality1_data = list(self.all_data.values())[0]\n",
    "        else:\n",
    "            modality1_data = self.all_data[self.modality1_select_widget.value]\n",
    "            # modality2_data = self.all_data[self.modality2_select_widget.value]\n",
    "\n",
    "        # output widgets\n",
    "        self.hover_widget = HoverWidget()\n",
    "\n",
    "        self.embeddings, self.logit_scale = get_embeddings_per_modality(m, self.dataset_name, self.all_data)\n",
    "        self.scatter_widget = ScatterPlotWidget(hover_callback=self.scatter_hover_fn, unhover_callback=self.scatter_unhover_fn)#(modality1_label=modality1_data.name, modality2_label=modality2_data.name)\n",
    "        \n",
    "        self.heatmap_widget = SimilarityHeatmapClusteringWidget(\n",
    "            cluster_label_data=modality1_data, \n",
    "            hover_callback=self.heatmap_hover_fn)\n",
    "        \n",
    "        self.log_widget = widgets.Output()\n",
    "\n",
    "        # TODO: make dynamic -> users should be able to add/remove modalities dynamically\n",
    "        if len(self.available_modalities) == 2:\n",
    "            embedding_modality1 = self.embeddings[self.modality1_select_widget.value]\n",
    "            embedding_modality2 = self.embeddings[self.modality2_select_widget.value]\n",
    "            self.scatter_widget.embedding = {self.modality1_select_widget.value: embedding_modality1, self.modality2_select_widget.value: embedding_modality2} \n",
    "            # self.scatter_widget.embedding = np.concatenate((embedding_modality1, embedding_modality2))\n",
    "\n",
    "            # TODO: calculate this for all modality combinations\n",
    "            modality_distance = get_modality_distance(embedding_modality1, embedding_modality2)\n",
    "            validation_loss = calculate_val_loss(embedding_modality1, embedding_modality2, self.logit_scale.exp())\n",
    "            with self.log_widget:\n",
    "                print('Modality distance: %.2f | Loss: %.2f'%(modality_distance, validation_loss))\n",
    "\n",
    "            self.heatmap_widget.embedding = {self.modality1_select_widget.value: embedding_modality1, self.modality2_select_widget.value: embedding_modality2} #np.concatenate((embedding_modality1, embedding_modality2))\n",
    "        else:\n",
    "            embedding_modality1 = self.embeddings[self.modality1_select_widget.value]\n",
    "            embedding_modality2 = self.embeddings[self.modality2_select_widget.value]\n",
    "            self.scatter_widget.embedding = self.embeddings\n",
    "            self.heatmap_widget.embedding = self.embeddings\n",
    "            with self.log_widget:\n",
    "                print(\"ready\")\n",
    "\n",
    "        # callback functions\n",
    "        self.model_select_widget.observe(self.model_changed, names=\"value\")\n",
    "        self.close_modality_gap_widget.observe(self.model_changed, names='value')\n",
    "        self.modality1_select_widget.observe(self.modality_changed, names=\"value\")\n",
    "        self.modality2_select_widget.observe(self.modality_changed, names=\"value\")\n",
    "\n",
    "        # display everyting\n",
    "        header_list = []\n",
    "        header_list.append(widgets.HBox([self.model_select_widget, self.close_modality_gap_widget]))\n",
    "        if len(self.available_modalities) == 2: # TODO: make dynamic -> users should be able to add/remove modalities dynamically\n",
    "            header_list.append(widgets.HBox([self.modality1_select_widget, self.modality2_select_widget]))\n",
    "        header_list.append(self.log_widget)\n",
    "        \n",
    "        self.header = widgets.VBox(header_list)\n",
    "        self.header.layout.height = '%ipx'%(40*len(header_list))\n",
    "        vis_widgets = widgets.HBox([self.heatmap_widget, self.scatter_widget])\n",
    "        self.center = vis_widgets\n",
    "        self.right_sidebar = self.hover_widget\n",
    "        self.height = '700px'\n",
    "\n",
    "\n",
    "    def model_changed(self, change):\n",
    "\n",
    "        self.log_widget.clear_output()\n",
    "        with self.log_widget:\n",
    "            print(\"loading...\")\n",
    "\n",
    "        m = self.models[self.model_select_widget.value]\n",
    "        self.available_modalities = set(list(self.all_data.keys())).intersection(m.encoding_functions.keys())\n",
    "        self.modality1_select_widget.options = list(self.available_modalities)\n",
    "        self.modality2_select_widget.options = list(self.available_modalities)\n",
    "\n",
    "        self.embeddings, self.logit_scale = get_embeddings_per_modality(m, self.dataset_name, self.all_data)\n",
    "        \n",
    "        # TODO: make dynamic -> users should be able to add/remove modalities dynamically\n",
    "        if len(self.available_modalities) == 2:\n",
    "            self.modality_changed(change)\n",
    "        else:\n",
    "            self.scatter_widget.embedding = self.embeddings\n",
    "            self.heatmap_widget.embedding = self.embeddings\n",
    "            self.log_widget.clear_output()\n",
    "            with self.log_widget:\n",
    "                print('ready')\n",
    "\n",
    "\n",
    "\n",
    "    def modality_changed(self, change):\n",
    "        self.log_widget.clear_output()\n",
    "        with self.log_widget:\n",
    "            print(\"loading...\")\n",
    "            \n",
    "        modality1_data = self.all_data[self.modality1_select_widget.value]\n",
    "        modality2_data = self.all_data[self.modality2_select_widget.value]\n",
    "\n",
    "        # self.scatter_widget.modality1_label = modality1_data.name\n",
    "        # self.scatter_widget.modality2_label = modality2_data.name\n",
    "        # self.heatmap_widget.modality1_label = modality1_data.name\n",
    "        # self.heatmap_widget.modality2_label = modality2_data.name\n",
    "        self.heatmap_widget.cluster_label_data = modality1_data\n",
    "\n",
    "        embedding_modality1 = self.embeddings[self.modality1_select_widget.value]\n",
    "        embedding_modality2 = self.embeddings[self.modality2_select_widget.value]\n",
    "\n",
    "        if self.close_modality_gap_widget.value:\n",
    "            embedding_modality1, embedding_modality2 = get_closed_modality_gap(embedding_modality1, embedding_modality2)\n",
    "            # image_embedding, text_embedding = get_closed_modality_gap_rotated(image_embedding, text_embedding)\n",
    "\n",
    "        self.scatter_widget.embedding = np.concatenate((embedding_modality1, embedding_modality2))\n",
    "        # self.heatmap_widget.embedding = np.concatenate((embedding_modality1, embedding_modality2))\n",
    "        self.heatmap_widget.embedding = {self.modality1_select_widget.value: embedding_modality1, self.modality2_select_widget.value: embedding_modality2}\n",
    "\n",
    "        modality_distance = get_modality_distance(embedding_modality1, embedding_modality2)\n",
    "        # modality_distance = get_modality_distance_rotated(image_embedding, text_embedding)\n",
    "        \n",
    "        validation_loss = calculate_val_loss(embedding_modality1, embedding_modality2, self.logit_scale.exp())\n",
    "\n",
    "        self.log_widget.clear_output()\n",
    "        with self.log_widget:\n",
    "            print('Modality distance: %.2f | Loss: %.2f'%(modality_distance, validation_loss))\n",
    "\n",
    "\n",
    "    def heatmap_hover_fn(self, x_idx, y_idx, x_modality, y_modality):\n",
    "        self.hover_widget.valueX = self.all_data[x_modality].getVisItem(x_idx)\n",
    "        self.hover_widget.valueY = self.all_data[y_modality].getVisItem(y_idx)\n",
    "        # modality1_data = self.all_data[self.modality1_select_widget.value]\n",
    "        # modality2_data = self.all_data[self.modality2_select_widget.value]\n",
    "        # # show hover images/texts\n",
    "        # if x_modality == \"modality1\":\n",
    "        #     self.hover_widget.valueX = modality1_data.getVisItem(x_idx)\n",
    "        # else:\n",
    "        #     self.hover_widget.valueX = modality2_data.getVisItem(x_idx)\n",
    "        \n",
    "        # if y_modality == \"modality1\":\n",
    "        #     self.hover_widget.valueY = modality1_data.getVisItem(y_idx)\n",
    "        # else:\n",
    "        #     self.hover_widget.valueY = modality2_data.getVisItem(y_idx)\n",
    "\n",
    "        self.scatter_widget.hover_idcs = {x_modality: x_idx, y_modality: y_idx}\n",
    "\n",
    "    def scatter_hover_fn(self, trace, points, state):\n",
    "        if len(points.point_inds) < 1:\n",
    "            return\n",
    "        idx = points.point_inds[0]\n",
    "        # print(trace.name, idx) # image vs text trace\n",
    "        modality1_data = self.all_data[self.modality1_select_widget.value]\n",
    "        modality2_data = self.all_data[self.modality2_select_widget.value]\n",
    "\n",
    "        self.hover_widget.valueY = modality1_data.getVisItem(idx)\n",
    "        self.hover_widget.valueX = modality2_data.getVisItem(idx)\n",
    "        \n",
    "        inverse_idcs = np.argsort(self.heatmap_widget.idcs)\n",
    "        heatmap_idx = inverse_idcs[idx]\n",
    "        # self.heatmap_widget.hover_idx = [(heatmap_idx, self.size + heatmap_idx), (self.size + heatmap_idx, heatmap_idx)]\n",
    "        self.heatmap_widget.hover_idx = [(heatmap_idx + i*self.size, heatmap_idx + i*self.size) for i in range(len(self.available_modalities))]\n",
    "\n",
    "\n",
    "    def scatter_unhover_fn(self, trace, points, state):\n",
    "        self.heatmap_widget.hover_idx = []\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_21828\\2517419370.py:64: FutureWarning: The input object of type 'Image' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Image', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  self.all_images = np.array(all_images)\n",
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_21828\\2517419370.py:64: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.all_images = np.array(all_images)\n",
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_21828\\2517419370.py:66: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  self.all_audios = np.array(all_audios)\n",
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_21828\\2517419370.py:66: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.all_audios = np.array(all_audios)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Christina\\anaconda3\\envs\\amumo_test\\lib\\site-packages\\torchvision\\transforms\\_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Christina\\anaconda3\\envs\\amumo_test\\lib\\site-packages\\torchvision\\transforms\\_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found cached embeddings for test_audio_ImageBind_huge text\n",
      "found cached embeddings for test_audio_ImageBind_huge image\n",
      "found cached embeddings for test_audio_ImageBind_huge audio\n",
      " False\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a490918a35ab461f93a7892f03730a2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CLIPExplorerWidget(children=(VBox(children=(HBox(children=(Dropdown(description='Model: ', options=('ImageBind…"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Triplet_Dataset(path=\"C:\\\\Users\\\\Christina\\\\Data\\\\imagebind\\\\text-audio-image\\\\\", batch_size=100)\n",
    "all_images, all_prompts, all_audios = dataset.get_data()\n",
    "print(len(all_images), len(all_prompts), len(all_audios))\n",
    "\n",
    "my_widget = CLIPExplorerWidget(\"test_audio\", all_data={\"text\": all_prompts, \"image\": all_images, \"audio\": all_audios}, models=[am_model.ImageBind_Model()])\n",
    "my_widget.scatter_widget.select_projection_method.value = \"PCA\"\n",
    "my_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<amumo.data.ImageType at 0x21553e4faf0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_widget.heatmap_widget.cluster_label_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PartialTSNEEmbedding([[ 8.66231252e-01, -1.80334395e+00],\n",
       "                      [-6.98845549e-02,  4.86421987e-01],\n",
       "                      [-1.80532078e+00, -4.17764983e-01],\n",
       "                      [-1.69716285e+00, -4.86806520e-01],\n",
       "                      [ 6.34820991e-01,  4.77157939e-01],\n",
       "                      [-7.02025889e-01,  9.50794310e-01],\n",
       "                      [ 6.78130001e-01,  8.57959178e-01],\n",
       "                      [-1.88099454e+00,  8.72435976e-01],\n",
       "                      [-1.56499882e-02,  1.42629699e-01],\n",
       "                      [ 2.93348297e-01,  2.69271462e+00],\n",
       "                      [ 1.79785216e+00, -2.89591829e-01],\n",
       "                      [ 8.25588826e-01, -3.33093071e-01],\n",
       "                      [-3.73619880e-01, -8.62589914e-01],\n",
       "                      [-6.13675611e-01,  1.70157657e+00],\n",
       "                      [ 1.13072201e+00, -1.31279701e+00],\n",
       "                      [ 1.03003015e+00,  9.68657049e-01],\n",
       "                      [-1.19196244e+00, -7.00524880e-01],\n",
       "                      [ 3.59805494e-01, -9.34062436e-02],\n",
       "                      [-1.57394147e+00, -3.81698429e-01],\n",
       "                      [-1.38493737e+00, -4.87656220e-01],\n",
       "                      [-1.73849665e+00,  1.30661619e+00],\n",
       "                      [ 4.69603287e-01, -1.10053191e+00],\n",
       "                      [-1.85401090e+00, -4.63749291e-01],\n",
       "                      [ 8.62070662e-01,  6.53581685e-01],\n",
       "                      [ 2.36916843e+00, -1.29850527e+00],\n",
       "                      [ 1.54058995e+00,  1.54267608e+00],\n",
       "                      [ 1.12519129e+00,  1.69921872e-01],\n",
       "                      [-1.33976856e+00, -1.95693303e-01],\n",
       "                      [-1.74885858e+00, -4.57466235e-01],\n",
       "                      [-1.84019753e+00, -3.91253715e-01],\n",
       "                      [-7.10629694e-01, -5.62369836e-01],\n",
       "                      [ 1.52154671e+00, -2.26323855e+00],\n",
       "                      [-3.59708323e-01,  6.53028378e-01],\n",
       "                      [-8.45590964e-01,  8.39538277e-01],\n",
       "                      [ 8.71761578e-01,  7.86592020e-01],\n",
       "                      [ 1.46027409e+00,  2.25379438e-01],\n",
       "                      [ 4.86747374e-01,  7.41036706e-01],\n",
       "                      [ 1.79141724e+00, -6.17732723e-01],\n",
       "                      [-7.73905933e-01,  8.45014011e-01],\n",
       "                      [-1.73132760e+00,  1.14963643e+00],\n",
       "                      [ 7.53461010e-01, -4.83165795e-01],\n",
       "                      [ 1.30760860e-01,  5.82341108e-01],\n",
       "                      [ 4.85478865e-01,  9.70411254e-01],\n",
       "                      [ 6.99849711e-02, -1.07213897e-01],\n",
       "                      [ 5.98076264e-01,  1.57475883e+00],\n",
       "                      [ 9.09518359e-01, -3.50901714e-01],\n",
       "                      [-1.03265267e+00, -1.39754573e-01],\n",
       "                      [ 1.33741519e+00,  3.34620140e-01],\n",
       "                      [ 1.85316509e+00, -8.66616851e-01],\n",
       "                      [ 7.13854913e-01,  6.96264427e-01],\n",
       "                      [-1.23785241e+00, -4.89448989e-01],\n",
       "                      [ 1.50616016e+00,  3.78724045e-04],\n",
       "                      [ 1.83442901e+00, -5.59370815e-01],\n",
       "                      [-4.40891037e-01,  1.66516006e+00],\n",
       "                      [-1.76278783e+00, -7.72983816e-01],\n",
       "                      [-1.58533177e-01,  2.17088859e-03],\n",
       "                      [ 1.44191733e+00, -6.44938636e-01],\n",
       "                      [ 3.92938145e-01, -1.75172995e+00],\n",
       "                      [ 1.42323622e-01,  5.32800731e-01],\n",
       "                      [ 9.38900226e-01, -5.09669136e-01],\n",
       "                      [ 1.43536635e+00,  8.07487693e-02],\n",
       "                      [-7.12271285e-01, -1.92067515e-02],\n",
       "                      [-3.07681662e+00, -2.94524947e-01],\n",
       "                      [ 9.38678966e-01,  9.20833914e-01],\n",
       "                      [ 1.12357831e+00,  8.91754679e-02],\n",
       "                      [ 8.71742936e-01,  3.95757747e-01],\n",
       "                      [ 1.11563716e+00,  9.94055200e-01],\n",
       "                      [-1.61168403e+00,  1.97060078e+00],\n",
       "                      [-1.28725168e+00, -3.38750143e-01],\n",
       "                      [ 5.94465706e-01, -1.04923300e+00],\n",
       "                      [-8.96757713e-01, -4.72063101e-01],\n",
       "                      [ 1.28687792e+00, -2.73019463e+00],\n",
       "                      [-6.18864711e-01, -4.47773973e-01],\n",
       "                      [-1.27638240e+00, -2.40858343e+00],\n",
       "                      [ 7.93887597e-01,  5.39978960e-01],\n",
       "                      [-1.14842899e-02,  6.94548650e-01],\n",
       "                      [-1.81724634e+00,  8.38377359e-01],\n",
       "                      [ 1.28702799e+00, -6.93361049e-01],\n",
       "                      [ 3.24353667e-01,  5.52903747e-01],\n",
       "                      [-9.59983885e-01,  2.12443083e+00],\n",
       "                      [-1.42369511e+00, -5.29626951e-01],\n",
       "                      [-4.10343441e-01,  1.80024402e-01],\n",
       "                      [ 2.44738973e-01, -7.10713835e-01],\n",
       "                      [-1.62432537e+00, -7.76180537e-01],\n",
       "                      [ 1.82394371e+00, -7.01573238e-01],\n",
       "                      [-2.14903252e+00,  1.99517194e+00],\n",
       "                      [ 2.29119881e+00, -1.33138696e+00],\n",
       "                      [ 3.33479562e-01,  1.23440773e+00],\n",
       "                      [ 6.29515555e-01, -4.86417046e-01],\n",
       "                      [ 1.05024445e+00, -5.29534252e-01],\n",
       "                      [ 1.04128389e-01,  2.32304786e-01],\n",
       "                      [-1.60225360e+00, -4.29806979e-01],\n",
       "                      [-1.51832568e+00, -1.30532097e+00],\n",
       "                      [-1.79348324e+00, -5.26680188e-01],\n",
       "                      [-1.95347568e+00,  4.99278037e-01],\n",
       "                      [-1.83111869e+00, -4.13196746e-01],\n",
       "                      [-6.80968573e-01,  1.42687844e+00],\n",
       "                      [ 1.56669390e+00, -2.35092238e+00],\n",
       "                      [ 1.74571326e+00,  1.81269059e+00],\n",
       "                      [ 5.20085845e-01,  6.27048979e-01]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TSNE().fit(np.random.rand(100, 100)).transform(np.random.rand(100, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100\n",
      "found cached embeddings for test_audio_ImageBind_huge image\n",
      "found cached embeddings for test_audio_ImageBind_huge text\n",
      "found cached embeddings for test_audio_ImageBind_huge audio\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7347af2a15474d6bbbabe1be45bae8a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SimilarityHeatmapClusteringWidget(children=(HBox(children=(Checkbox(value=False, description='Cluster matrix b…"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "embeddings, logit_scale = get_embeddings_per_modality(am_model.ImageBind_Model(), \"test_audio\", {\"image\": all_images, \"text\": all_prompts, \"audio\": all_audios}, batch_size=100)\n",
    "\n",
    "heatmap = SimilarityHeatmapClusteringWidget(cluster_label_data=all_prompts)\n",
    "heatmap.embedding = {\"image\": np.array(embeddings[\"image\"]), \"text\": np.array(embeddings[\"text\"]), \"audio\": np.array(embeddings[\"audio\"])}\n",
    "heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 200)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heatmap.value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities_all = get_similarities_all(heatmap.embedding)\n",
    "len(similarities_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 300)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(similarities_all[np.concatenate([heatmap.idcs + i*heatmap.size for i in range(len(heatmap.modality_labels))], axis=0),:][:,np.concatenate([heatmap.idcs + i*heatmap.size for i in range(len(heatmap.modality_labels))], axis=0)]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_30488\\3688791554.py:54: FutureWarning: The input object of type 'Image' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Image', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  self.all_images = np.array(all_images)\n",
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_30488\\3688791554.py:54: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.all_images = np.array(all_images)\n",
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_30488\\3688791554.py:56: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  self.all_audios = np.array(all_audios)\n",
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_30488\\3688791554.py:56: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.all_audios = np.array(all_audios)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Christina\\anaconda3\\envs\\amumo_test\\lib\\site-packages\\torchvision\\transforms\\_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Christina\\anaconda3\\envs\\amumo_test\\lib\\site-packages\\torchvision\\transforms\\_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found cached embeddings for test_audio_ImageBind_huge text\n",
      "found cached embeddings for test_audio_ImageBind_huge image\n",
      "found cached embeddings for test_audio_ImageBind_huge audio\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffcae93c2a0d4d73929b84c77890dc6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CLIPExplorerWidget(children=(VBox(children=(HBox(children=(Dropdown(description='Model: ', options=('ImageBind…"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "my_widget = am_widgets.CLIPExplorerWidget(\"test_audio\", all_data={\"text\": all_prompts, \"image\": all_images, \"audio\": all_audios }, models=[am_model.ImageBind_Model()]) \n",
    "my_widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image-Thermal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ThermalType(am_data.ImageType):\n",
    "    name = \"Thermal\"\n",
    "\n",
    "    def __init__(self, data) -> None:\n",
    "        super().__init__(data)\n",
    "\n",
    "\n",
    "class LLVIP_Dataset(am_data.DatasetInterface):\n",
    "    name='LLVIP'\n",
    "\n",
    "    def __init__(self, path, seed=31415, batch_size = 100):\n",
    "        # download dataset: https://bupt-ai-cz.github.io/LLVIP/\n",
    "        super().__init__(path, seed, batch_size)\n",
    "        # path: path to the LLVIP dataset\n",
    "        image_paths = glob(path + \"\\\\visible\\\\test\\\\*.jpg\", recursive = True)\n",
    "        thermal_paths = glob(path + \"\\\\infrared\\\\test\\\\*.jpg\", recursive = True)\n",
    "        \n",
    "        batch_idcs = self._get_random_subsample(len(image_paths))\n",
    "        image_paths = np.array(image_paths)[batch_idcs]\n",
    "        thermal_paths = np.array(thermal_paths)[batch_idcs]\n",
    "        \n",
    "        all_images = []\n",
    "        for image_path in image_paths:\n",
    "            with open(image_path, \"rb\") as fopen:\n",
    "                image = Image.open(fopen).convert(\"RGB\")\n",
    "                all_images.append(image)\n",
    "\n",
    "            \n",
    "        all_thermals = []\n",
    "        \n",
    "        for thermal_path in thermal_paths:\n",
    "            with open(thermal_path, \"rb\") as fopen:\n",
    "                thermal = Image.open(fopen).convert(\"L\")\n",
    "                all_thermals.append(thermal)\n",
    "\n",
    "        self.MODE2_Type = ThermalType\n",
    "\n",
    "        # TODO... load images and thermals on demand with a custom loader\n",
    "        self.all_images = np.array(all_images)\n",
    "        self.all_prompts = np.array(all_thermals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_24244\\3221552142.py:99: FutureWarning: The input object of type 'Image' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Image', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  self.all_images = np.array(all_images)\n",
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_24244\\3221552142.py:99: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.all_images = np.array(all_images)\n",
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_24244\\3221552142.py:100: FutureWarning: The input object of type 'Image' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Image', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  self.all_prompts = np.array(all_thermals)\n",
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_24244\\3221552142.py:100: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.all_prompts = np.array(all_thermals)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100\n",
      "found cached embeddings for test_thermal_ImageBind_huge image\n",
      "batch 1 of 1\n",
      "torch.Size([100, 1, 224, 224])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17508ad4cb6642ec8692300f7a57d876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CLIPExplorerWidget(children=(VBox(children=(HBox(children=(Dropdown(description='Model: ', options=('ImageBind…"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download dataset: https://bupt-ai-cz.github.io/LLVIP/\n",
    "dataset = LLVIP_Dataset(path=\"C:\\\\Users\\\\Christina\\\\Data\\\\imagebind\\\\LLVIP\\\\\", batch_size=100) \n",
    "all_images, all_thermals = dataset.get_data()\n",
    "print(len(all_images), len(all_thermals))\n",
    "\n",
    "am_widgets.CLIPExplorerWidget(\"test_thermal\", all_data={\"image\": all_images, \"thermal\": all_thermals}, models=[am_model.ImageBind_Model()]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image-Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class DepthType(am_data.ImageType):\n",
    "    name = \"Depth\"\n",
    "\n",
    "    def __init__(self, data) -> None:\n",
    "        # data is an array of (1,224,224) tensors\n",
    "        super().__init__(data)\n",
    "\n",
    "    def getVisItem(self, idx):\n",
    "        output_img = io.BytesIO()\n",
    "        plt.imsave(output_img, self.data[idx][0], cmap='gray')\n",
    "        plt.savefig(output_img, format='JPEG')\n",
    "        return {\"displayType\": am_data.DisplayTypes.IMAGE, \"value\": output_img.getvalue()}\n",
    "    \n",
    "    \n",
    "\n",
    "class SUNRGBD_Dataset(am_data.DatasetInterface):\n",
    "    name='SUNRGBD_NYU'\n",
    "\n",
    "    def __init__(self, path, seed=31415, batch_size = 100):\n",
    "        # download \"SUNRGBD_V1\" dataset from https://rgbd.cs.princeton.edu/\n",
    "        super().__init__(path, seed, batch_size)\n",
    "        # path: path to the SUNRGBD dataset\n",
    "        image_paths = glob(path + \"\\\\kv1\\\\NYUdata\\\\NYU*\\\\fullres\\\\*.jpg\", recursive = True)\n",
    "        depth_paths = glob(path + \"\\\\kv1\\\\NYUdata\\\\NYU*\\\\fullres\\\\*.png\", recursive = True)\n",
    "        \n",
    "        batch_idcs = self._get_random_subsample(len(image_paths))\n",
    "        image_paths = np.array(image_paths)[batch_idcs]\n",
    "        depth_paths = np.array(depth_paths)[batch_idcs]\n",
    "        \n",
    "        all_images = []\n",
    "        for image_path in image_paths:\n",
    "            with open(image_path, \"rb\") as fopen:\n",
    "                image = Image.open(fopen).convert(\"RGB\")\n",
    "                all_images.append(image)\n",
    "\n",
    "            \n",
    "        all_depths = []\n",
    "        for depth_path in depth_paths:\n",
    "            with open(depth_path, \"rb\") as fopen:\n",
    "                depth = Image.open(fopen)\n",
    "                depth = np.array(depth, dtype=int)\n",
    "                depth = depth.astype(np.float32) / depth.max() # TODO: need to normalize?\n",
    "                # depth = depth[np.newaxis,:,:] # need 1 channel -> (1,224,224)\n",
    "                depth = torch.from_numpy(depth).unsqueeze(0) # need 1 channel -> (1,224,224)\n",
    "                all_depths.append(depth)\n",
    "\n",
    "        self.MODE2_Type = DepthType\n",
    "\n",
    "        # TODO... load images and depths on demand with a custom loader\n",
    "        self.all_images = np.array(all_images)\n",
    "        self.all_prompts = np.array(all_depths)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_24244\\3221552142.py:64: FutureWarning:\n",
      "\n",
      "The input object of type 'Image' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Image', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "\n",
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_24244\\3221552142.py:64: VisibleDeprecationWarning:\n",
      "\n",
      "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\n",
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_24244\\3221552142.py:65: FutureWarning:\n",
      "\n",
      "The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "\n",
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_24244\\3221552142.py:65: VisibleDeprecationWarning:\n",
      "\n",
      "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100\n",
      "found cached embeddings for test_depth_ImageBind_huge image\n",
      "found cached embeddings for test_depth_ImageBind_huge depth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f38b43e86ba4314a512d7d62837bade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CLIPExplorerWidget(children=(VBox(children=(HBox(children=(Dropdown(description='Model: ', options=('ImageBind…"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download \"SUNRGBD_V1\" dataset from https://rgbd.cs.princeton.edu/\n",
    "dataset = SUNRGBD_Dataset(path=\"C:\\\\Users\\\\Christina\\\\Data\\\\imagebind\\\\SUNRGBD\\\\\", batch_size=100)\n",
    "all_images, all_depths = dataset.get_data()\n",
    "print(len(all_images), len(all_depths))\n",
    "\n",
    "am_widgets.CLIPExplorerWidget(\"test_depth\", all_data={\"image\": all_images, \"depth\": all_depths}, models=[am_model.ImageBind_Model()]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amumo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
