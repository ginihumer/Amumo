{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install git+https://github.com/facebookresearch/ImageBind\n",
    "! pip install soundfile\n",
    "! pip install librosa\n",
    "! pip install \"imagebind @ git+https://github.com/facebookresearch/ImageBind@c6a47d6dc2b53eced51d398c181d57049ca59286\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "data_path = \"../../../../../Data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create triplet dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "import soundfile as sf\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_dir = data_path + \"imagebind/text-audio-image/\"\n",
    "if not os.path.exists(triplet_dir):\n",
    "    # https://huggingface.co/datasets/agkphysics/AudioSet\n",
    "    # dataset = load_dataset(\"agkphysics/AudioSet\", \"bal\", split=\"test\")\n",
    "    dataset = load_dataset(\"arrow\", split=\"test\", data_files={\"test\":\"../../../../../.cache/huggingface/datasets/agkphysics___audio_set/bal/audio_set-test-000**-of-00018.arrow\"})#, split=\"test[0:10]\")\n",
    "    # https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.filter\n",
    "    dataset_it = dataset.to_iterable_dataset()\n",
    "\n",
    "    animals = [\"Bird\", \"Cat\", \"Dog\", \"Horse\"]\n",
    "\n",
    "    infos = []\n",
    "    idx = 0\n",
    "    for animal in animals:\n",
    "        for example in dataset_it.filter(lambda x: animal in x[\"human_labels\"]).take(25):\n",
    "            # save audiofile\n",
    "            sf.write(triplet_dir + \"audio/%03d.wav\"%idx, example[\"audio\"][\"array\"], example[\"audio\"][\"sampling_rate\"], format=\"wav\")\n",
    "\n",
    "            # save image\n",
    "            # https://stackoverflow.com/questions/2068344/how-do-i-get-a-youtube-video-thumbnail-from-the-youtube-api\n",
    "            url = 'https://img.youtube.com/vi/%s/hqdefault.jpg'%example[\"video_id\"]\n",
    "            data = requests.get(url).content \n",
    "            with open(data_dir + \"image/%03d.jpg\"%idx,'wb') as f:\n",
    "                f.write(data) \n",
    "\n",
    "            # save info\n",
    "            infos.append({\"id\": idx, \"video_id\":example[\"video_id\"], \"labels\":example[\"human_labels\"], \"animal\": animal})\n",
    "\n",
    "            idx += 1\n",
    "\n",
    "    pd.DataFrame(infos).to_csv(data_dir + \"info.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image-Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amumo import model as am_model\n",
    "from amumo import data as am_data\n",
    "from amumo import utils as am_utils\n",
    "from amumo import widgets as am_widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Helpers\n",
    "def get_data_helper(dataset, filters=[], method=any):\n",
    "    all_images, all_prompts = dataset.get_filtered_data(filters, method=method)\n",
    "    print(len(all_images))\n",
    "\n",
    "    dataset_name = dataset.name\n",
    "    if len(filters) > 0:\n",
    "        dataset_name = dataset_name + '_filter-' + method.__name__ + '_' + '-'.join(filters)\n",
    "    else:\n",
    "        dataset_name = dataset_name + '_size-%i'%len(all_images)\n",
    "\n",
    "    return all_images, all_prompts, dataset_name\n",
    "\n",
    "# subset of mscoco validation data\n",
    "dataset_mscoco_val = am_data.MSCOCO_Val_Dataset(path=data_path+'mscoco/validation', batch_size=100) \n",
    "mscoco_val_images, mscoco_val_prompts, mscoco_val_dataset_name = get_data_helper(dataset_mscoco_val, filters=[], method=any)\n",
    "mscoco_val_dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am_widgets.CLIPExplorerWidget(mscoco_val_dataset_name, all_data={\"image\": mscoco_val_images, \"text\": mscoco_val_prompts}, models=[am_model.ImageBind_Model(), \"CLIP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am_widgets.CLIPComparerWidget(mscoco_val_dataset_name, all_images=mscoco_val_images, all_prompts=mscoco_val_prompts, models=[am_model.ImageBind_Model(), \"CLIP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image-Text-Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amumo import model as am_model\n",
    "from amumo import data as am_data\n",
    "from amumo import utils as am_utils\n",
    "from amumo import widgets as am_widgets\n",
    "# ! pip install torch\n",
    "# ! pip install torchaudio\n",
    "import torch\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "\n",
    "class Triplet_Dataset(am_data.DatasetInterface):\n",
    "    name='Triplet'\n",
    "\n",
    "    def __init__(self, path, seed=31415, batch_size=100, sample_rate=16000):\n",
    "        # create triplet dataset if it does not exist\n",
    "        super().__init__(path, seed, batch_size)\n",
    "        # path: path to the triplet dataset\n",
    "        image_paths = glob(path + \"image/*.jpg\", recursive = True)\n",
    "        audio_paths = glob(path + \"audio/*.wav\", recursive = True)\n",
    "\n",
    "        self.sample_rate = sample_rate\n",
    "        \n",
    "        all_images = []\n",
    "        for image_path in image_paths:\n",
    "            with open(image_path, \"rb\") as fopen:\n",
    "                image = Image.open(fopen).convert(\"RGB\")\n",
    "                all_images.append(image)\n",
    "\n",
    "        all_audios = []\n",
    "        for audio_path in audio_paths:\n",
    "            waveform, sr = torchaudio.load(audio_path)\n",
    "            if sample_rate != sr:\n",
    "                waveform = torchaudio.functional.resample(\n",
    "                    waveform, orig_freq=sr, new_freq=sample_rate\n",
    "                )\n",
    "            all_audios.append(waveform)\n",
    "        \n",
    "        self.all_infos = pd.read_csv(path + \"info.csv\", converters={\"labels\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \")})\n",
    "\n",
    "        # TODO... load on demand with a custom loader\n",
    "        self.all_images = np.array(all_images)\n",
    "        self.all_prompts = np.array(self.all_infos[\"labels\"].map(lambda x: \", \".join(x)))\n",
    "        self.all_audios = np.array(all_audios)\n",
    "    \n",
    "    \n",
    "    def get_data(self):\n",
    "        \n",
    "        if self.batch_size is None:\n",
    "            images = self.MODE1_Type(self.all_images)\n",
    "            texts = self.MODE2_Type(self.all_prompts)\n",
    "            audios = am_data.AudioType(self.all_audios, self.sample_rate)\n",
    "        \n",
    "            return images, texts, audios\n",
    "\n",
    "        # create a random batch\n",
    "        batch_idcs = self._get_random_subsample(len(self.all_images))\n",
    "\n",
    "        images = self.MODE1_Type(self.all_images[batch_idcs])\n",
    "        texts = self.MODE2_Type(self.all_prompts[batch_idcs])\n",
    "        audios = am_data.AudioType(self.all_audios[batch_idcs], self.sample_rate)\n",
    "        \n",
    "        return images, texts, audios\n",
    "    \n",
    "        \n",
    "    def get_filtered_data(self, filter_list, method=any):\n",
    "        # filter_list: a list of strings that are used for filtering\n",
    "        # method: any -> any substring given in filter_list is present; all -> all substrings must be contained in the string\n",
    "        if filter_list is None or len(filter_list) <= 0:\n",
    "            return self.get_data()\n",
    "\n",
    "        subset_ids = np.array([i for i in range(len(self.all_prompts)) if method(substring in self.all_prompts[i].lower() for substring in filter_list)])\n",
    "        if len(subset_ids) <= 0:\n",
    "            print(\"no filter matches found\")\n",
    "            return [], [], []\n",
    "        \n",
    "        # create a random batch\n",
    "        batch_idcs = self._get_random_subsample(len(subset_ids))\n",
    "        subset_ids = subset_ids[batch_idcs]\n",
    "        \n",
    "        images = self.MODE1_Type(self.all_images[subset_ids])\n",
    "        texts = self.MODE2_Type(self.all_prompts[subset_ids])\n",
    "        audios = am_data.AudioType(self.all_audios[subset_ids], self.sample_rate)\n",
    "        return images, texts, audios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Triplet_Dataset(path=triplet_dir, batch_size=100)\n",
    "all_images, all_prompts, all_audios = dataset.get_data()\n",
    "print(len(all_images), len(all_prompts), len(all_audios))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_widget = am_widgets.CLIPExplorerWidget(\"test_audio\", all_data={\"text\": all_prompts, \"audio\": all_audios}, models=[am_model.ImageBind_Model()])\n",
    "my_widget.scatter_widget.select_projection_method.value = \"PCA\"\n",
    "my_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_widget = am_widgets.CLIPExplorerWidget(\"test_audio\", all_data={\"text\": all_prompts, \"audio\": all_audios, \"image\": all_images}, models=[am_model.ImageBind_Model()]) \n",
    "my_widget.scatter_widget.select_projection_method.value = \"PCA\"\n",
    "my_widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image-Thermal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ThermalType(am_data.ImageType):\n",
    "    name = \"Thermal\"\n",
    "\n",
    "    def __init__(self, data) -> None:\n",
    "        super().__init__(data)\n",
    "\n",
    "\n",
    "class LLVIP_Dataset(am_data.DatasetInterface):\n",
    "    name='LLVIP'\n",
    "\n",
    "    def __init__(self, path, seed=31415, batch_size = 100):\n",
    "        # download dataset: https://bupt-ai-cz.github.io/LLVIP/\n",
    "        super().__init__(path, seed, batch_size)\n",
    "        # path: path to the LLVIP dataset\n",
    "        image_paths = glob(path + \"visible/test/*.jpg\", recursive = True)\n",
    "        thermal_paths = glob(path + \"infrared/test/*.jpg\", recursive = True)\n",
    "        \n",
    "        batch_idcs = self._get_random_subsample(len(image_paths))\n",
    "        image_paths = np.array(image_paths)[batch_idcs]\n",
    "        thermal_paths = np.array(thermal_paths)[batch_idcs]\n",
    "        \n",
    "        all_images = []\n",
    "        for image_path in image_paths:\n",
    "            with open(image_path, \"rb\") as fopen:\n",
    "                image = Image.open(fopen).convert(\"RGB\")\n",
    "                all_images.append(image)\n",
    "\n",
    "            \n",
    "        all_thermals = []\n",
    "        \n",
    "        for thermal_path in thermal_paths:\n",
    "            with open(thermal_path, \"rb\") as fopen:\n",
    "                thermal = Image.open(fopen).convert(\"L\")\n",
    "                all_thermals.append(thermal)\n",
    "\n",
    "        self.MODE2_Type = ThermalType\n",
    "\n",
    "        # TODO... load images and thermals on demand with a custom loader\n",
    "        self.all_images = np.array(all_images)\n",
    "        self.all_prompts = np.array(all_thermals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download dataset: https://bupt-ai-cz.github.io/LLVIP/\n",
    "dataset = LLVIP_Dataset(path=data_path+\"imagebind/LLVIP/\", batch_size=100) \n",
    "all_images, all_thermals = dataset.get_data()\n",
    "print(len(all_images), len(all_thermals))\n",
    "\n",
    "am_widgets.CLIPExplorerWidget(\"test_thermal\", all_data={\"image\": all_images, \"thermal\": all_thermals}, models=[am_model.ImageBind_Model()]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image-Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class DepthType(am_data.ImageType):\n",
    "    name = \"Depth\"\n",
    "\n",
    "    def __init__(self, data) -> None:\n",
    "        # data is an array of (1,224,224) tensors\n",
    "        super().__init__(data)\n",
    "\n",
    "    def getVisItem(self, idx):\n",
    "        output_img = io.BytesIO()\n",
    "        plt.imsave(output_img, self.data[idx][0], cmap='gray')\n",
    "        plt.savefig(output_img, format='JPEG')\n",
    "        return {\"displayType\": am_data.DisplayTypes.IMAGE, \"value\": output_img.getvalue()}\n",
    "    \n",
    "    \n",
    "\n",
    "class SUNRGBD_Dataset(am_data.DatasetInterface):\n",
    "    name='SUNRGBD_NYU'\n",
    "\n",
    "    def __init__(self, path, seed=31415, batch_size = 100):\n",
    "        # download \"SUNRGBD_V1\" dataset from https://rgbd.cs.princeton.edu/\n",
    "        super().__init__(path, seed, batch_size)\n",
    "        # path: path to the SUNRGBD dataset\n",
    "        image_paths = glob(path + \"kv1/NYUdata/NYU*/fullres/*.jpg\", recursive = True)\n",
    "        depth_paths = glob(path + \"kv1/NYUdata/NYU*/fullres/*.png\", recursive = True)\n",
    "        \n",
    "        batch_idcs = self._get_random_subsample(len(image_paths))\n",
    "        image_paths = np.array(image_paths)[batch_idcs]\n",
    "        depth_paths = np.array(depth_paths)[batch_idcs]\n",
    "        \n",
    "        all_images = []\n",
    "        for image_path in image_paths:\n",
    "            with open(image_path, \"rb\") as fopen:\n",
    "                image = Image.open(fopen).convert(\"RGB\")\n",
    "                all_images.append(image)\n",
    "\n",
    "            \n",
    "        all_depths = []\n",
    "        for depth_path in depth_paths:\n",
    "            with open(depth_path, \"rb\") as fopen:\n",
    "                depth = Image.open(fopen)\n",
    "                depth = np.array(depth, dtype=int)\n",
    "                depth = depth.astype(np.float32) / depth.max() # TODO: need to normalize?\n",
    "                # depth = depth[np.newaxis,:,:] # need 1 channel -> (1,224,224)\n",
    "                depth = torch.from_numpy(depth).unsqueeze(0) # need 1 channel -> (1,224,224)\n",
    "                all_depths.append(depth)\n",
    "\n",
    "        self.MODE2_Type = DepthType\n",
    "\n",
    "        # TODO... load images and depths on demand with a custom loader\n",
    "        self.all_images = np.array(all_images)\n",
    "        self.all_prompts = np.array(all_depths)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download \"SUNRGBD_V1\" dataset from https://rgbd.cs.princeton.edu/\n",
    "dataset = SUNRGBD_Dataset(path=data_path+\"imagebind/SUNRGBD/\", batch_size=100)\n",
    "all_images, all_depths = dataset.get_data()\n",
    "print(len(all_images), len(all_depths))\n",
    "\n",
    "am_widgets.CLIPExplorerWidget(\"test_depth\", all_data={\"image\": all_images, \"depth\": all_depths}, models=[am_model.ImageBind_Model()]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amumo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
