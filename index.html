<!--
  Copyright 2018 The Distill Template Authors

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!doctype html>

<head>
    <title>Understanding and Comparing Multi-Modal Models</title>
    <link rel="icon" type="image/x-icon" href="favicon.ico">
    <script src="scripts/template.v2.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta charset="utf8">
    <script src="scripts/d3.v7.min.js"></script>
    <script src="scripts/plotly-2.24.1.min.js" charset="utf-8"></script>
    <script src="scripts/bootstrap.bundle.min.js" charset="utf-8"></script>

    <link href="styles/bootstrap.min.css" rel="stylesheet">
    <style>

        /* title */
        d-title {
            /* grid-column-gap: 10px; */
        }

        d-title div {
            grid-column: 4 / 11;
        }

        d-title figure {
            grid-column: 11 / span 3;
        }

        /* table of contents */


        b i {
            display: inline;
        }

        .abstract {
            grid-column: 3 / -2;
        }

        d-article d-contents {
            align-self: start;
            grid-column-start: 1;
            grid-row: auto / span 4;
            justify-self: end;
            margin-top: 0em;
            padding-right: 3em;
            padding-left: 2em;
            padding-top: 1em;
            border-right: 1px solid rgba(0, 0, 0, 0.1);
        }
        d-article hr{
            margin-top: 30px;
            margin-bottom: 30px;
        }

        d-contents a:hover {
            border-bottom: none;
        }

        d-contents nav h3 {
            margin-top: 0;
            margin-bottom: 1em;
        }

        d-contents nav div {
            color: rgba(0, 0, 0, 0.8);
            font-weight: bold;
        }

        d-contents nav a {
            color: rgba(0, 0, 0, 0.8);
            border-bottom: none;
            text-decoration: none;
        }

        d-contents li {
            list-style-type: none;
        }

        d-contents ul {
            padding-left: 1em;
        }

        d-contents nav ul li {
            margin-bottom: 0.25em;
        }

        d-contents nav a:hover {
            text-decoration: underline solid rgba(0, 0, 0, 0.6);
        }

        d-contents nav ul {
            margin-top: 0;
            margin-bottom: 6px;
        }

        d-contents nav>div {
            display: block;
            outline: none;
            margin-bottom: 0.5em;
        }

        d-contents nav>div>a {
            font-size: 13px;
            font-weight: 600;
        }

        d-contents nav>div>a:hover,
        d-contents nav>ul>li>a:hover {
            text-decoration: none;
        }
        
        @media (max-width: 1000px) {
            .abstract {
                grid-column: page;
            }
            d-contents {
                justify-self: start;
                align-self: start;
                grid-column-start: 2;
                grid-column-end: 6;
                padding-bottom: 0.5em;
                margin-bottom: 1em;
                padding-left: 0.25em;
                border-bottom: 1px solid rgba(0, 0, 0, 0.1);
                border-bottom-width: 1px;
                border-bottom-style: solid;
                border-bottom-color: rgba(0, 0, 0, 0.1);
            }
            
            d-title div {
                grid-column: page;
            }

            d-title figure {
                display: none;
            }
        }

        @media (min-width: 1000px) {
            .abstract {
                grid-column: 4 / -2;
            }
            d-contents {
                align-self: start;
                grid-column-start: 1;
                grid-column-end: 4;
                justify-self: end;
                padding-right: 3em;
                padding-left: 2em;
                border-right: 1px solid rgba(0, 0, 0, 0.1);
                border-right-width: 1px;
                border-right-style: solid;
                border-right-color: rgba(0, 0, 0, 0.1);
            }
        }

        @media (max-width: 1300px) {
            d-contents {
                display: none;
            }
        }
    </style>
</head>

<body>
    <script src="scripts/data.js"></script>
    <script>
        // have to load it again because distill template somehow overrides plotly css
        window.onload = () => {
            var tag = document.createElement("script");
            tag.src = "./scripts/plotly-2.24.1.min.js";
            document.getElementsByTagName("head")[0].appendChild(tag);
        
        }
    </script>
    <script src="scripts/widgets.js"></script>


    <d-front-matter>
        <script id='distill-front-matter' type="text/json">{
        "title": "Understanding and Comparing Multi-Modal Models",
        "description": "Exploring the Latent Space of CLIP-like Models Using Inter-Modal Pairs.",
        "published": "February 22, 2024",
        "authors": [
        {
            "author":"Christina Humer",
            "authorURL":"https://jku-vds-lab.at/persons/humer/",
            "affiliations": [{"name": "Visual Data Science Lab, Johannes Kepler University Linz", "url": "https://jku-vds-lab.at"}]
        },
        {
            "author":"Elisabeth Rumetshofer",
            "affiliations": [{"name": "TODO", "url": "https://www.jku.at/"}]
        },
        {
            "author":"Ana Sánchez",
            "affiliations": [{"name": "TODO", "url": "https://www.jku.at/"}]
        },
        {
            "author":"Vidya Prasad",
            "authorURL":"https://research.tue.nl/en/persons/vidya-prasad",
            "affiliations": [{"name": "Eindhoven University of Technology", "url": "https://www.tue.nl/en/our-university/departments/mathematics-and-computer-science"}]
        },
        {
            "author":"Günter Klambauer",
            "authorURL":"https://www.jku.at/institut-fuer-machine-learning/ueber-uns/team/assocprof-mag-dr-guenter-klambauer/",
            "affiliations": [{"name": "Institute for Machine Learnine, Johannes Kepler University Linz", "url": "https://www.jku.at/en/institute-for-machine-learning/"}]
        },
        {
            "author":"Marc Streit",
            "authorURL":"https://marc-streit.com",
            "affiliations": [{"name": "Visual Data Science Lab, Johannes Kepler University Linz", "url": "https://jku-vds-lab.at"}]
        },
        {
            "author":"Hendrik Strobelt",
            "authorURL":"http://hendrik.strobelt.com/",
            "affiliations": [{"name": "IBM Research", "url": "https://www.research.ibm.com/labs/cambridge/"}]
        }
        ],
        "katex": {
        "delimiters": [
            {"left": "$$", "right": "$$", "display": false}
        ]
        }
    }</script>
    </d-front-matter>
    <d-title>
        <div>
            <h1>Understanding and Comparing Multi-Modal Models</h1>
            <p>
                Exploring the Latent Space of CLIP-like Models Using Inter-Modal Pairs
            </p>
        </div>
        <figure style="width: 200px;">
            <img style="width: 100%;" src="exported_data/amumo.jpg" alt="Amumo Logo: A mummy disguised as a detective with a magnifying glass. Generated with Bing Create.">
        </figure>
    </d-title>
    <d-byline></d-byline>
    <d-article>
        <d-contents>
            <nav class="toc figcaption">
                <h4>Contents</h4>
                <div><a href="#section-introduction">Introduction</a></div>
                <div><a href="#section-modality-gap-clip">CLIP and Its Modality Gap</a></div>
                <div><a href="#section-other-models">What About Other CLIP-Like Models?</a></div>
                <ul>
                    <li><a href="#section-modality-gap-cyclip">CyCLIP</a></li>
                    <li><a href="#section-reason">Why Do We Want to Close the Modality Gap?</a></li>
                    <li><a href="#section-modality-gap-cloob">CLOOB</a></li>
                    <li><a href="#section-modality-gap-summary">Summary</a></li>
                </ul>
                <div><a href="#section-amumo">Converting the Technique into a Tool</a></div>
                <ul>
                    <li><a href="#section-amumo-subsets">Identifying Data Subsets</a></li>
                    <li><a href="#section-amumo-diffusiondb">Analyzing the DiffusionDB Dataset</a></li>
                    <li><a href="#section-amumo-imagebind">Analyzing ImageBind Using Text-Image-Audio Data</a></li>
                    <li><a href="#section-amumo-cloome">Analyzing CLOOME Using Microscopy-Molecule Data</a></li>
                    <li><a href="#section-amumo-augmentation">Augmentation Analyses</a></li>
                </ul>
                <div><a href="#section-conclusion">Conclusion</a></div>
                <div><a href="#section-acknowledgements">Acknowledgements</a></div>
                <div><a href="#section-researchmaterial">Research Material Statements</a></div>
                <div><a href="#section-reproduce">Reproducibility</a></div>
                <div><a href="#section-authorship">Authorship</a></div>
                <div><a href="#section-conflict">Conflict of Interest</a></div>
                <div><a href="#section-appendix">Appendix</a></div>
                <ul>
                    <li><a href="#section-lg-dataset">Closing the Gap Using a Larger Dataset</a></li>
                    <li><a href="#section-lg-dataset-visually">Visually Closing the Modality Gap</a></li>
                </ul>
                
            </nav>
            <!-- <figure style="width: 100%; padding-top: 3em; padding-right: 3em; padding-left: 2em;">
                <img style="max-width: 200px;" src="exported_data/amumo.jpg" alt="Amumo Logo: A mummy disguised as a detective with a magnifying glass. Generated with Bing Create.">
            </figure> -->
        </d-contents>
        <div class="abstract">
            <!-- style="background: #f8f8fb;"> -->
            <h2 id="section-abstract" style="margin-top: 0px;">
                Abstract
            </h2>
            <p id="section-abstract-introduction">
                <b>Introduction</b> <br>
                Multi-modal contrastive learning models are trained to map data from two or more modalities to a shared
                embedding space. This latent data representation can then be used for zero- or few-shot classification,
                cross-modal data retrieval, or generation tasks. Although remarkable results have been reported when
                testing multi-modal models on these tasks, understanding the latent representations remains challenging.
                In particular, many multi-modal models exhibit a phenomenon called the “modality gap”, leading to a
                latent space that cleanly separates the modalities.
            </p>
            <p id="section-abstract-conclusion">
                <b>Conclusion</b> <br>
                This article introduces and compares three models trained on image-text pairs. We use these models and
                interactive visualizations to explain where the modality gap arises from, how it can be closed, and why
                closing it is important. In the second part, we introduce “Amumo”, a framework we implemented for
                analyzing multi-modal models. We describe various analysis tasks that can be performed with Amumo. In
                particular, Amumo can be used for (i) analyzing models, (ii) comparing models with each other, and (iii)
                analyzing multi-modal datasets. We demonstrate Amumo’s capabilities and generalizability using image,
                text, audio, and molecule data in combination with several different models.
            </p>
            <p id="section-abstract-impl">
                <b>Implementation</b> <br>
                For smooth integration into research workflows, we implemented the Amumo as a Python package with
                Jupyter widgets. We implemented the interactive visualizations in this article with JavaScript and
                plotly.js.
            </p>
            <p id="section-abstract-demo">
                <b>Demonstration & Materials</b> <br>
                TODO: either link to cloom binder or deploy some other minimal example with binder...

                Code for the Amumo python package and guidelines on how to use it can be found in the <a target="_blank"
                    href="https://github.com/ginihumer/Amumo/" title="Link to the Amumo python package.">github
                    repository</a>.
            </p>
        </div>

        <hr>

        <h2 id="section-introduction">
            Introduction
        </h2>
        <p>
            Contrastive Language Image Pre-training (CLIP)<d-cite key="radford_learning_2021"></d-cite> and variations
            of this approach, like CyCLIP<d-cite key="goel_cyclip_2022"></d-cite> or CLOOB<d-cite
                key="furst_cloob_2022"></d-cite>, are trained on image-text pairs with a contrastive objective. The goal
            of contrastive loss objectives is to minimize latent-space distances of data points that have the same
            underlying meaning. We refer to the particular cases of contrastive learning that CLIP-like models perform
            as multi-modal contrastive learning because they use two (or more<d-cite
                key="girdhar_imagebind_2023"></d-cite>) modes of data (e.g., images and texts) where each mode uses
            their own encoder to generate a latent embedding space. More specifically, the objective that CLIP is
            optimized for minimizes the distances between image-text embeddings of pairs that have the same semantic
            meaning while maximizing the distances to all other combinations of text and image embeddings.
            We would expect that such a shared latent space places similar concepts of images and texts close to each
            other, as demonstrated in the following sketch. However, the reality is a bit more complicated.
        </p>
        <figure style="grid-column: text; margin: 1rem 0;">
            <img src="./exported_data/idea_figure.png" style="width:100%;"
                alt="Example of how we imagined a 2-dimensional projection of CLIP's image and text embeddings. Image and text points are shown in one scatter plot and instants that are semantically similar are plotted close together.">

            <figcaption style="text-align: center; margin-left: 10%; width: 80%;">
                Example of how we imagined a two-dimensional projection of CLIP's image and text embeddings.
            </figcaption>
        </figure>

        <h2 id="section-modality-gap-clip">
            CLIP and its Modality Gap
        </h2>
        <p>
            Despite the clear objective that is supposed to bring texts and images to a shared embedding space there is
            a phenomenon called "Modality Gap"<d-cite key="liang_mind_2022"></d-cite> which describes that embeddings of
            different modalities lie in their own embedding subspaces. The example below visualizes the Modality Gap
            between images and texts of CLIP<d-footnote>We use the official CLIP implementation by OpenAI: <a
                    src=’https://github.com/openai/CLIP’>https://github.com/openai/CLIP</a> with the RN50 image
                encoder.</d-footnote> embeddings for a subset of 100 randomly selected images from MSCOCOs<d-cite
                key="lin_microsoft_2015"></d-cite> validation set. The image and text embeddings are projected to a
            2-dimensional space and visualized in a scatter plot. We use a gray line to connect image-text pairs that
            belong together.
        </p>


        <figure style="grid-column: text;">
            <div id="clip-modalitygap-scatterplot" style="display: inline-flex;"></div>
        </figure>

        <script>
            mscoco_val_projections_promise.then(function (data) {
                const scatter_widget = new ScatterPlotWidget(data, 'CLIP');
                document.getElementById('clip-modalitygap-scatterplot').appendChild(scatter_widget.div)

                mscoco_val_prompts_promise.then(captions => {
                    const hover_widget = new HoverWidget('MSCOCO-Val_size-100', captions);
                    connect_scatter_hover(scatter_widget, hover_widget)
                    document.getElementById('clip-modalitygap-scatterplot').appendChild(hover_widget.div)
                    return captions;
                })
                return data;
            });
        </script>

        <p>
            The use of dimensionality reduction methods to compute a 2-dimensional view of the data clearly shows the
            separation between the two modalities. However, dimensionality reduction comes hand in hand with a loss of
            information and possible distortion of data. We propose a different way to visualize the dimensionality gap.
            Similarity heatmaps are a simple yet effective way of visualizing latent space embeddings that also helped
            us to better understand the modality gap. Using this visualization also allowed us to gain interesting
            insights unrelated to the modality gap, which we could not have found with the scatter plot visualization
            alone.
        </p>
        <p>
            The similarity heatmap below shows the same subset of 100 image-text pairs previously shown in a scatter
            plot. However, in this case, we show the cosine similarities<d-footnote>Note that the cosine similarity can
                yield values between [-1; 1].</d-footnote> calculated between all image and text embeddings.
            This results in a matrix with four quadrants:
        <ul>
            <li>
                top-left: in-modal similarities of the 100 image embeddings,
            </li>
            <li>
                bottom-right: in-modal similarities of the 100 text embeddings,
            </li>
            <li>
                top-right: cross-modal similarities between the 100 image embeddings and the 100 text embeddings,
            </li>
            <li>
                bottom-left: a transposed version of the cross-modal image-text similarities.
            </li>
        </ul>
        The diagonal axis of each quadrant
        represents the matching data points (i.e., for the in-modal similarities, the diagonal shows the
        similarities of the image or text embedding to itself, while for the cross-modal similarities, the diagonal
        shows the matching image-text pairs). The modality gap is immediately visible: the in-modal similarities are
        much higher overall compared to the cross-modal similarities.
        </p>
        <figure style="grid-column: text;">
            <div id="clip-modalitygap-heatmap" style="display: inline-flex;"></div>
        </figure>
        <script>
            const heatmap_widget = new SimilarityHeatmapWidget();
            heatmap_widget.update_heatmap('MSCOCO-Val_size-100', 'CLIP').then(() => {

                document.getElementById('clip-modalitygap-heatmap').appendChild(heatmap_widget.div)

                mscoco_val_prompts_promise.then(captions => {
                    const hover_widget = new HoverWidget('MSCOCO-Val_size-100', captions);
                    connect_heatmap_hover(heatmap_widget, hover_widget);
                    document.getElementById('clip-modalitygap-heatmap').appendChild(hover_widget.div)

                    return captions;
                });
            });
        </script>
        <p><b>Now, where does this modality gap even come from?</b></p>
        <p>
            As analyzed by Liang et al.<d-cite key="liang_mind_2022"></d-cite>, the modality gap already appears before
            models are trained, possibly caused by random weight initialization and different model architectures. The
            authors also highlight that the gap between modalities persists throughout training, which means that CLIP's
            objective function cannot overcome this phenomenon. A reason for that could be that the objective function
            only trains on the alignment of (non-)matching image-text combinations but does not contain any
            regularization terms with regard to the overall layout of the embedding spaces and in-modality
            alignment<d-cite key="goel_cyclip_2022"></d-cite>.
            Liang et al.<d-cite key="liang_mind_2022"></d-cite> also formally define the modality gap as the Euclidean
            difference between the centers of each modality:
        </p>

        <d-math block>
            \delta_{gap} = \frac{1}{n} \sum_{i=1}^n{x_i} - \frac{1}{n} \sum_{i=1}^n{y_i}
        </d-math>

        <p>where <d-math>x_i</d-math> and <d-math>y_i</d-math> are the normalized image and text embedding vectors.</p>


        <p>They also experimented with manually reducing the modality gap by moving the embeddings closer together along
            the gap vector.</p>

        <figure style="grid-column: text; margin: 1rem 0;">
            <img style="width: 100%" src="./exported_data/closethegap-schema.png" />
            <figcaption style="text-align: center; margin-left: 10%; width: 80%;">
                Manually reducing the modality gap by moving the embeddings closer together along the gap vector.
            </figcaption>
        </figure>
        <p>
            However, since the modality subspaces trained by CLIP are not symmetric<d-footnote>Goel et al.<d-cite
                    key="goel_cyclip_2022"></d-cite> argue that the CLIP objective does, in fact, symmetrize the spaces
                in its optimal solution; in practice however this ideal scenario does not happen.</d-footnote> to each
            other, a modification of the embeddings destroys the complex relationship between images and texts that was
            derived during training. This naturally results in an increasing loss when changing the distance between
            modalities that was originally trained, as shown in the visualization below. The x-axis shows the Euclidean
            distance between the two embedding centers (the black dashed line indicates the original distance). The
            y-axis shows the contrastive loss that results when moving the two embeddings closer together or further
            away from each
            other. We calculated these values with the whole 5000 samples MSCOCO validation set. It becomes visible that
            the global minimum of this manual intervention is at the point of the original (trained) modality gap.
        </p>

        <figure style="grid-column: text;">
            <div id="clip-loss-landscape"></div>
        </figure>
        <script>
            clip_loss_landscape_promise.then(data => {
                const modality_gap_widget = new ModalityGapWidget('Loss Landscape CLIP', data);
                document.getElementById('clip-loss-landscape').appendChild(modality_gap_widget.div)

                return data;
            });
        </script>

        <p>
            Now, what do the similarity heatmap and scatter plot look like when we manually close the gap? As shown in
            the visualization below, the similarity matrix seems more homogeneous (i.e., in-modal similarities and
            cross-modal similarities are on a similar level), and points in the scatter plot are closer together.
            However, the scatter plot also shows that the edges between image-text pairs are still long, and the text
            embeddings concentrate more on the center compared to the image embeddings.
            <!-- The many edge crossings between the image-text pairs are also a qualitative indicator that the embeddings are not well -->
            <!-- aligned. Furthermore, the text embeddings concentrate more on the center compared to the image embeddings. -->
        </p>


        <figure style="grid-column: screen;">
            <div class="container">
                <div id="clip-nomodalitygap" class="row d-flex justify-content-center "></div>
            </div>
        </figure>
        <script>
            clip_explorer_by_model('MSCOCO-Val_size-100', 'CLIP_nogap', "clip-nomodalitygap", mscoco_val_prompts_promise, mscoco_val_projections_promise)
        </script>

        <h2 id="section-other-models">What About Other CLIP-Like Models?</h2>
        <p>
            As mentioned previously, the two modalities (i.e., texts and images) live on different embedding spaces, and
            the two embeddings vary
            in structure (i.e., they are not symmetric to each other). Recently published papers propose different
            versions of CLIP, where the objective function has been adjusted to regularize the trained embedding space.
            We look closer into two promising approaches, namely, CyCLIP<d-cite key="goel_cyclip_2022"></d-cite> and
            CLOOB<d-cite key="furst_cloob_2022"></d-cite>.
        </p>

        <h3 id="section-modality-gap-cyclip">CyCLIP</h3>
        <p>
            According to Goel et al.<d-cite key="goel_cyclip_2022"></d-cite> using CLIP-generated image-text embeddings
            interchangeably (e.g., for Language-guided image generation) is suboptimal because the embeddings are–in
            practice–not aligned. They propose an augmentation of the InfoNCE loss<d-cite key="oord_representation_2019"></d-cite> used to optimize CLIP that adds two
            regularization terms that enforce the two embedding spaces to be symmetric: one for in-modal symmetry
            (<d-math>L_I</d-math>) and one for cross-modal symmetry (<d-math>L_C</d-math>) of the similarities.
        </p>

        <d-math block>
            L_{CyCLIP} = L_{InfoNCE} + L_I + L_C
        </d-math>

        <p>
            The visualizations below already hint that the embeddings are symmetric: The in-modal similarity heatmaps of
            the image and text quadrants look similar, and the modalities in the image-text points in the PCA projection
            seem almost parallel. However, it is also clear that the modality gap is still present.
        </p>

        <figure style="grid-column: screen;">
            <div class="container">
                <div id="cyclip-modalitygap" class="row d-flex justify-content-center"></div>
            </div>
        </figure>
        <script>
            clip_explorer_by_model('MSCOCO-Val_size-100', 'CyCLIP', "cyclip-modalitygap", mscoco_val_prompts_promise, mscoco_val_projections_promise)
        </script>

        <p>
            We can confirm this with a slight modification of the similarity heatmap. For this, we calculate the
            difference between the in-modal similarities of images and texts, which results in a matrix where values
            approach zero (i.e., the matrix is a blue rectangle).
        </p>

        <div id="cyclip-showsymmetry" style="display: flex;" class="l-body side"></div>
        <script>
            load_similarities_fn('MSCOCO-Val_size-100', 'CyCLIP').then((data) => {
                // return %s.slice(0, 100);

                const inmodal_image_similarities = data.slice(0, data.length / 2).map((val_arr) => val_arr.slice(0, data.length / 2))
                const inmodal_text_similarities = data.slice(data.length / 2).map((val_arr) => val_arr.slice(data.length / 2))

                const simmilarity_diff = inmodal_image_similarities.map((arr, index1) => arr.map((val, index2) => val - inmodal_text_similarities[index1][index2]))
                var trace1 = {
                    name: '',
                    z: simmilarity_diff,
                    type: 'heatmap',
                    // hoverinfo: 'text',
                    hovertemplate: '%{z:.3f}',
                    colorscale: plasma_colors, //'YlOrRd',
                    // reversescale: true,
                    zmin: 0.35,
                    zmax: 1.0,
                    showscale: false,
                };
                var traces = [trace1];

                var layout = {
                    width: 90,
                    height: 90,
                    margin: { l: 0, r: 0, t: 0, b: 0 },
                    xaxis: {
                        tickmode: 'array',
                        ticktext: [],
                        tickvals: [],
                        fixedrange: false
                    },
                    yaxis: {
                        tickmode: 'array',
                        tickvals: [],
                        ticktext: [],
                        fixedrange: false,
                        autorange: 'reversed',
                    },
                }
                Plotly.newPlot('cyclip-showsymmetry', traces, layout, { staticPlot: false, displayModeBar: false });
            })
        </script>

        <p>
            When looking back to Liang et al.’s<d-cite key="liang_mind_2022"></d-cite> experiments that tried to align
            the embeddings by moving them along the modality gap vector, we saw that this does not work well for CLIP
            embeddings because the space is not symmetrized. For CyCLIP, however, the symmetrization is enforced with
            the objective function, and we could see in the previous plots that the spaces are indeed mostly symmetric.
            Let's look into the loss landscape for different modality distances of CyCLIP embeddings when moving the
            embeddings along the gap vector. In the visualization below, we can see that CyCLIP’s loss landscape looks
            very different from CLIP’s landscape: in the distance interval of [-1.5; 1.5] the loss does not change much,
            which is another indicator that CyCLIP indeed learns nearly symmetric embedding spaces<d-footnote>Note that
                the loss outside the [-1.5; 1.5] interval increases. This could be due to the fact that the spaces are
                not perfectly symmetric. Another explanation could be that this happens due to normalizing the
                embeddings to the unit sphere, but the method proposed for closing the gap is done in Euclidean
                space.</d-footnote>.
        </p>


        <figure style="grid-column: page;">
            <div class="container">
                <div class="row d-flex justify-content-center">
                    <div id="clip2-loss-landscape" class="col-lg-6"></div>
                    <div id="cy-clip-loss-landscape" class="col-lg-6"></div>
                </div>
            </div>
        </figure>
        <script>
            clip_loss_landscape_promise.then(data => {
                const modality_gap_widget = new ModalityGapWidget('Loss Landscape CLIP', data);
                document.getElementById('clip2-loss-landscape').appendChild(modality_gap_widget.div)
                return data;
            });
            cyclip_loss_landscape_promise.then(data => {
                const modality_gap_widget = new ModalityGapWidget('Loss Landscape CyCLIP', data);
                document.getElementById('cy-clip-loss-landscape').appendChild(modality_gap_widget.div)
                return data;
            });
        </script>

        <p>
            Again, when closing the modality gap, the similarity heatmap becomes more homogenous (i.e., the in-modal
            similarities and cross-modal similarities are similarly strong). The image-text pairs in the scatter plot
            are closer, but still scattered.
            <!-- While the image-text pairs in the scatter plot are closer, there are still quite some edge crossings. -->
        </p>
        <figure style="grid-column: screen;">
            <div class="container">
                <div id="cyclip-nomodalitygap" class="row d-flex justify-content-center"></div>
            </div>
        </figure>
        <script>
            clip_explorer_by_model('MSCOCO-Val_size-100', 'CyCLIP_nogap', "cyclip-nomodalitygap", mscoco_val_prompts_promise, mscoco_val_projections_promise)
        </script>

        <p>
            Alternatively, we can use UMAP or tSNE projection to better utilize the neighborhoods of similar embeddings
            instead of linearly determining the axes with the highest variance, as done in PCA. In the case of CyCLIP
            (scatter plot on the right hand), the use of a neighborhood-based dimensionality reduction technique results
            in shorter edges and better clustering of similar embeddings. For CLIP (scatter plot on the left hand),
            edges remain long, and similar images and texts are not clustered together. See the Appendix for examples
            with 5000 samples.
        </p>

        <figure style="grid-column: page;">
            <div class="container">
                <div class="row d-flex justify-content-center">
                    <div id="clip-nogap-umap" class="col-lg-6"></div>
                    <div id="cy-clip-nogap-umap" class="col-lg-6"></div>
                </div>
            </div>
        </figure>
        <script>
            mscoco_val_projections_promise.then(function (data) {
                const scatter_widget = new ScatterPlotWidget(data, 'CLIP_nogap', selected_method = 'UMAP', title = 'CLIP');
                document.getElementById('clip-nogap-umap').appendChild(scatter_widget.div)

                const scatter_widget_cyclip = new ScatterPlotWidget(data, 'CyCLIP_nogap', selected_method = 'UMAP', title = 'CyCLIP');
                document.getElementById('cy-clip-nogap-umap').appendChild(scatter_widget_cyclip.div)
                return data;
            });
        </script>


        <p>
            Let us summarize how the loss changes for CLIP and CyCLIP when moving the embeddings together along the
            modality gap vector. For that, we again use the entire MSCOCO validation dataset of 5000 samples.
        </p>

        <table>
            <tr>
                <th>Model</th>
                <th>Original Distance</th>
                <th>Original Loss</th>
                <th>Closed Distance</th>
                <th>Closed Loss</th>
                <th>Loss Difference</th>
            </tr>
            <tr>
                <td>CLIP</td>
                <td>0.818611</td>
                <td>0.355370</td>
                <td>0.035077</td>
                <td>1.124780</td>
                <td><b>0.769410</b></td>
            </tr>
            <tr>
                <td>CyCLIP</td>
                <td>0.873026</td>
                <td>0.763433</td>
                <td>0.001218</td>
                <td>0.848867</td>
                <td><b>0.085434</b></td>
            </tr>
        </table>


        <p>
            The numbers confirm that CyCLIP's loss changes far less than CLIP's loss when manually closing the gap – it
            neither gets better nor significantly worse. However, the question arises:
        </p>
        <h3 id="section-reason">
            Why Do We Even Want to Close the Modality Gap, if We Do Not Gain Performance?
        </h3>
        <p>
            From a performance optimization point of view, this is a valid question. What's the point of interfering in
            a well-performing system? On the other hand, the alignment of embedding spaces can become important for
            other downstream tasks. For example, using image and text embeddings interchangeably, as done in
            language-guided image generation, relies on the fact that image and text embeddings are aligned with each
            other and live in the same space. Another aspect is that an aligned embedding space is closer to how humans
            expect multi-modal models to see the data. Furthermore, closing the modality gap allows us to actually
            visualize texts and images in the same space, and develop interactive exploration tools that help to
            understand multi-modal data (e.g., analyzing pairs of human written captions and machine-generated images to
            find insights about text-to-image generation models like StableDiffusion<d-cite
                key="rombach_high-resolution_2022"></d-cite>).
        </p>
        <p>
            These example use cases of why closing the modality gap might be helpful should give you an incentive about
            why the pure "performance optimization" point of view is not the only one. In fact, if we can close the
            modality gap without significantly losing performance, we have a win-win situation!
        </p>

        <h3 id="section-modality-gap-cloob">CLOOB</h3>
        <p>
            In the previous section, we established a way to manually close the modality gap of CyCLIP embeddings.
            However, wouldn’t it be better to already close the gap during training and not rely on post-hoc
            manipulations? While we did not experiment with further modifying (Cy)CLIP’s objective to close the gap
            during training, we stumbled upon a different learning approach that naturally closes the gap.
        </p>
        <p>
            Contrastive Leave One Out Boost (short: CLOOB)<d-cite key="furst_cloob_2022"></d-cite> is a variation of
            CLIP that proposes an alternative objective together with an associative memory to train the model. The two
            main components of their method
            function are (i) modern Hopfield networks and (ii) the InfoLOOB loss instead of the InfoNCE loss used by
            CLIP. The authors argue that their modifications solve CLIP's "explaining away" problem (i.e., focusing on a
            small subset of features while ignoring other relevant features) and InfoNCE's saturation
            problem<d-footnote>See Fürst et al.<d-cite key="furst_cloob_2022"></d-cite> for more
                information.</d-footnote>.
        </p>
        <p>However, we also observe something else: These modifications seem to aid the
            closure of the modality gap!</p>

        <figure style="grid-column: screen;">
            <div class="container">
                <div id="cloob-explorer" class="row d-flex justify-content-center"></div>
            </div>
        </figure>
        <script>
            clip_explorer_by_model('MSCOCO-Val_size-100', 'CLOOB', "cloob-explorer", mscoco_val_prompts_promise, mscoco_val_projections_promise, 'UMAP')
        </script>


        <p>
            We believe that CLOOB’s ability to close the gap during training mainly stems from the cross-modality
            retrieval
            applied before calculating the InfoLOOB loss. In this step, the batch of image embeddings
            and text embeddings is used as an associative memory to create a weighted average of embeddings for the
            current instance. Since this is done for each combination of embedding and associative memory (i.e., each
            image and each text embedding is associated with the entire batch of image embeddings AND the entire batch
            of text embeddings), a stronger correlation between the image/text instances to their own
            modality and the opposite modality is established.
            Other influencing factors on closing the modality gap could be the use of InfoLOOB, or the use of modern
            Hopfield networks
            that generally have a denoising effect<d-footnote>See this blog post for a detailed explanation of
                modern hopfield networks: <a target="_blank" href="https://ml-jku.github.io/hopfield-layers/"
                    title="Blog post about Hopfield Networks">https://ml-jku.github.io/hopfield-layers/</a>.</d-footnote>.
        </p>

        
        <figure style="grid-column: screen;">
            <div class="container" id="cloob_ablation">
                <div class="row">
                    <div id="cloob_ablation_compare" class="col-md-10 col-xs-12 row"></div>
                    <div id="cloob_ablation_compare-hover" class="col-md-2 col-xs-12"></div>
                </div>
            </div>
        </figure>
        <script>
            
            const mscoco_val_ablation_prompts_promise = fetch('./exported_data/MSCOCO-Val_size-100_CLOOB_ablation/prompts.txt')
                .then(response => response.text())
                .then(data => data.split('\n'))
                .catch(error => console.error(error));

            const mscoco_val_ablation_projections_promise = d3.csv("./exported_data/MSCOCO-Val_size-100_CLOOB_ablation/projections.csv");

            const cloob_ablation_models = {'clip': 'Info NCE (aka CLIP)', 'clip_infoLOOB': 'Info LOOB', 'cloob_infoNCE': 'Info NCE + Hopfield', 'cloob': 'Info LOOB + Hopfield (aka CLOOB)'};
            const cloob_ablation_variants = ['rn50_cc_epoch_31', 'rn50_cc_epoch_128', 'rn50_yfcc_epoch_28']

            // const cartesian = (...a) => a.reduce((a, b) => a.flatMap(d => b.map(e => [d, e].flat())));
            // console.log(cartesian(cloob_ablation_models, cloob_ablation_variants))

            
            const div = document.getElementById('cloob_ablation');
            // init variant select
            const variant_select = document.createElement('select');
            for (const key in cloob_ablation_variants) {
                var opt = new Option(cloob_ablation_variants[key], cloob_ablation_variants[key])
                variant_select.appendChild(opt);
            }
            variant_select.id = 'cloob_ablation_variant_select';
            variant_select.selectedIndex = 0;
            variant_select.style.marginLeft = '5px';
            div.prepend(variant_select);
            
            const label = document.createElement('label');
            label.setAttribute('for', variant_select.id);
            label.textContent = 'Variant:';
            label.style = 'font-size:15px; margin-left:5px;';
            div.prepend(label);
            
            
            // handle UI changes
            const update_comparer = () => {
                document.getElementById("cloob_ablation_compare").innerHTML = ''
                document.getElementById("cloob_ablation_compare-hover").innerHTML = ''
                const ablation_models = Object.keys(cloob_ablation_models).reduce((a, v) => ({ ...a, [cloob_ablation_models[v]]: v + "_" + variant_select.value}), {});
                clip_comparer(ablation_models, mscoco_val_ablation_prompts_promise, 'MSCOCO-Val_size-100_CLOOB_ablation', 'cloob_ablation_compare', z_min = -0.25, z_max = 1, show_meta_info = true)
            };
            variant_select.addEventListener("change", update_comparer);

            update_comparer()
            
        </script>


        <h3 id="section-modality-gap-summary">Summary of Modality Gap Analysis</h3>
        <p>
            The following visualizations give an overview of the similarity heatmap for CLIP and CyCLIP before (left)
            and after (right) manually removing the modality gap, as well as the similarity heatmap for CLOOB with the
            official checkpoints from the paper and a CLOOB version that was trained on the LAION 400M dataset and used
            a ViT instead of a CNN to encode images<d-cite key="crowson_cloob-training_2023"></d-cite>. Note how the
            overall distribution of similarity values differs between the various embedding spaces. For example, the two
            CLOOB models seem to discriminate more strictly between matching and non-matching image-text pairs.
        </p>


        <figure style="grid-column: screen;">
            <div class="container">
                <div class="row">
                    <div id="similarity-heatmaps" class="col-md-10 col-xs-12 row"></div>
                    <div id="similarity-heatmaps-hover" class="col-md-2 col-xs-12"></div>
                </div>
            </div>
        </figure>
        <script>
            clip_comparer(['CLIP', 'CLIP_nogap', 'CyCLIP', 'CyCLIP_nogap', 'CLOOB', 'CLOOB_LAION400M'], mscoco_val_prompts_promise, 'MSCOCO-Val_size-100', 'similarity-heatmaps', z_min = -0.25, z_max = 1)
        </script>

        <p>
            The previous sections taught us about the modality gap, where it comes from, and ways to close it. We also
            gave reasons for why closing the gap might be beneficial and now demonstrate how closing the modality gap
            can help with analyzing multi-modal data. We also introduced a handy new way of visualizing latent
            space embeddings and utilize this again in the following analyses. Finally, we also want to mention that
            there is a way to visually close the modality gap (i.e., without actually closing the gap in the embedding
            space), as described in the Appendix.
        </p>

        <h2 id="section-amumo">Converting the Technique into a Tool</h2>
        <p>
            Using the previously introduced techniques, we implemented an interactive prototype called <a
                target="_blank" href="https://github.com/ginihumer/Amumo/">“Amumo” (Analyze
                Multi-Modal Models)</a>. Users can switch between models, explore the similarity heatmap and scatter
            plot
            visualizations, manually close the modality gap, and try various projection methods.
        </p>

        <figure style="grid-column: screen;">
            <div id="clip-explorer-mscoco" class="container"></div>
        </figure>
        <script>
            clip_explorer_widget('MSCOCO-Val_size-100', "clip-explorer-mscoco", mscoco_val_prompts_promise, mscoco_val_projections_promise, 'UMAP')
        </script>



        <h3 id="section-amumo-subsets">Identifying Data Subsets</h3>
        <p>
            We can look into semantic subsets of data by filtering instances based on their captions. The following
            example shows the visualizations for the subset that contains the substring "dog". We notice that some lines
            in the similarity matrix have a darker color. When hovering over those darker lines, we can see that most of
            these instances correspond to images and texts about "hot dogs" or other images that do not show a dog or
            where a dog is in an uncommon setting. To make this even more obvious, we can use the "Cluster matrix by
            similarity" function that reorders the similarity heatmap such that similar lines are grouped together. One
            cluster that stands out in all three CLIP-like models is the "hot dog" cluster. However, we can also see
            clusters for "dog and frisbee", "dog and bed", or "dog and car".
            <!-- clustering could be done differently or made interactive (e.g., allow users to select the number of clusters, or whether they want to cluster by in-modal or cross-modal similarities...) -->
        </p>

        <figure style="grid-column: screen;">
            <div id="clip-explorer-filtered" class="container"></div>
        </figure>
        <script>
            let dataset_name = 'MSCOCO-Val_filter-any_dog';
            const mscoco_filtered_val_prompts_promise = fetch('./exported_data/' + dataset_name + '/prompts.txt')
                .then(response => response.text())
                .then(data => data.split('\n'))
                .catch(error => console.error(error));

            const mscoco_filtered_val_projections_promise = d3.csv('./exported_data/' + dataset_name + '/projections.csv');

            clip_explorer_widget(dataset_name, "clip-explorer-filtered", mscoco_filtered_val_prompts_promise, mscoco_filtered_val_projections_promise, 'UMAP')
        </script>


        <h3 id="section-amumo-diffusiondb">Analyzing the DiffusionDB Dataset</h3>
        <p>
            We would like to see what the models’ latent-space embeddings look like for a dataset that is not (entirely)
            procured by humans. To this end, we use DiffusionDB<d-cite key="wang_diffusiondb_2023"></d-cite>, a
            collection of human-written captions and images generated from these captions by Stable Diffusion<d-cite
                key="rombach_high-resolution_2022"></d-cite>. We use a subset of 100 randomly selected samples to
            qualitatively explore the embedding spaces created by the CLIP models we previously introduced. You can use
            the instance of Amumo below to follow along with the analysis described.
        </p>
        <p>
            With the default settings, we randomly explore the dataset and get a feeling for the data contained in this
            subset. We can investigate instances that are outliers in the similarity heatmap by hovering rows or cells
            that have particularly large or low similarity values. For example, there are some particularly bright cells
            scattered in the image in-modal similarity heatmap. Upon hovering, we see that all of these images are
            blurry. We know that DiffusionDB added blur filters for images that were detected to show inappropriate
            content. Interestingly, CLIP seems to create similar latent embeddings for blurry items, causing them to
            show high similarity in the similarity heatmap.
        </p>
        <p>
            For further analyses, we choose "Cluster matrix by similarity" to order the matrix in a way that groups
            similar rows in the heatmap and investigate the clusters that are emerging. We can see a cluster for
            "impressionism and crystal" that seems to have homomorphic similarities over all images (i.e., there is a
            distinct purple line along all images of this cluster). Upon further investigation, we see that the captions
            in this cluster are mostly vague texts or single words (e.g., "crystal", "impressionism") that can apply to
            a lot of images. The same cluster becomes apparent in the text in-modal similarity heatmap, where all
            captions within the cluster seem to have high similarity.
        </p>
        <p>
            Let’s close the modality gap to investigate clusters in a 2-dimensional scatter plot. We can either do this
            by switching to the CLOOB model or using CyCLIP in combination with the "Close modality gap" option. We see
            that the embeddings are aligned and can use the interactive scatter plot to investigate clusters. For
            example, we can try to find the cluster of blurry images, or we can try to find the cluster with instances
            of "impressionism". Of course, this would be much more fun on a larger scale :)
        </p>

        <figure style="grid-column: screen;">
            <div id="clip-explorer-diffusiondb" class="container"></div>
        </figure>
        <script>
            dataset_name = 'DiffusionDB_size-100';
            const diffusiondb_val_prompts_promise = fetch('./exported_data/' + dataset_name + '/prompts.txt')
                .then(response => response.text())
                .then(data => data.split('\n'))
                .catch(error => console.error(error));

            const diffusiondb_val_projections_promise = d3.csv('./exported_data/' + dataset_name + '/projections.csv');

            clip_explorer_widget(dataset_name, "clip-explorer-diffusiondb", diffusiondb_val_prompts_promise, diffusiondb_val_projections_promise, 'UMAP')
        </script>



        <h3 id="section-amumo-imagebind">Analyzing ImageBind Using Text-Image-Audio Data</h3>
        <p>
            So far, we have looked into models trained to map two modalities to a shared space. ImageBind is a model
            trained to embed six different modalities<d-cite key="girdhar_imagebind_2023"></d-cite>: images, text, audio, depth, thermal, and
            inertial sensor data. The used approach “binds” the modalities using images-data pairs. More specifically,
            each modality was only paired with image (and video) data, but not across all other modalities. Although
            there is no explicit matching between modalities, ImageBind succeeded zero-shot evaluation between the
            modalities.
        </p>
        <p>
            Going beyond the quantitative verification used by the authors of ImageBind, we are interested in how well
            the embedding spaces align between explicitly paired modalities and implicit pairing.
        </p>

        <h4 id="section-amumo-imagebind-textimage">
            Text + Image
        </h4>
        <p>
            Let us first look into the explicit pairing of image-text modalities using our MSCOCO dataset. Similarly to
            CLIP, ImageBind exhibits a modality gap between images and texts, which makes sense because ImageBind also
            uses the InfoNCE loss<d-cite key="oord_representation_2019"></d-cite>.
        </p>

        <figure style="grid-column: screen;">
            <div id="clip-explorer-imagebind-textimage" class="container"></div>
        </figure>
        <script>
            clip_explorer_widget('MSCOCO-Val_size-100', "clip-explorer-imagebind-textimage", mscoco_val_prompts_promise, mscoco_val_projections_promise, 'PCA', ["ImageBind", ...AVAILABLE_MODELS])
        </script>


        <h4 id="section-amumo-imagebind-textimageaudio">
            Text + Image + Audio
        </h4>
        <p>
            For analyzing the implicit pairing of image, text, and audio modalities, we use a subset of the AudioSet<d-cite key="gemmeke_audio_2017"></d-cite> dataset that contains animal sounds of birds, cats, dogs, and horses. The AudioSet dataset only contains text and audio pairs. However, each instance also contains the YouTube ID from which the audio data was retrieved. We use this ID to retrieve thumbnail images for each audio instance. This results in a triplet dataset of text-audio-image pairs that we can analyze with Amumo. 
        </p>
        <p>
            Here we can see that the in-modal similarities of images and texts are very pronounced (similar to how the CLIP similarities of images and texts look), which indicates a modality gap. The audio embeddings, on the other hand, have a lower overall similarity to other audio embeddings. In particular, similarities between audio embeddings and embeddings from other modalities seem more evenly distributed, which we previously saw in models that do not suffer from a modality gap. This could indicate that a modality gap's severity depends on the modality type. Another reason could be the availability and quality of training data for each modality.
        </p>
        
        <figure style="grid-column: screen;">
            <div id="clip-explorer-imagebind-textimageaudio" class="container"></div>
        </figure>
        <script>
            const triplet_prompts_promise = fetch('./exported_data/Triplet_size-100/prompts.txt')
                .then(response => response.text())
                .then(data => data.split('\n'))
                .catch(error => console.error(error));

            const triplet_projections_promise = d3.csv("./exported_data/Triplet_size-100/projections.csv");

            clip_explorer_widget('Triplet_size-100', "clip-explorer-imagebind-textimageaudio", triplet_prompts_promise, triplet_projections_promise, 'PCA', ["ImageBind"], {"Image": null, "Text": null, "Audio": null})
        </script>

        <p>
            When clustering the similarity matrix by text embeddings, four main clusters emerge—one for each type of animal included in the dataset—and one smaller cluster that contains a mix of cats and dogs labeled “domestic animals”. 
        </p>
        <p>
            We can also use the audio-to-audio similarities for clustering the heatmap. Interestingly, the audio embeddings seem to be very similar among all instances of cats and dogs. These instances are clustered under the label “domestic animals | pets”. When listening to the audio samples in this cluster, it becomes apparent that most instances contain human chatter. 
        </p>


        <h3 id="section-amumo-cloome">Analyzing CLOOME Using Microscopy-Molecule Data</h3>
        <p>
            In this experiment, we focus on analyzing the modality gap between microscopy images and molecular structures. We visualized the resulting embeddings from CLOOME<d-cite key="sanchez-fernandez_contrastive_2022"></d-cite>, an adapted version of CLIP and CLOOB for this kind of data. Both models were trained with an image resolution of 520x520 and structure-based vector representation (ECFP fingerprint). In both cases, images and molecules from a hold-out test set were used for this experiment. As it can be observed in the heatmap, for both models the molecule-to-molecule distances seem to be the most pronounced. Also, for the CLOOB-based model, the average distance between modalities is lower.
        </p>
        <p>
            Interestingly, the modality gap for CLIP trained with microscopy images and molecular structures is not as pronounced as for natural image and text pairs. For the latter, it can be observed in the dimensionality reduction plots (cite figure) that the sets of samples corresponding to each modality are parallel to each other, while for the former this is not the case. This could be caused by the fact that CLIP was trained with a considerably larger dataset than CLOOME. Hence, if a molecule and its corresponding image are similar enough to a pair of samples in the training set, it would be easier for the model to have learned to encode them with high similarity. However, this hypothesis needs to be further studied.
        </p>


        <h3 id="section-amumo-augmentation">Augmentation Analyses</h3>
        <p>
            As previously demonstrated, we can identify patterns in datasets and subsets of datasets using the
            similarity heatmap visualizations. Now, we would also like to see if we can use the same techniques to find
            patterns in augmentations of a single data point. For example, we take a single image, generate rotated
            versions of this image, and use this augmented dataset to compute CLIP embeddings and similarities. The
            results of this experiment for the three CLIP-like models are shown in the visualization below (note that we
            again show two variants of the CLOOB model). To generate this dataset, we gradually rotate a selected image
            by 360 degrees over the course of 100 steps. Each step results in a “new” image and a new data point. Note
            that we only augment the image, but not the text, which results in a completely homogeneous similarity in
            the in-modal text quadrant of the heatmap and homogenous stripes along the text dimension in the
            cross-modal quadrants of the heatmap.
        </p>
        <p>
            When looking at the in-modal image similarity quadrant of the heatmap for each model using augmentations of
            the first image, we can see an interesting pattern emerge. In addition to the bright yellow diagonal axis
            that corresponds to the similarities of images to themselves, there is also the perpendicular off-diagonal
            axis of the matrix sticking out. When hovering along the off-diagonal, we see that the two images along this
            axis are actually mirrored versions of each other. It seems like all models are invariant to the horizontal
            flip transformation for this image. We can also see a checkerboard-like pattern emerge for some images
            emerging in all models except for the CLOOB_LAION400M. When looking into the darker areas of this heatmap in
            more detail, we can see that the pattern occurs around multiples of 90-degree rotations. The fact that this
            pattern occurs mainly for the three models that use a CNN-based image encoder<d-footnote>Note that CLIP and
                CLOOB_LAION400M (both 400M instances) were trained on a much larger dataset than CLOOB and CyCLIP, which
                could also be an indicator for varying robustness.</d-footnote> and not for the one with the vision
            transformer could be an indicator that the two architectures vary in their ability to learn rotation
            invariant properties. The checkerboard-like pattern seems to be consistent with findings described
            by Timme et al.<d-cite key="timme_robustness_2020"></d-cite> where they tested the rotation
            robustness of various CNN classifiers by measuring the accuracy. The accuracy of the CNNs showed
            local maxima at multiples of 90-degree rotations and was lower in-between those angles.
        </p>
        <p>
            When looking at the overall distribution of similarity values, we also notice that the ViT-based CLOOB model
            seems to have more patches of low-similarity values compared to its CNN-based counterparts. This might
            indicate that ViT’s overall robustness to rotation transformations is lower. In further investigations, we
            might want to directly compare two versions of CLIP: the current version with the CNN-based image encoder
            and a version with a ViT-based image encoder, and study the phenomenon on a larger dataset.
        </p>
        <p>Use the interactions to explore the heatmaps for different images yourself.</p>

        <figure style="grid-column: screen;">
            <div id="similarity-heatmaps-rotated-picker" class="container"></div>
            <div class="container">
                <div class="row">
                    <div class="container col-xl-10 col-xs-12">
                        <div id="similarity-heatmaps-rotated" class="row"></div>
                    </div>
                    <div id="similarity-heatmaps-rotated-hover" class="col-xl-2 col-xs-12"></div>
                </div>

            </div>
        </figure>
        <script>
            augmented_heatmap_comparer('Rotated', 'similarity-heatmaps-rotated');
        </script>

        <p>
            In a second experiment, we analyze the heatmaps for an image to which we add an increasingly higher noise
            level. When looking at the heatmaps for the first image, it seems like there is a certain level of noise for
            each model, after which the model cannot seem to recognize the content of the image anymore. All images with
            a higher level of noise than this threshold seem to look (almost) the same (as indicated by a bright yellow
            rectangle at the lower-right corner of the in-modal image similarity quadrant).
        </p>

        <figure style="grid-column: screen;">
            <div id="similarity-heatmaps-noisy-picker" class="container"></div>
            <div class="container">
                <div class="row">
                    <div class="container col-xl-10 col-xs-12">
                        <div id="similarity-heatmaps-noisy" class="row"></div>
                    </div>
                    <div id="similarity-heatmaps-noisy-hover" class="col-xl-2 col-xs-12"></div>
                </div>

            </div>
        </figure>
        <script>
            augmented_heatmap_comparer('Noisy', 'similarity-heatmaps-noisy');
        </script>


        <p>
            Similarly, for blurry images, we see that at a certain point of blurriness, all images look the same to the
            models, and they cannot map images and texts together. You can use the dropdown menu to explore the effects
            of various augmentations.
        </p>

        <figure style="grid-column: screen;">
            <div class="container">
                Pick Augmentation method:
                <select id="augmentation-picker" style="margin-bottom: 10px;">
                    <option value="Blurred">Blurred</option>
                    <option value="HShift">HShift</option>
                    <option value="Noisy">Noisy</option>
                    <option value="Rotated">Rotated</option>
                    <option value="VShift">VShift</option>
                </select>
            </div>
            <div id="similarity-heatmaps-augmented-picker" class="container"></div>
            <div class="container">
                <div class="row">
                    <div class="container col-xl-10 col-xs-12">
                        <div id="similarity-heatmaps-augmented" class="row"></div>
                    </div>
                    <div id="similarity-heatmaps-augmented-hover" class="col-xl-2 col-xs-12"></div>
                </div>

            </div>
        </figure>
        <script>
            const augmentation_picker = document.getElementById('augmentation-picker');
            augmentation_picker.onchange = () => {
                augmented_heatmap_comparer(augmentation_picker.value, 'similarity-heatmaps-augmented');
            }
            augmented_heatmap_comparer(augmentation_picker.value, 'similarity-heatmaps-augmented');

        </script>

        <h2 id="section-conclusion">Conclusion</h2>
        <p>
            Throughout this article, we investigated latent embeddings of CLIP-like models. Using scatter plots and
            similarity heatmaps, we visualized and analyzed the modality gap that naturally occurs for CLIP embeddings.
            Closing this gap without losing significant performance can be important for downstream tasks like image
            generation, visual analytics, or human understanding. We showed how to close the gap using CyCLIP in
            combination with a post-processing method that aligns the embedding spaces and investigated another model
            (CLOOB) that is able to align the spaces during training. Finally, we introduced
            Amumo, an interactive visual prototype that allows users to explore embeddings from bi-modal contrastive
            learning models to help with understanding of their latent space embeddings. We used Amumo to analyze
            various (sub-)sets and augmentations of data. We believe that Amumo, and the similarity heatmap in
            particular, are useful tools to create intuition about bi-modal latent space embeddings. It allows for
            comparison of bi-modal models (e.g., their robustness to transformations) and can help to formulate
            hypotheses or ideas about such models. However, we want to stress that the analysis is based on a small
            subset of data points, and insights must still be verified on a larger scale.
        </p>



    </d-article>

    <d-appendix>

        <h2 id="section-acknowledgements">Acknowledgments</h2>
        <p>
            This work was funded by the Austrian Marshall Plan Foundation under the Marshall Plan Scholarship, the
            Austrian Science Fund under grant number FWF DFH 23--N, and under the Human-Interpretable Machine Learning
            project (funded by the State of Upper Austria). The project was conducted during a research visit at the
            MIT-IBM Watson AI Lab in Cambridge, MA.
            We would like to thank Elisabeth Rumetshofer for her feedback on CLOOB and its analysis.
        </p>
        
        <h2 id="section-researchmaterial">Research Material Statements</h2>
        <p>
            The data shown in this article were produced with the <a target="_blank"
                href="https://github.com/ginihumer/Amumo/">Amumo python package</a>.
            We provide notebooks to reproduce the results of <a target="_blank"
                href="https://github.com/ginihumer/Amumo/blob/main/notebooks/clip_article.ipynb">CLIP, CyCLIP, and
                CLOOB</a>
            ; TODO: add notebooks foradditional analyses…
            We provide the notebooks for exporting the data used in the interactive article:
            https://github.com/ginihumer/Amumo/blob/main/notebooks/export_data.ipynb; TODO: add export notebooks for
            additional analyses…
        </p>
        
        <h2 id="section-reproduce">Reproducibility</h2>
        <p>
            The data in this interactive article is precomputed.
            Use this <a target="_blank" href="https://github.com/ginihumer/Amumo/blob/main/notebooks/clip_article.ipynb"
                title="Open computational notebook">computational notebook</a> to reproduce the results shown in the
            article or as a starting point for your own investigations.
        </p>

        <h2 id="section-authorship">Authorship</h2>
        <p>
            <b>Christina Humer</b>: Conceptualization, Software, Validation, Investigation, Writing - Original Draft,
            Visualization. <b>Elisabeth Rumetshofer</b>: Investigation, Resources, Writing - Original Draft. <b>Ana Sánchez</b>:
            Investigation, Resources, Writing - Original Draft. <b>Vidya Prasad</b>: Investigation, Writing - Review & Editing.
            <b>Günter Klambauer</b>: Writing - Review & Editing. <b>Marc Streit</b>: Conceptualization, Writing - Review & Editing.
            <b>Hendrik Strobelt</b>: Conceptualization, Writing - Review & Editing.
        </p>
        
        <h2 id="section-conflict">Conflict of Interest</h2>
        <p>
            The authors declare that there are no competing interests.
        </p>


        <d-bibliography src="bibliography.bib"></d-bibliography>


        <hr>
        <h1 id="section-appendix">Appendix</h1>
        <p></p>
        <h2 id="section-lg-dataset">Closing the Gap Using a Larger Dataset</h2>

        <p>
            Amumo - our interactive prototype - is an easy way to explore a small subset of image-text pairs. This
            analysis can
            help form intuition about a particular dataset or the model used to map them into a latent space embedding.
            In addition to the interactive prototype, we also want to showcase the results of the proposed methods for
            closing the modality gap with a larger dataset. To that end, we again used the entire 5000 sample MSCOCO
            validation dataset and applied the two methods introduced in the article. We then take the aligned
            embeddings, project them with UMAP, and plot them in a static 2-d scatter plot with lines connecting the
            matching image-text pairs.
        </p>

        <h3 id="section-lg-dataset-manual">Manually Removing the Modality Gap</h3>
        <p>
            For the first method to remove the modality gap, we need to compute CyCLIP embeddings and manually move the
            two embedding spaces together. The following scatter plot shows the 2-dimensional projection of these
            modified embeddings. The plot shows that clusters are forming and a lot of connection lines are within
            clusters, which means that instances that carry a similar meaning are indeed close together in the latent
            space embedding. However, there are also a lot of intra-cluster connections. The emergence of these long
            connections between the clusters can be caused by various factors.
        </p>
        <p>
            For example, the manual modification of the latent space might disturb some parts of the latent space.
            Although CyCLIP does add restrictions to the objective function to facilitate the emergence of symmetric
            embedding spaces, it is only an approximation and the spaces are not perfectly perpendicular. Another
            influencing factor might be that the image-text pairs are not perceived as similar by the model and
            therefore placed in different areas of the embedding space. Finally, there may also be distortions coming
            from the dimensionality reduction technique we used for creating the 2-d space.
        </p>
        <p>
            To investigate these assumptions further, we recommend the use of interactive tools that allow exploring
            large sets of points and clusters in a 2-d space (e.g., the Projection Space Explorer<dt-cite
                key="eckelt_visual_2022"></dt-cite>).
        </p>
        <div>
            <img style="width: 100%" src="./exported_data/cyclip-rm-gap-5000.png" />
        </div>

        <p>
            The following plot shows the same procedure of manually removing the modality gap, but here we used CLIP
            embeddings instead. This shows again, that manually removing the gap by moving CLIP embeddings on the same
            plane destroys the trained latent space too much to be useful anymore.
        </p>
        <div>
            <img style="width: 100%" src="./exported_data/clip-rm-gap-5000.png" />
        </div>


        <h3 id="section-lg-dataset-inherent">Inherently Removing the Modality Gap</h3>
        <p>
            The second method of removing the modality gap utilizes a bi-modal model capable of aligning the embedding
            spaces: CLOOB.
            In this case, we can directly use the image and text
            embeddings generated by the model and project it to a 2-d space for visualization. We see a similar result
            to before: clusters emerge and a lot of inter-cluster connections, but also plenty of intra-cluster
            connections. There also seems to be a rather large cluster of points with many connections.
        </p>
        <div>
            <img style="width: 100%" src="./exported_data/cloob-rm-gap-5000.png" />
        </div>

        <p>
            In comparison, we also show the results for the same model, but trained with 400M instances of the LAION
            dataset and a vision transformer architecture used for image embedding instead of a CNN architecture. We can
            see smaller cluster entities and it seems like connections between clusters are less. To confirm these
            qualitative findings and be able to compare the methods, we would have to use quantitative measure (e.g.,
            measuring how many intra-cluster connections there are for each method).
        </p>
        <div>
            <img style="width: 100%" src="./exported_data/cloob400M-rm-gap-5000.png" />
        </div>


        <h2 id="section-lg-dataset-visually">Visually Closing the Modality Gap</h2>

        <p>
            In the previous sections, we learned about two ways that can help us close the modality gap:
        <ol>
            <li>
                Manually: post-process embeddings by moving them together along the modality gap vector; this only makes
                sense if the two embedding spaces are symmetric, like in CyCLIP.
            </li>
            <li>
                Inherently: define the model architecture and/or training objective in a way that aids the closing of
                the gap, like in CLOOB.
            </li>
        </ol>
        </p>
        <p>
            Let's also recall the reasons for why we would like to close the gap:
        <ol>
            <li>can have advantages for downstream tasks (e.g., if embeddings from two modalities need to be
                interchangeable)</li>
            <li>aids the development of multi-modal visual analytics tools</li>
            <li>match human expectations of how the embedding space should look like</li>
        </ol>
        </p>
        <p>
            <!-- The pre-hoc method, where we adopt a training objective that closes the modality gap, supports all three reasons; the post-hoc only reasons 1 and 2... -->
            From a visualization point of view, we might not care about the other two reasons, as long as we can
            visually close the modality gap. By visually closing the gap, we do not change the embeddings or the model,
            but map them into a shared low-dimensional space. This can be accomplished in several ways: (i)
            out-of-sample projection, (ii) concatenating image-text embeddings and treating them as one combined
            embedding.
            The first method results in a low-dimensional datapoint for each image and each text embedding; the second
            method results in a combined low-dimensional space for the image-text pairs. The method you would want to
            choose depends on the goal of the visualization.
        </p>

        <h3 id="section-lg-dataset-oos">Out-of-Sample Projection</h3>
        <p>
            We can first project embeddings of either images or texts using UMAP. This projection builds a neighborhood
            graph using
            in-modal similarities and results in a low-dimensional projection for one modality. We can utilize UMAP's
            out-of-sample projection to also project the embeddings of the second modality onto the space of the first
            modality. Since the out-of-sample projection again tries to map each point to the most similar points in the
            existing low-dimensional space, you can imagine this as using the cross-modal similarities between images
            and texts. Since this is what CLIP was trained on (i.e., optimizing distances between texts and images), the
            mapping should visually remove the modality gap.
        </p>
        <p>
            As an example, we take the 5000 sample MSCOCO validation set and first fit and transform the image
            embeddings. We then transform the text embeddings with the existing UMAP embedding and show the results in a
            scatter plot. The overall structure seems to align images and texts; while there are a lot of cross-cluster
            connections that show that image-text pairs are not always close to each other, most of the connections seem
            to be within clusters. Cross-cluster connections can be indicators of various things. For example, the pairs
            may not be deemed similar by CLIP, which results in embeddings that are far away from each other in the
            high-dimensional latent space, which would be reflected in the low-dimensional projection of the embeddings.
            Another issue might come from the projection method itself. As mentioned previously, projecting data to a
            low-dimensional space comes with a loss of information that might introduce artifacts. The fact that we use
            out-of-sample projection might amplify this effect even further.
        </p>
        <p>
            Visual analytics tools can be helpful in gaining further insights into the data and why certain pairs seem
            to
            be far away from each other. Basic interactions like hover information or selection summaries could already
            be a good start for further investigation. The rich nature of image and text data also allows for more
            advanced analytic visualizations; for example, texts can be used to extract labels for clusters that carry
            rich semantic meaning, or example images could be used to summarize clusters. Visually encoding the
            high-dimensional similarity of embedding pairs (e.g., as saturation of the lines between pairs) could be a
            helpful indicator that could show whether point pairs are far apart from each other due to artifacts from
            the projection or due to CLIP not recognizing them to be similar.
        </p>

        <div>
            <img style="width: 100%" src="./exported_data/oosExample.png" />
        </div>


        <h3 id="section-lg-dataset-concat">Concatenating Image and Text Embeddings</h3>
        <p>
            For a different kind of visual representation of image-text embedding spaces, we can simply concatenate the
            embedding vectors and transform them into a combined low-dimensional space. The low-dimensional space can be
            visualized in a scatter plot and visual analytics approaches can be used to explore the data. Note that with
            this approach, we only have one low-dimensional data point per image-text pair.
        </p>
        <div>
            <img style="width: 100%" src="./exported_data/concatExample.png" />
        </div>
    </d-appendix>


</body>