{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting soundfile\n",
      "  Using cached soundfile-0.12.1-py2.py3-none-win_amd64.whl (1.0 MB)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\christina\\appdata\\roaming\\python\\python39\\site-packages (from soundfile) (1.15.1)\n",
      "Collecting pycparser (from cffi>=1.0->soundfile)\n",
      "  Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
      "Installing collected packages: pycparser, soundfile\n",
      "Successfully installed pycparser-2.21 soundfile-0.12.1\n"
     ]
    }
   ],
   "source": [
    "# ! pip install git+https://github.com/facebookresearch/ImageBind\n",
    "! pip install soundfile\n",
    "! pip install librosa\n",
    "! pip install \"imagebind @ git+https://github.com/facebookresearch/ImageBind@c6a47d6dc2b53eced51d398c181d57049ca59286\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d2c64d1867448b87724548bdda9b66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Christina\\anaconda3\\envs\\amumo_test\\lib\\site-packages\\datasets\\table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "# create triplet dataset\n",
    "from datasets import load_dataset\n",
    "import soundfile as sf\n",
    "import requests\n",
    "import pandas as pd\n",
    "# https://huggingface.co/datasets/agkphysics/AudioSet\n",
    "# dataset = load_dataset(\"agkphysics/AudioSet\", \"bal\", split=\"test\")\n",
    "dataset = load_dataset(\"arrow\", split=\"test\", data_files={\"test\":\"C:/Users/Christina/.cache/huggingface/datasets/agkphysics___audio_set/bal/audio_set-test-000**-of-00018.arrow\"})#, split=\"test[0:10]\")\n",
    "# https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.filter\n",
    "dataset_it = dataset.to_iterable_dataset()\n",
    "\n",
    "data_dir = \"C:\\\\Users\\\\Christina\\\\Data\\\\imagebind\\\\text-audio-image\\\\\"\n",
    "animals = [\"Bird\", \"Cat\", \"Dog\", \"Horse\"]\n",
    "\n",
    "infos = []\n",
    "idx = 0\n",
    "for animal in animals:\n",
    "    for example in dataset_it.filter(lambda x: animal in x[\"human_labels\"]).take(25):\n",
    "        # save audiofile\n",
    "        sf.write(data_dir + \"audio\\\\%03d.wav\"%idx, example[\"audio\"][\"array\"], example[\"audio\"][\"sampling_rate\"], format=\"wav\")\n",
    "\n",
    "        # save image\n",
    "        # https://stackoverflow.com/questions/2068344/how-do-i-get-a-youtube-video-thumbnail-from-the-youtube-api\n",
    "        url = 'https://img.youtube.com/vi/%s/hqdefault.jpg'%example[\"video_id\"]\n",
    "        data = requests.get(url).content \n",
    "        with open(data_dir + \"image\\\\%03d.jpg\"%idx,'wb') as f:\n",
    "            f.write(data) \n",
    "\n",
    "        # save info\n",
    "        infos.append({\"id\": idx, \"video_id\":example[\"video_id\"], \"labels\":example[\"human_labels\"], \"animal\": animal})\n",
    "\n",
    "        idx += 1\n",
    "\n",
    "pd.DataFrame(infos).to_csv(data_dir + \"info.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61a6e5092bd843aea7d2235064690b54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Christina\\anaconda3\\envs\\amumo_test\\lib\\site-packages\\datasets\\table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "# # https://huggingface.co/docs/datasets/loading#slice-splits\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# # dataset = load_dataset(\"agkphysics/AudioSet\", \"bal\", split=\"test\") #, streaming=True\n",
    "# dataset = load_dataset(\"arrow\", split=\"test\", data_files={\"test\":\"C:/Users/Christina/.cache/huggingface/datasets/agkphysics___audio_set/bal/audio_set-test-000**-of-00018.arrow\"})#, split=\"test[0:10]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image-Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christina\\AppData\\Roaming\\Python\\Python39\\site-packages\\umap\\distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "C:\\Users\\Christina\\AppData\\Roaming\\Python\\Python39\\site-packages\\umap\\distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "C:\\Users\\Christina\\AppData\\Roaming\\Python\\Python39\\site-packages\\umap\\distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "C:\\Users\\Christina\\AppData\\Roaming\\Python\\Python39\\site-packages\\umap\\umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "from amumo import model as am_model\n",
    "from amumo import data as am_data\n",
    "from amumo import utils as am_utils\n",
    "from amumo import widgets as am_widgets\n",
    "\n",
    "# load dataset\n",
    "# dataset = am_data.DiffusionDB_Dataset(path=\"2m_first_1k\", batch_size=100) # data helper for the diffusionDB dataset; for the interactive prototype, we only use a random subset of 100 samples\n",
    "# all_images, all_prompts = dataset.get_data()\n",
    "# cache_name = 'diffusiondb_random_100' # path used to cache the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.05s)\n",
      "creating index...\n",
      "index created!\n",
      "100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'MSCOCO-Val_size-100'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Helpers\n",
    "def get_data_helper(dataset, filters=[], method=any):\n",
    "    all_images, all_prompts = dataset.get_filtered_data(filters, method=method)\n",
    "    print(len(all_images))\n",
    "\n",
    "    dataset_name = dataset.name\n",
    "    if len(filters) > 0:\n",
    "        dataset_name = dataset_name + '_filter-' + method.__name__ + '_' + '-'.join(filters)\n",
    "    else:\n",
    "        dataset_name = dataset_name + '_size-%i'%len(all_images)\n",
    "\n",
    "    return all_images, all_prompts, dataset_name\n",
    "\n",
    "# Load Data\n",
    "data_path = '../../../../../Data/'\n",
    "# subset of mscoco validation data\n",
    "dataset_mscoco_val = am_data.MSCOCO_Val_Dataset(path=data_path+'mscoco/validation', batch_size=100) \n",
    "mscoco_val_images, mscoco_val_prompts, mscoco_val_dataset_name = get_data_helper(dataset_mscoco_val, filters=[], method=any)\n",
    "mscoco_val_dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Christina\\anaconda3\\envs\\amumo_test\\lib\\site-packages\\torchvision\\transforms\\_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Christina\\anaconda3\\envs\\amumo_test\\lib\\site-packages\\torchvision\\transforms\\_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1 of 1\n",
      "batch 1 of 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb5b506efe3432bb8dfa64640f7e660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CLIPExplorerWidget(children=(VBox(children=(HBox(children=(Dropdown(description='Model: ', options=('ImageBind…"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found cached embeddings for MSCOCO-Val_size-100_CLIP_RN50 image\n",
      "found cached embeddings for MSCOCO-Val_size-100_CLIP_RN50 text\n",
      " False\n",
      "found cached embeddings for MSCOCO-Val_size-100_ImageBind_huge image\n",
      "found cached embeddings for MSCOCO-Val_size-100_ImageBind_huge text\n",
      "found cached embeddings for MSCOCO-Val_size-100_CLIP_RN50 image\n",
      "found cached embeddings for MSCOCO-Val_size-100_CLIP_RN50 text\n",
      "found cached embeddings for MSCOCO-Val_size-100_ImageBind_huge image\n",
      "found cached embeddings for MSCOCO-Val_size-100_ImageBind_huge text\n",
      " True\n",
      " True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\christina\\repositories\\icg\\researchstay\\amumo\\amumo\\utils.py:126: ClusterWarning:\n",
      "\n",
      "The symmetric non-negative hollow observation matrix looks suspiciously like an uncondensed distance matrix\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " False\n",
      "found cached embeddings for MSCOCO-Val_size-100_ImageBind_huge image\n",
      "found cached embeddings for MSCOCO-Val_size-100_ImageBind_huge text\n",
      "found cached embeddings for MSCOCO-Val_size-100_ImageBind_huge image\n",
      "found cached embeddings for MSCOCO-Val_size-100_ImageBind_huge text\n",
      " True\n",
      "found cached embeddings for MSCOCO-Val_size-100_CLIP_RN50 image\n",
      "found cached embeddings for MSCOCO-Val_size-100_CLIP_RN50 text\n"
     ]
    }
   ],
   "source": [
    "am_widgets.CLIPExplorerWidget(mscoco_val_dataset_name, all_data={\"image\": mscoco_val_images, \"text\": mscoco_val_prompts}, models=[am_model.ImageBind_Model(), \"CLIP\"]) # {\"image\": all_images, \"depth\": all_depths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True [False, False] {'ImageBind': <amumo.model.ImageBind_Model object at 0x000001B878F032E0>, 'CLIP': <amumo.model.CLIPModel object at 0x000001B767795E20>}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\christina\\repositories\\icg\\researchstay\\amumo\\amumo\\widgets.py:940: FutureWarning:\n",
      "\n",
      "The input object of type 'PngImageFile' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'PngImageFile', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "\n",
      "c:\\users\\christina\\repositories\\icg\\researchstay\\amumo\\amumo\\widgets.py:940: VisibleDeprecationWarning:\n",
      "\n",
      "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found cached embeddings for diffusiondb_random_100_ImageBind_huge\n",
      "found cached embeddings for diffusiondb_random_100_CLIP_RN50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e29ae00ffa9f41edaaf7549652f884de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CLIPComparerWidget(children=(HoverWidget(children=(VBox(children=(HTML(value='', layout=Layout(width='300px'))…"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "am_widgets.CLIPComparerWidget(cache_name, all_images=all_images, all_prompts=all_prompts, models=[am_model.ImageBind_Model(), \"CLIP\"]) # {\"image\": all_images, \"depth\": all_depths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image-Text-Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christina\\AppData\\Roaming\\Python\\Python39\\site-packages\\umap\\distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "C:\\Users\\Christina\\AppData\\Roaming\\Python\\Python39\\site-packages\\umap\\distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "C:\\Users\\Christina\\AppData\\Roaming\\Python\\Python39\\site-packages\\umap\\distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "C:\\Users\\Christina\\AppData\\Roaming\\Python\\Python39\\site-packages\\umap\\umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "from amumo import model as am_model\n",
    "from amumo import data as am_data\n",
    "from amumo import utils as am_utils\n",
    "from amumo import widgets as am_widgets\n",
    "# ! pip install torch\n",
    "# ! pip install torchaudio\n",
    "import torch\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import io\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class AudioType(am_data.DataTypeInterface):\n",
    "    name = \"Audio\"\n",
    "\n",
    "    def __init__(self, data, sample_rate) -> None:\n",
    "        super().__init__(data)\n",
    "        self.sample_rate = sample_rate\n",
    "    \n",
    "    def getVisItem(self, idx):\n",
    "        import soundfile as sf\n",
    "        buffer = io.BytesIO()\n",
    "        sf.write(buffer, self.data[idx][0], self.sample_rate, format=\"wav\")\n",
    "        audio_bytes = buffer.getvalue() # wav bytes\n",
    "        return {\"displayType\": am_data.DisplayTypes.AUDIO, \"value\": audio_bytes}\n",
    "\n",
    "\n",
    "\n",
    "class Triplet_Dataset(am_data.DatasetInterface):\n",
    "    name='Triplet'\n",
    "\n",
    "    def __init__(self, path, seed=31415, batch_size=100, sample_rate=16000):\n",
    "        # create triplet dataset if it does not exist\n",
    "        super().__init__(path, seed, batch_size)\n",
    "        # path: path to the triplet dataset\n",
    "        image_paths = glob(path + \"image\\\\*.jpg\", recursive = True)\n",
    "        audio_paths = glob(path + \"audio\\\\*.wav\", recursive = True)\n",
    "\n",
    "        self.sample_rate = sample_rate\n",
    "        \n",
    "        all_images = []\n",
    "        for image_path in image_paths:\n",
    "            with open(image_path, \"rb\") as fopen:\n",
    "                image = Image.open(fopen).convert(\"RGB\")\n",
    "                all_images.append(image)\n",
    "\n",
    "        all_audios = []\n",
    "        for audio_path in audio_paths:\n",
    "            waveform, sr = torchaudio.load(audio_path)\n",
    "            if sample_rate != sr:\n",
    "                waveform = torchaudio.functional.resample(\n",
    "                    waveform, orig_freq=sr, new_freq=sample_rate\n",
    "                )\n",
    "            all_audios.append(waveform)\n",
    "        \n",
    "        self.all_infos = pd.read_csv(path + \"info.csv\", converters={\"labels\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \")})\n",
    "\n",
    "        # TODO... load on demand with a custom loader\n",
    "        self.all_images = np.array(all_images)\n",
    "        self.all_prompts = np.array(self.all_infos[\"labels\"].map(lambda x: \", \".join(x)))\n",
    "        self.all_audios = np.array(all_audios)\n",
    "    \n",
    "    \n",
    "    def get_data(self):\n",
    "        \n",
    "        if self.batch_size is None:\n",
    "            images = self.MODE1_Type(self.all_images)\n",
    "            texts = self.MODE2_Type(self.all_prompts)\n",
    "            audios = AudioType(self.all_audios, self.sample_rate)\n",
    "        \n",
    "            return images, texts, audios\n",
    "\n",
    "        # create a random batch\n",
    "        batch_idcs = self._get_random_subsample(len(self.all_images))\n",
    "\n",
    "        images = self.MODE1_Type(self.all_images[batch_idcs])\n",
    "        texts = self.MODE2_Type(self.all_prompts[batch_idcs])\n",
    "        audios = AudioType(self.all_audios[batch_idcs], self.sample_rate)\n",
    "        \n",
    "        return images, texts, audios\n",
    "    \n",
    "        \n",
    "    def get_filtered_data(self, filter_list, method=any):\n",
    "        # filter_list: a list of strings that are used for filtering\n",
    "        # method: any -> any substring given in filter_list is present; all -> all substrings must be contained in the string\n",
    "        if filter_list is None or len(filter_list) <= 0:\n",
    "            return self.get_data()\n",
    "\n",
    "        subset_ids = np.array([i for i in range(len(self.all_prompts)) if method(substring in self.all_prompts[i].lower() for substring in filter_list)])\n",
    "        if len(subset_ids) <= 0:\n",
    "            print(\"no filter matches found\")\n",
    "            return [], [], []\n",
    "        \n",
    "        # create a random batch\n",
    "        batch_idcs = self._get_random_subsample(len(subset_ids))\n",
    "        subset_ids = subset_ids[batch_idcs]\n",
    "        \n",
    "        images = self.MODE1_Type(self.all_images[subset_ids])\n",
    "        texts = self.MODE2_Type(self.all_prompts[subset_ids])\n",
    "        audios = AudioType(self.all_audios[subset_ids], self.sample_rate)\n",
    "        return images, texts, audios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_33712\\2901351163.py:55: FutureWarning:\n",
      "\n",
      "The input object of type 'Image' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Image', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "\n",
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_33712\\2901351163.py:55: VisibleDeprecationWarning:\n",
      "\n",
      "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\n",
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_33712\\2901351163.py:57: FutureWarning:\n",
      "\n",
      "The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "\n",
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_33712\\2901351163.py:57: VisibleDeprecationWarning:\n",
      "\n",
      "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100\n",
      "found cached embeddings for test_audio_ImageBind_huge text\n",
      "found cached embeddings for test_audio_ImageBind_huge audio\n",
      " False\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16da212cb0bb43dd8ba015af517e1f2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CLIPExplorerWidget(children=(VBox(children=(HBox(children=(Dropdown(description='Model: ', options=('ImageBind…"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 39 ['text', 'audio'] 1 0\n",
      "98 38 ['audio'] 0 0\n",
      "93 39 ['audio'] 0 0\n",
      "90 43 ['audio'] 0 0\n",
      "65 65 ['audio'] 0 0\n",
      "43 83 ['audio'] 0 0\n",
      "41 91 ['audio'] 0 0\n",
      "92 86 ['audio'] 0 0\n",
      "13 91 ['audio'] 0 0\n",
      "33 64 ['audio'] 0 0\n",
      "68 57 ['audio'] 0 0\n",
      "98 73 ['text'] 0 0\n",
      "87 68 ['text'] 0 0\n",
      "87 62 ['text'] 0 0\n",
      "87 51 ['text'] 0 0\n",
      "87 50 ['text'] 0 0\n",
      "84 39 ['text'] 0 0\n",
      "81 32 ['text'] 0 0\n",
      "72 33 ['text'] 0 0\n",
      "54 36 ['text'] 0 0\n",
      "52 36 ['text'] 0 0\n",
      "46 43 ['text'] 0 0\n",
      "44 46 ['text'] 0 0\n",
      "45 46 ['text'] 0 0\n",
      "46 45 ['text'] 0 0\n",
      "45 44 ['text'] 0 0\n",
      "44 44 ['text'] 0 0\n",
      "54 46 ['text'] 0 0\n"
     ]
    }
   ],
   "source": [
    "dataset = Triplet_Dataset(path=\"C:\\\\Users\\\\Christina\\\\Data\\\\imagebind\\\\text-audio-image\\\\\", batch_size=100)\n",
    "all_images, all_prompts, all_audios = dataset.get_data()\n",
    "print(len(all_images), len(all_prompts), len(all_audios))\n",
    "\n",
    "from importlib import reload\n",
    "reload(am_widgets)\n",
    "my_widget = am_widgets.CLIPExplorerWidget(\"test_audio\", all_data={\"text\": all_prompts, \"audio\": all_audios}, models=[am_model.ImageBind_Model()])\n",
    "my_widget.scatter_widget.select_projection_method.value = \"PCA\"\n",
    "my_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found cached embeddings for test_audio_ImageBind_huge text\n",
      "found cached embeddings for test_audio_ImageBind_huge audio\n",
      "found cached embeddings for test_audio_ImageBind_huge image\n",
      " False\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e257ff53e7264549881ce66e51044749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CLIPExplorerWidget(children=(VBox(children=(HBox(children=(Dropdown(description='Model: ', options=('ImageBind…"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset = Triplet_Dataset(path=\"C:\\\\Users\\\\Christina\\\\Data\\\\imagebind\\\\text-audio-image\\\\\", batch_size=100)\n",
    "# all_images, all_prompts, all_audios = dataset.get_data()\n",
    "# print(len(all_images), len(all_prompts), len(all_audios))\n",
    "\n",
    "my_widget = CLIPExplorerWidget(\"test_audio\", all_data={\"text\": all_prompts, \"audio\": all_audios, \"image\": all_images}, models=[am_model.ImageBind_Model()]) \n",
    "my_widget.scatter_widget.select_projection_method.value = \"PCA\"\n",
    "my_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PartialTSNEEmbedding([[ 8.66231252e-01, -1.80334395e+00],\n",
       "                      [-6.98845549e-02,  4.86421987e-01],\n",
       "                      [-1.80532078e+00, -4.17764983e-01],\n",
       "                      [-1.69716285e+00, -4.86806520e-01],\n",
       "                      [ 6.34820991e-01,  4.77157939e-01],\n",
       "                      [-7.02025889e-01,  9.50794310e-01],\n",
       "                      [ 6.78130001e-01,  8.57959178e-01],\n",
       "                      [-1.88099454e+00,  8.72435976e-01],\n",
       "                      [-1.56499882e-02,  1.42629699e-01],\n",
       "                      [ 2.93348297e-01,  2.69271462e+00],\n",
       "                      [ 1.79785216e+00, -2.89591829e-01],\n",
       "                      [ 8.25588826e-01, -3.33093071e-01],\n",
       "                      [-3.73619880e-01, -8.62589914e-01],\n",
       "                      [-6.13675611e-01,  1.70157657e+00],\n",
       "                      [ 1.13072201e+00, -1.31279701e+00],\n",
       "                      [ 1.03003015e+00,  9.68657049e-01],\n",
       "                      [-1.19196244e+00, -7.00524880e-01],\n",
       "                      [ 3.59805494e-01, -9.34062436e-02],\n",
       "                      [-1.57394147e+00, -3.81698429e-01],\n",
       "                      [-1.38493737e+00, -4.87656220e-01],\n",
       "                      [-1.73849665e+00,  1.30661619e+00],\n",
       "                      [ 4.69603287e-01, -1.10053191e+00],\n",
       "                      [-1.85401090e+00, -4.63749291e-01],\n",
       "                      [ 8.62070662e-01,  6.53581685e-01],\n",
       "                      [ 2.36916843e+00, -1.29850527e+00],\n",
       "                      [ 1.54058995e+00,  1.54267608e+00],\n",
       "                      [ 1.12519129e+00,  1.69921872e-01],\n",
       "                      [-1.33976856e+00, -1.95693303e-01],\n",
       "                      [-1.74885858e+00, -4.57466235e-01],\n",
       "                      [-1.84019753e+00, -3.91253715e-01],\n",
       "                      [-7.10629694e-01, -5.62369836e-01],\n",
       "                      [ 1.52154671e+00, -2.26323855e+00],\n",
       "                      [-3.59708323e-01,  6.53028378e-01],\n",
       "                      [-8.45590964e-01,  8.39538277e-01],\n",
       "                      [ 8.71761578e-01,  7.86592020e-01],\n",
       "                      [ 1.46027409e+00,  2.25379438e-01],\n",
       "                      [ 4.86747374e-01,  7.41036706e-01],\n",
       "                      [ 1.79141724e+00, -6.17732723e-01],\n",
       "                      [-7.73905933e-01,  8.45014011e-01],\n",
       "                      [-1.73132760e+00,  1.14963643e+00],\n",
       "                      [ 7.53461010e-01, -4.83165795e-01],\n",
       "                      [ 1.30760860e-01,  5.82341108e-01],\n",
       "                      [ 4.85478865e-01,  9.70411254e-01],\n",
       "                      [ 6.99849711e-02, -1.07213897e-01],\n",
       "                      [ 5.98076264e-01,  1.57475883e+00],\n",
       "                      [ 9.09518359e-01, -3.50901714e-01],\n",
       "                      [-1.03265267e+00, -1.39754573e-01],\n",
       "                      [ 1.33741519e+00,  3.34620140e-01],\n",
       "                      [ 1.85316509e+00, -8.66616851e-01],\n",
       "                      [ 7.13854913e-01,  6.96264427e-01],\n",
       "                      [-1.23785241e+00, -4.89448989e-01],\n",
       "                      [ 1.50616016e+00,  3.78724045e-04],\n",
       "                      [ 1.83442901e+00, -5.59370815e-01],\n",
       "                      [-4.40891037e-01,  1.66516006e+00],\n",
       "                      [-1.76278783e+00, -7.72983816e-01],\n",
       "                      [-1.58533177e-01,  2.17088859e-03],\n",
       "                      [ 1.44191733e+00, -6.44938636e-01],\n",
       "                      [ 3.92938145e-01, -1.75172995e+00],\n",
       "                      [ 1.42323622e-01,  5.32800731e-01],\n",
       "                      [ 9.38900226e-01, -5.09669136e-01],\n",
       "                      [ 1.43536635e+00,  8.07487693e-02],\n",
       "                      [-7.12271285e-01, -1.92067515e-02],\n",
       "                      [-3.07681662e+00, -2.94524947e-01],\n",
       "                      [ 9.38678966e-01,  9.20833914e-01],\n",
       "                      [ 1.12357831e+00,  8.91754679e-02],\n",
       "                      [ 8.71742936e-01,  3.95757747e-01],\n",
       "                      [ 1.11563716e+00,  9.94055200e-01],\n",
       "                      [-1.61168403e+00,  1.97060078e+00],\n",
       "                      [-1.28725168e+00, -3.38750143e-01],\n",
       "                      [ 5.94465706e-01, -1.04923300e+00],\n",
       "                      [-8.96757713e-01, -4.72063101e-01],\n",
       "                      [ 1.28687792e+00, -2.73019463e+00],\n",
       "                      [-6.18864711e-01, -4.47773973e-01],\n",
       "                      [-1.27638240e+00, -2.40858343e+00],\n",
       "                      [ 7.93887597e-01,  5.39978960e-01],\n",
       "                      [-1.14842899e-02,  6.94548650e-01],\n",
       "                      [-1.81724634e+00,  8.38377359e-01],\n",
       "                      [ 1.28702799e+00, -6.93361049e-01],\n",
       "                      [ 3.24353667e-01,  5.52903747e-01],\n",
       "                      [-9.59983885e-01,  2.12443083e+00],\n",
       "                      [-1.42369511e+00, -5.29626951e-01],\n",
       "                      [-4.10343441e-01,  1.80024402e-01],\n",
       "                      [ 2.44738973e-01, -7.10713835e-01],\n",
       "                      [-1.62432537e+00, -7.76180537e-01],\n",
       "                      [ 1.82394371e+00, -7.01573238e-01],\n",
       "                      [-2.14903252e+00,  1.99517194e+00],\n",
       "                      [ 2.29119881e+00, -1.33138696e+00],\n",
       "                      [ 3.33479562e-01,  1.23440773e+00],\n",
       "                      [ 6.29515555e-01, -4.86417046e-01],\n",
       "                      [ 1.05024445e+00, -5.29534252e-01],\n",
       "                      [ 1.04128389e-01,  2.32304786e-01],\n",
       "                      [-1.60225360e+00, -4.29806979e-01],\n",
       "                      [-1.51832568e+00, -1.30532097e+00],\n",
       "                      [-1.79348324e+00, -5.26680188e-01],\n",
       "                      [-1.95347568e+00,  4.99278037e-01],\n",
       "                      [-1.83111869e+00, -4.13196746e-01],\n",
       "                      [-6.80968573e-01,  1.42687844e+00],\n",
       "                      [ 1.56669390e+00, -2.35092238e+00],\n",
       "                      [ 1.74571326e+00,  1.81269059e+00],\n",
       "                      [ 5.20085845e-01,  6.27048979e-01]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TSNE().fit(np.random.rand(100, 100)).transform(np.random.rand(100, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100\n",
      "found cached embeddings for test_audio_ImageBind_huge image\n",
      "found cached embeddings for test_audio_ImageBind_huge text\n",
      "found cached embeddings for test_audio_ImageBind_huge audio\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7347af2a15474d6bbbabe1be45bae8a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SimilarityHeatmapClusteringWidget(children=(HBox(children=(Checkbox(value=False, description='Cluster matrix b…"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "embeddings, logit_scale = get_embeddings_per_modality(am_model.ImageBind_Model(), \"test_audio\", {\"image\": all_images, \"text\": all_prompts, \"audio\": all_audios}, batch_size=100)\n",
    "\n",
    "heatmap = SimilarityHeatmapClusteringWidget(cluster_label_data=all_prompts)\n",
    "heatmap.embedding = {\"image\": np.array(embeddings[\"image\"]), \"text\": np.array(embeddings[\"text\"]), \"audio\": np.array(embeddings[\"audio\"])}\n",
    "heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 200)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heatmap.value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities_all = get_similarities_all(heatmap.embedding)\n",
    "len(similarities_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 300)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(similarities_all[np.concatenate([heatmap.idcs + i*heatmap.size for i in range(len(heatmap.modality_labels))], axis=0),:][:,np.concatenate([heatmap.idcs + i*heatmap.size for i in range(len(heatmap.modality_labels))], axis=0)]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_30488\\3688791554.py:54: FutureWarning: The input object of type 'Image' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Image', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  self.all_images = np.array(all_images)\n",
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_30488\\3688791554.py:54: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.all_images = np.array(all_images)\n",
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_30488\\3688791554.py:56: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  self.all_audios = np.array(all_audios)\n",
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_30488\\3688791554.py:56: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.all_audios = np.array(all_audios)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Christina\\anaconda3\\envs\\amumo_test\\lib\\site-packages\\torchvision\\transforms\\_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Christina\\anaconda3\\envs\\amumo_test\\lib\\site-packages\\torchvision\\transforms\\_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found cached embeddings for test_audio_ImageBind_huge text\n",
      "found cached embeddings for test_audio_ImageBind_huge image\n",
      "found cached embeddings for test_audio_ImageBind_huge audio\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffcae93c2a0d4d73929b84c77890dc6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CLIPExplorerWidget(children=(VBox(children=(HBox(children=(Dropdown(description='Model: ', options=('ImageBind…"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "my_widget = am_widgets.CLIPExplorerWidget(\"test_audio\", all_data={\"text\": all_prompts, \"image\": all_images, \"audio\": all_audios }, models=[am_model.ImageBind_Model()]) \n",
    "my_widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image-Thermal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ThermalType(am_data.ImageType):\n",
    "    name = \"Thermal\"\n",
    "\n",
    "    def __init__(self, data) -> None:\n",
    "        super().__init__(data)\n",
    "\n",
    "\n",
    "class LLVIP_Dataset(am_data.DatasetInterface):\n",
    "    name='LLVIP'\n",
    "\n",
    "    def __init__(self, path, seed=31415, batch_size = 100):\n",
    "        # download dataset: https://bupt-ai-cz.github.io/LLVIP/\n",
    "        super().__init__(path, seed, batch_size)\n",
    "        # path: path to the LLVIP dataset\n",
    "        image_paths = glob(path + \"\\\\visible\\\\test\\\\*.jpg\", recursive = True)\n",
    "        thermal_paths = glob(path + \"\\\\infrared\\\\test\\\\*.jpg\", recursive = True)\n",
    "        \n",
    "        batch_idcs = self._get_random_subsample(len(image_paths))\n",
    "        image_paths = np.array(image_paths)[batch_idcs]\n",
    "        thermal_paths = np.array(thermal_paths)[batch_idcs]\n",
    "        \n",
    "        all_images = []\n",
    "        for image_path in image_paths:\n",
    "            with open(image_path, \"rb\") as fopen:\n",
    "                image = Image.open(fopen).convert(\"RGB\")\n",
    "                all_images.append(image)\n",
    "\n",
    "            \n",
    "        all_thermals = []\n",
    "        \n",
    "        for thermal_path in thermal_paths:\n",
    "            with open(thermal_path, \"rb\") as fopen:\n",
    "                thermal = Image.open(fopen).convert(\"L\")\n",
    "                all_thermals.append(thermal)\n",
    "\n",
    "        self.MODE2_Type = ThermalType\n",
    "\n",
    "        # TODO... load images and thermals on demand with a custom loader\n",
    "        self.all_images = np.array(all_images)\n",
    "        self.all_prompts = np.array(all_thermals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_24244\\3221552142.py:99: FutureWarning: The input object of type 'Image' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Image', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  self.all_images = np.array(all_images)\n",
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_24244\\3221552142.py:99: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.all_images = np.array(all_images)\n",
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_24244\\3221552142.py:100: FutureWarning: The input object of type 'Image' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Image', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  self.all_prompts = np.array(all_thermals)\n",
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_24244\\3221552142.py:100: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.all_prompts = np.array(all_thermals)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100\n",
      "found cached embeddings for test_thermal_ImageBind_huge image\n",
      "batch 1 of 1\n",
      "torch.Size([100, 1, 224, 224])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17508ad4cb6642ec8692300f7a57d876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CLIPExplorerWidget(children=(VBox(children=(HBox(children=(Dropdown(description='Model: ', options=('ImageBind…"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download dataset: https://bupt-ai-cz.github.io/LLVIP/\n",
    "dataset = LLVIP_Dataset(path=\"C:\\\\Users\\\\Christina\\\\Data\\\\imagebind\\\\LLVIP\\\\\", batch_size=100) \n",
    "all_images, all_thermals = dataset.get_data()\n",
    "print(len(all_images), len(all_thermals))\n",
    "\n",
    "am_widgets.CLIPExplorerWidget(\"test_thermal\", all_data={\"image\": all_images, \"thermal\": all_thermals}, models=[am_model.ImageBind_Model()]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image-Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class DepthType(am_data.ImageType):\n",
    "    name = \"Depth\"\n",
    "\n",
    "    def __init__(self, data) -> None:\n",
    "        # data is an array of (1,224,224) tensors\n",
    "        super().__init__(data)\n",
    "\n",
    "    def getVisItem(self, idx):\n",
    "        output_img = io.BytesIO()\n",
    "        plt.imsave(output_img, self.data[idx][0], cmap='gray')\n",
    "        plt.savefig(output_img, format='JPEG')\n",
    "        return {\"displayType\": am_data.DisplayTypes.IMAGE, \"value\": output_img.getvalue()}\n",
    "    \n",
    "    \n",
    "\n",
    "class SUNRGBD_Dataset(am_data.DatasetInterface):\n",
    "    name='SUNRGBD_NYU'\n",
    "\n",
    "    def __init__(self, path, seed=31415, batch_size = 100):\n",
    "        # download \"SUNRGBD_V1\" dataset from https://rgbd.cs.princeton.edu/\n",
    "        super().__init__(path, seed, batch_size)\n",
    "        # path: path to the SUNRGBD dataset\n",
    "        image_paths = glob(path + \"\\\\kv1\\\\NYUdata\\\\NYU*\\\\fullres\\\\*.jpg\", recursive = True)\n",
    "        depth_paths = glob(path + \"\\\\kv1\\\\NYUdata\\\\NYU*\\\\fullres\\\\*.png\", recursive = True)\n",
    "        \n",
    "        batch_idcs = self._get_random_subsample(len(image_paths))\n",
    "        image_paths = np.array(image_paths)[batch_idcs]\n",
    "        depth_paths = np.array(depth_paths)[batch_idcs]\n",
    "        \n",
    "        all_images = []\n",
    "        for image_path in image_paths:\n",
    "            with open(image_path, \"rb\") as fopen:\n",
    "                image = Image.open(fopen).convert(\"RGB\")\n",
    "                all_images.append(image)\n",
    "\n",
    "            \n",
    "        all_depths = []\n",
    "        for depth_path in depth_paths:\n",
    "            with open(depth_path, \"rb\") as fopen:\n",
    "                depth = Image.open(fopen)\n",
    "                depth = np.array(depth, dtype=int)\n",
    "                depth = depth.astype(np.float32) / depth.max() # TODO: need to normalize?\n",
    "                # depth = depth[np.newaxis,:,:] # need 1 channel -> (1,224,224)\n",
    "                depth = torch.from_numpy(depth).unsqueeze(0) # need 1 channel -> (1,224,224)\n",
    "                all_depths.append(depth)\n",
    "\n",
    "        self.MODE2_Type = DepthType\n",
    "\n",
    "        # TODO... load images and depths on demand with a custom loader\n",
    "        self.all_images = np.array(all_images)\n",
    "        self.all_prompts = np.array(all_depths)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_24244\\3221552142.py:64: FutureWarning:\n",
      "\n",
      "The input object of type 'Image' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Image', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "\n",
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_24244\\3221552142.py:64: VisibleDeprecationWarning:\n",
      "\n",
      "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\n",
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_24244\\3221552142.py:65: FutureWarning:\n",
      "\n",
      "The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "\n",
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_24244\\3221552142.py:65: VisibleDeprecationWarning:\n",
      "\n",
      "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100\n",
      "found cached embeddings for test_depth_ImageBind_huge image\n",
      "found cached embeddings for test_depth_ImageBind_huge depth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f38b43e86ba4314a512d7d62837bade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CLIPExplorerWidget(children=(VBox(children=(HBox(children=(Dropdown(description='Model: ', options=('ImageBind…"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download \"SUNRGBD_V1\" dataset from https://rgbd.cs.princeton.edu/\n",
    "dataset = SUNRGBD_Dataset(path=\"C:\\\\Users\\\\Christina\\\\Data\\\\imagebind\\\\SUNRGBD\\\\\", batch_size=100)\n",
    "all_images, all_depths = dataset.get_data()\n",
    "print(len(all_images), len(all_depths))\n",
    "\n",
    "am_widgets.CLIPExplorerWidget(\"test_depth\", all_data={\"image\": all_images, \"depth\": all_depths}, models=[am_model.ImageBind_Model()]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amumo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
