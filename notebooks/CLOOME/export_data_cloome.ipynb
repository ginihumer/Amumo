{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Data for CLOOM Analysis\n",
    "### This notebook exports data for the CLOOM analysis done after the interactive article was accepted by VISxAI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install git+https://github.com/ginihumer/Amumo.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import amumo\n",
    "from amumo import data as am_data\n",
    "from amumo import utils as am_utils\n",
    "from amumo import model as am_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Helpers\n",
    "def get_data_helper(dataset, filters=[], method=any):\n",
    "    all_images, all_prompts = dataset.get_filtered_data(filters, method=method)\n",
    "    print(len(all_images))\n",
    "\n",
    "    dataset_name = dataset.name\n",
    "    if len(filters) > 0:\n",
    "        dataset_name = dataset_name + '_filter-' + method.__name__ + '_' + '-'.join(filters)\n",
    "    else:\n",
    "        dataset_name = dataset_name + '_size-%i'%len(all_images)\n",
    "\n",
    "    return all_images, all_prompts, dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def create_dir_if_not_exists(dir):\n",
    "    if not os.path.exists(dir):\n",
    "        os.mkdir(dir)\n",
    "    return dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_directory = './exported_data_checkpoints/'\n",
    "create_dir_if_not_exists(export_directory)\n",
    "\n",
    "def export_data(dataset_name, images, prompts, models):\n",
    "\n",
    "    # create folder structure\n",
    "    dataset_directory = create_dir_if_not_exists(export_directory + dataset_name)\n",
    "    images_dir = create_dir_if_not_exists(dataset_directory + '/images')\n",
    "    similarities_dir = create_dir_if_not_exists(dataset_directory + '/similarities')\n",
    "\n",
    "    # save images\n",
    "    for i in range(len(images)):\n",
    "        im = images[i]\n",
    "        im.resize((400,400))\n",
    "        im.save('%s/%i.jpg'%(images_dir,i))\n",
    "\n",
    "    # save texts\n",
    "    with open(dataset_directory + \"/prompts.txt\", \"w\") as file:\n",
    "        for prompt in prompts:\n",
    "            file.write(prompt + \"\\n\")\n",
    "\n",
    "    # export projections and similarities\n",
    "    import torch\n",
    "    from sklearn.decomposition import PCA\n",
    "    from openTSNE import TSNE\n",
    "    from umap import UMAP\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import json\n",
    "\n",
    "    projections_df = pd.DataFrame({'emb_id': list(np.arange(0,len(images),1))+list(np.arange(0,len(prompts),1)), 'data_type':['image']*len(images)+['text']*len(prompts)})\n",
    "\n",
    "\n",
    "    for model in models:\n",
    "        # compute embeddings\n",
    "        image_embedding_gap, text_embedding_gap, logit_scale = am_utils.get_embedding(model, dataset_name, images, prompts)\n",
    "        image_embedding_nogap, text_embedding_nogap = am_utils.get_closed_modality_gap(image_embedding_gap, text_embedding_gap)\n",
    "        \n",
    "        for image_embedding, text_embedding, mode in [(image_embedding_gap, text_embedding_gap, ''), (image_embedding_nogap, text_embedding_nogap, '_nogap')]:\n",
    "            \n",
    "            # compute similarities\n",
    "            similarity_image_text, similarity = am_utils.get_similarity(image_embedding, text_embedding)\n",
    "            np.savetxt('%s/%s%s.csv'%(similarities_dir,model.model_name,mode), similarity, delimiter=',')\n",
    "            \n",
    "            # compute meta information and similarity clustering\n",
    "            meta_info = {}\n",
    "            meta_info['gap_distance'] = float(am_utils.get_modality_distance(image_embedding, text_embedding))\n",
    "            meta_info['loss'] = float(am_utils.calculate_val_loss(image_embedding, text_embedding, logit_scale.exp()))\n",
    "\n",
    "            idcs, clusters, clusters_unsorted = am_utils.get_cluster_sorting(similarity_image_text)\n",
    "            cluster_labels = []\n",
    "            cluster_sizes = []\n",
    "            for c in set(clusters):\n",
    "                cluster_size = int(np.count_nonzero(clusters==c))\n",
    "                cluster_label = prompts.getMinSummary(np.where(clusters_unsorted==c)[0]) # am_utils.get_textual_label_for_cluster(np.where(clusters_unsorted==c)[0], prompts)\n",
    "                cluster_labels.append(cluster_label)\n",
    "                cluster_sizes.append(cluster_size)\n",
    "\n",
    "            idcs_reverse = np.argsort(idcs)\n",
    "            meta_info['cluster_sort_idcs'] = idcs.tolist()\n",
    "            meta_info['cluster_sort_idcs_reverse'] = idcs_reverse.tolist()\n",
    "            meta_info['cluster_sizes'] = cluster_sizes\n",
    "            meta_info['cluster_labels'] = cluster_labels\n",
    "            # print(meta_info)\n",
    "\n",
    "            with open(\"%s/%s%s_meta_info.json\"%(similarities_dir, model.model_name, mode), \"w\") as file:\n",
    "                json.dump(meta_info, file)\n",
    "\n",
    "            # compute projections\n",
    "            embedding = np.array(torch.concatenate([image_embedding, text_embedding]))\n",
    "\n",
    "            projection_methods = {\n",
    "                'PCA': PCA,\n",
    "                'UMAP': UMAP,\n",
    "                'TSNE': TSNE\n",
    "            }\n",
    "            for method in projection_methods.keys():\n",
    "                if method == 'PCA':\n",
    "                    proj = projection_methods[method](n_components=2)\n",
    "                else:\n",
    "                    proj = projection_methods[method](n_components=2, metric='cosine', random_state=31415)\n",
    "                \n",
    "                if method == 'TSNE':\n",
    "                    low_dim_data = proj.fit(embedding)\n",
    "                else:\n",
    "                    low_dim_data = proj.fit_transform(embedding)\n",
    "                \n",
    "                projections_df['%s%s_%s_x'%(model.model_name, mode, method)] = low_dim_data[:,0]\n",
    "                projections_df['%s%s_%s_y'%(model.model_name, mode, method)] = low_dim_data[:,1]\n",
    "\n",
    "\n",
    "    projections_df.to_csv(dataset_directory + '/projections.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../../../../Data/'\n",
    "\n",
    "# TODO: update to your dataset\n",
    "dataset_diffusiondb = am_data.DiffusionDB_Dataset(path=\"2m_first_1k\", batch_size=100)\n",
    "diffusiondb_images, diffusiondb_prompts, diffusiondb_dataset_name = get_data_helper(dataset_diffusiondb)\n",
    "# TODO: update to your model\n",
    "export_data(diffusiondb_dataset_name, diffusiondb_images, diffusiondb_prompts, [am_model.CLIPModel()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
