{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/facebookresearch/ImageBind\n",
      "  Cloning https://github.com/facebookresearch/ImageBind to c:\\users\\christina\\appdata\\local\\temp\\pip-req-build-lso60aku\n",
      "  Resolved https://github.com/facebookresearch/ImageBind to commit c6a47d6dc2b53eced51d398c181d57049ca59286\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting pytorchvideo@ git+https://github.com/facebookresearch/pytorchvideo.git@28fe037d212663c6a24f373b94cc5d478c8c1a1d (from imagebind==0.1.0)\n",
      "  Using cached pytorchvideo-0.1.5-py3-none-any.whl\n",
      "Requirement already satisfied: torch==1.13.0 in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from imagebind==0.1.0) (1.13.0)\n",
      "Requirement already satisfied: torchvision==0.14.0 in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from imagebind==0.1.0) (0.14.0)\n",
      "Requirement already satisfied: torchaudio==0.13.0 in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from imagebind==0.1.0) (0.13.0)\n",
      "Requirement already satisfied: timm==0.6.7 in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from imagebind==0.1.0) (0.6.7)\n",
      "Requirement already satisfied: ftfy in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from imagebind==0.1.0) (6.1.1)\n",
      "Requirement already satisfied: regex in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from imagebind==0.1.0) (2023.6.3)\n",
      "Requirement already satisfied: einops in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from imagebind==0.1.0) (0.7.0)\n",
      "Requirement already satisfied: fvcore in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from imagebind==0.1.0) (0.1.5.post20221221)\n",
      "Requirement already satisfied: eva-decord==0.6.1 in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from imagebind==0.1.0) (0.6.1)\n",
      "Requirement already satisfied: iopath in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from imagebind==0.1.0) (0.1.10)\n",
      "Requirement already satisfied: numpy>=1.19 in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from imagebind==0.1.0) (1.23.5)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from imagebind==0.1.0) (3.7.2)\n",
      "Requirement already satisfied: types-regex in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from imagebind==0.1.0) (2023.12.25.20231225)\n",
      "Requirement already satisfied: mayavi in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from imagebind==0.1.0) (4.8.1)\n",
      "Requirement already satisfied: cartopy in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from imagebind==0.1.0) (0.22.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from torch==1.13.0->imagebind==0.1.0) (4.7.1)\n",
      "Requirement already satisfied: requests in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from torchvision==0.14.0->imagebind==0.1.0) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from torchvision==0.14.0->imagebind==0.1.0) (10.0.0)\n",
      "Requirement already satisfied: shapely>=1.7 in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from cartopy->imagebind==0.1.0) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20 in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from cartopy->imagebind==0.1.0) (23.1)\n",
      "Requirement already satisfied: pyshp>=2.1 in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from cartopy->imagebind==0.1.0) (2.3.1)\n",
      "Requirement already satisfied: pyproj>=3.1.0 in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from cartopy->imagebind==0.1.0) (3.6.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from matplotlib->imagebind==0.1.0) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from matplotlib->imagebind==0.1.0) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from matplotlib->imagebind==0.1.0) (4.41.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from matplotlib->imagebind==0.1.0) (1.4.4)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from matplotlib->imagebind==0.1.0) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from matplotlib->imagebind==0.1.0) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from matplotlib->imagebind==0.1.0) (6.0.0)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from ftfy->imagebind==0.1.0) (0.2.6)\n",
      "Requirement already satisfied: yacs>=0.1.6 in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from fvcore->imagebind==0.1.0) (0.1.8)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from fvcore->imagebind==0.1.0) (6.0.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from fvcore->imagebind==0.1.0) (4.65.0)\n",
      "Requirement already satisfied: termcolor>=1.1 in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from fvcore->imagebind==0.1.0) (2.4.0)\n",
      "Requirement already satisfied: tabulate in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from fvcore->imagebind==0.1.0) (0.9.0)\n",
      "Requirement already satisfied: portalocker in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from iopath->imagebind==0.1.0) (2.8.2)\n",
      "Requirement already satisfied: apptools in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from mayavi->imagebind==0.1.0) (5.2.1)\n",
      "Requirement already satisfied: envisage in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from mayavi->imagebind==0.1.0) (7.0.3)\n",
      "Requirement already satisfied: pyface>=6.1.1 in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from mayavi->imagebind==0.1.0) (8.0.0)\n",
      "Requirement already satisfied: pygments in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from mayavi->imagebind==0.1.0) (2.15.1)\n",
      "Requirement already satisfied: traits>=6.0.0 in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from mayavi->imagebind==0.1.0) (6.4.3)\n",
      "Requirement already satisfied: traitsui>=7.0.0 in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from mayavi->imagebind==0.1.0) (8.0.0)\n",
      "Requirement already satisfied: vtk in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from mayavi->imagebind==0.1.0) (9.3.0)\n",
      "Requirement already satisfied: av in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from pytorchvideo@ git+https://github.com/facebookresearch/pytorchvideo.git@28fe037d212663c6a24f373b94cc5d478c8c1a1d->imagebind==0.1.0) (11.0.0)\n",
      "Requirement already satisfied: parameterized in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from pytorchvideo@ git+https://github.com/facebookresearch/pytorchvideo.git@28fe037d212663c6a24f373b94cc5d478c8c1a1d->imagebind==0.1.0) (0.9.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from pytorchvideo@ git+https://github.com/facebookresearch/pytorchvideo.git@28fe037d212663c6a24f373b94cc5d478c8c1a1d->imagebind==0.1.0) (3.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib->imagebind==0.1.0) (3.16.2)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in c:\\users\\christina\\appdata\\roaming\\python\\python39\\site-packages (from pyface>=6.1.1->mayavi->imagebind==0.1.0) (4.12.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from pyproj>=3.1.0->cartopy->imagebind==0.1.0) (2023.5.7)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->imagebind==0.1.0) (1.16.0)\n",
      "Requirement already satisfied: configobj in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from apptools->mayavi->imagebind==0.1.0) (5.0.8)\n",
      "Requirement already satisfied: setuptools in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from envisage->mayavi->imagebind==0.1.0) (67.8.0)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from portalocker->iopath->imagebind==0.1.0) (306)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from requests->torchvision==0.14.0->imagebind==0.1.0) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from requests->torchvision==0.14.0->imagebind==0.1.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from requests->torchvision==0.14.0->imagebind==0.1.0) (2.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from tqdm->fvcore->imagebind==0.1.0) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/ImageBind 'C:\\Users\\Christina\\AppData\\Local\\Temp\\pip-req-build-lso60aku'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: soundfile in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (0.12.1)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\christina\\appdata\\roaming\\python\\python39\\site-packages (from soundfile) (1.15.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\christina\\anaconda3\\envs\\amumo\\lib\\site-packages (from cffi>=1.0->soundfile) (2.21)\n"
     ]
    }
   ],
   "source": [
    "! pip install git+https://github.com/facebookresearch/ImageBind\n",
    "! pip install soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Christina\\anaconda3\\envs\\amumo\\lib\\site-packages\\torchvision\\transforms\\_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Christina\\anaconda3\\envs\\amumo\\lib\\site-packages\\torchvision\\transforms\\_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Christina\\AppData\\Roaming\\Python\\Python39\\site-packages\\umap\\distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "C:\\Users\\Christina\\AppData\\Roaming\\Python\\Python39\\site-packages\\umap\\distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "C:\\Users\\Christina\\AppData\\Roaming\\Python\\Python39\\site-packages\\umap\\distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "C:\\Users\\Christina\\AppData\\Roaming\\Python\\Python39\\site-packages\\umap\\umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "Found cached dataset diffusiondb (C:/Users/Christina/.cache/huggingface/datasets/poloclub___diffusiondb/2m_first_1k/0.9.1/b3bc1e64570dc7149af62c4bac49ecfbce16b683dd4fee083292fae1afa95f7c)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93d3d7a8b05f46d3b7033c0954c9cb9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from amumo import model as am_model\n",
    "from amumo import data as am_data\n",
    "from amumo import utils as am_utils\n",
    "from amumo import widgets as am_widgets\n",
    "\n",
    "# load dataset\n",
    "dataset = am_data.DiffusionDB_Dataset(path=\"2m_first_1k\", batch_size=100) # data helper for the diffusionDB dataset; for the interactive prototype, we only use a random subset of 100 samples\n",
    "all_images, all_prompts = dataset.get_data()\n",
    "cache_name = 'diffusiondb_random_100' # path used to cache the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 244M/244M [01:27<00:00, 2.92MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found cached embeddings for diffusiondb_random_100_ImageBind_huge image\n",
      "found cached embeddings for diffusiondb_random_100_ImageBind_huge text\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "019294e3992a433eb1b83e3af890c8e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CLIPExplorerWidget(children=(VBox(children=(HBox(children=(Dropdown(description='Model: ', options=('ImageBind…"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found cached embeddings for diffusiondb_random_100_CLIP_RN50 image\n",
      "found cached embeddings for diffusiondb_random_100_CLIP_RN50 text\n",
      "found cached embeddings for diffusiondb_random_100_ImageBind_huge image\n",
      "found cached embeddings for diffusiondb_random_100_ImageBind_huge text\n"
     ]
    }
   ],
   "source": [
    "am_widgets.CLIPExplorerWidget(cache_name, all_data={\"image\": all_images, \"text\": all_prompts}, models=[am_model.ImageBind_Model(), \"CLIP\"]) # {\"image\": all_images, \"depth\": all_depths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True [False, False] {'ImageBind': <amumo.model.ImageBind_Model object at 0x000001B878F032E0>, 'CLIP': <amumo.model.CLIPModel object at 0x000001B767795E20>}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\christina\\repositories\\icg\\researchstay\\amumo\\amumo\\widgets.py:940: FutureWarning:\n",
      "\n",
      "The input object of type 'PngImageFile' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'PngImageFile', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "\n",
      "c:\\users\\christina\\repositories\\icg\\researchstay\\amumo\\amumo\\widgets.py:940: VisibleDeprecationWarning:\n",
      "\n",
      "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found cached embeddings for diffusiondb_random_100_ImageBind_huge\n",
      "found cached embeddings for diffusiondb_random_100_CLIP_RN50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e29ae00ffa9f41edaaf7549652f884de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CLIPComparerWidget(children=(HoverWidget(children=(VBox(children=(HTML(value='', layout=Layout(width='300px'))…"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "am_widgets.CLIPComparerWidget(cache_name, all_images=all_images, all_prompts=all_prompts, models=[am_model.ImageBind_Model(), \"CLIP\"]) # {\"image\": all_images, \"depth\": all_depths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Christina\\anaconda3\\envs\\amumo\\lib\\site-packages\\torchvision\\transforms\\_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Christina\\anaconda3\\envs\\amumo\\lib\\site-packages\\torchvision\\transforms\\_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Christina\\AppData\\Roaming\\Python\\Python39\\site-packages\\umap\\distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "C:\\Users\\Christina\\AppData\\Roaming\\Python\\Python39\\site-packages\\umap\\distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "C:\\Users\\Christina\\AppData\\Roaming\\Python\\Python39\\site-packages\\umap\\distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "C:\\Users\\Christina\\AppData\\Roaming\\Python\\Python39\\site-packages\\umap\\umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "from amumo import model as am_model\n",
    "from amumo import data as am_data\n",
    "from amumo import utils as am_utils\n",
    "from amumo import widgets as am_widgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class DepthType(am_data.ImageType):\n",
    "    name = \"Depth\"\n",
    "\n",
    "    def __init__(self, data) -> None:\n",
    "        # data is an array of (1,224,224) tensors\n",
    "        super().__init__(data)\n",
    "\n",
    "    def getVisItem(self, idx):\n",
    "        output_img = io.BytesIO()\n",
    "        plt.imsave(output_img, self.data[idx][0], cmap='gray')\n",
    "        plt.savefig(output_img, format='JPEG')\n",
    "        return output_img\n",
    "    \n",
    "    \n",
    "\n",
    "class ThermalType(am_data.ImageType):\n",
    "    name = \"Thermal\"\n",
    "\n",
    "    def __init__(self, data) -> None:\n",
    "        super().__init__(data)\n",
    "\n",
    "\n",
    "class SUNRGBD_Dataset(am_data.DatasetInterface):\n",
    "    name='SUNRGBD_NYU'\n",
    "\n",
    "    def __init__(self, path, seed=31415, batch_size = 100):\n",
    "        # download \"SUNRGBD_V1\" dataset from https://rgbd.cs.princeton.edu/\n",
    "        super().__init__(path, seed, batch_size)\n",
    "        # path: path to the SUNRGBD dataset\n",
    "        image_paths = glob(path + \"\\\\kv1\\\\NYUdata\\\\NYU*\\\\fullres\\\\*.jpg\", recursive = True)\n",
    "        depth_paths = glob(path + \"\\\\kv1\\\\NYUdata\\\\NYU*\\\\fullres\\\\*.png\", recursive = True)\n",
    "        \n",
    "        batch_idcs = self._get_random_subsample(len(image_paths))\n",
    "        image_paths = np.array(image_paths)[batch_idcs]\n",
    "        depth_paths = np.array(depth_paths)[batch_idcs]\n",
    "        \n",
    "        all_images = []\n",
    "        for image_path in image_paths:\n",
    "            with open(image_path, \"rb\") as fopen:\n",
    "                image = Image.open(fopen).convert(\"RGB\")\n",
    "                all_images.append(image)\n",
    "\n",
    "            \n",
    "        all_depths = []\n",
    "        for depth_path in depth_paths:\n",
    "            with open(depth_path, \"rb\") as fopen:\n",
    "                depth = Image.open(fopen)\n",
    "                depth = np.array(depth, dtype=int)\n",
    "                depth = depth.astype(np.float32) / depth.max() # TODO: need to normalize?\n",
    "                # depth = depth[np.newaxis,:,:] # need 1 channel -> (1,224,224)\n",
    "                depth = torch.from_numpy(depth).unsqueeze(0) # need 1 channel -> (1,224,224)\n",
    "                all_depths.append(depth)\n",
    "\n",
    "        self.MODE2_Type = DepthType\n",
    "\n",
    "        # TODO... load images and depths on demand with a custom loader\n",
    "        self.all_images = np.array(all_images)\n",
    "        self.all_prompts = np.array(all_depths)\n",
    "\n",
    "\n",
    "class LLVIP_Dataset(am_data.DatasetInterface):\n",
    "    name='LLVIP'\n",
    "\n",
    "    def __init__(self, path, seed=31415, batch_size = 100):\n",
    "        # download dataset: https://bupt-ai-cz.github.io/LLVIP/\n",
    "        super().__init__(path, seed, batch_size)\n",
    "        # path: path to the LLVIP dataset\n",
    "        image_paths = glob(path + \"\\\\visible\\\\test\\\\*.jpg\", recursive = True)\n",
    "        thermal_paths = glob(path + \"\\\\infrared\\\\test\\\\*.jpg\", recursive = True)\n",
    "        \n",
    "        batch_idcs = self._get_random_subsample(len(image_paths))\n",
    "        image_paths = np.array(image_paths)[batch_idcs]\n",
    "        thermal_paths = np.array(thermal_paths)[batch_idcs]\n",
    "        \n",
    "        all_images = []\n",
    "        for image_path in image_paths:\n",
    "            with open(image_path, \"rb\") as fopen:\n",
    "                image = Image.open(fopen).convert(\"RGB\")\n",
    "                all_images.append(image)\n",
    "\n",
    "            \n",
    "        all_thermals = []\n",
    "        \n",
    "        for thermal_path in thermal_paths:\n",
    "            with open(thermal_path, \"rb\") as fopen:\n",
    "                thermal = Image.open(fopen).convert(\"L\")\n",
    "                all_thermals.append(thermal)\n",
    "\n",
    "        self.MODE2_Type = ThermalType\n",
    "\n",
    "        # TODO... load images and thermals on demand with a custom loader\n",
    "        self.all_images = np.array(all_images)\n",
    "        self.all_prompts = np.array(all_thermals)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_24244\\3221552142.py:99: FutureWarning: The input object of type 'Image' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Image', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  self.all_images = np.array(all_images)\n",
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_24244\\3221552142.py:99: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.all_images = np.array(all_images)\n",
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_24244\\3221552142.py:100: FutureWarning: The input object of type 'Image' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Image', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  self.all_prompts = np.array(all_thermals)\n",
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_24244\\3221552142.py:100: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.all_prompts = np.array(all_thermals)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100\n",
      "found cached embeddings for test_thermal_ImageBind_huge image\n",
      "batch 1 of 1\n",
      "torch.Size([100, 1, 224, 224])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17508ad4cb6642ec8692300f7a57d876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CLIPExplorerWidget(children=(VBox(children=(HBox(children=(Dropdown(description='Model: ', options=('ImageBind…"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download dataset: https://bupt-ai-cz.github.io/LLVIP/\n",
    "dataset = LLVIP_Dataset(path=\"C:\\\\Users\\\\Christina\\\\Data\\\\imagebind\\\\LLVIP\\\\\", batch_size=100) \n",
    "all_images, all_thermals = dataset.get_data()\n",
    "print(len(all_images), len(all_thermals))\n",
    "\n",
    "am_widgets.CLIPExplorerWidget(\"test_thermal\", all_data={\"image\": all_images, \"thermal\": all_thermals}, models=[am_model.ImageBind_Model()]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_24244\\3221552142.py:64: FutureWarning:\n",
      "\n",
      "The input object of type 'Image' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Image', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "\n",
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_24244\\3221552142.py:64: VisibleDeprecationWarning:\n",
      "\n",
      "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\n",
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_24244\\3221552142.py:65: FutureWarning:\n",
      "\n",
      "The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "\n",
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_24244\\3221552142.py:65: VisibleDeprecationWarning:\n",
      "\n",
      "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100\n",
      "found cached embeddings for test_depth_ImageBind_huge image\n",
      "found cached embeddings for test_depth_ImageBind_huge depth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f38b43e86ba4314a512d7d62837bade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CLIPExplorerWidget(children=(VBox(children=(HBox(children=(Dropdown(description='Model: ', options=('ImageBind…"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download \"SUNRGBD_V1\" dataset from https://rgbd.cs.princeton.edu/\n",
    "dataset = SUNRGBD_Dataset(path=\"C:\\\\Users\\\\Christina\\\\Data\\\\imagebind\\\\SUNRGBD\\\\\", batch_size=100)\n",
    "all_images, all_depths = dataset.get_data()\n",
    "print(len(all_images), len(all_depths))\n",
    "\n",
    "am_widgets.CLIPExplorerWidget(\"test_depth\", all_data={\"image\": all_images, \"depth\": all_depths}, models=[am_model.ImageBind_Model()]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amumo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
