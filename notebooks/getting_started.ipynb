{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install Amumo\n",
    "\n",
    "# need ipykernel > 6\n",
    "# ! pip install ipykernel==6.23.1\n",
    "# ! pip install git+https://github.com/ginihumer/Amumo.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\husse\\anaconda3\\envs\\amumo_env\\lib\\site-packages\\umap\\distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "C:\\Users\\husse\\anaconda3\\envs\\amumo_env\\lib\\site-packages\\umap\\distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "C:\\Users\\husse\\anaconda3\\envs\\amumo_env\\lib\\site-packages\\umap\\distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "C:\\Users\\husse\\anaconda3\\envs\\amumo_env\\lib\\site-packages\\umap\\umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "# import \n",
    "from amumo import model as am_model\n",
    "from amumo import data as am_data\n",
    "from amumo import widgets as am_widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset diffusiondb (C:/Users/husse/.cache/huggingface/datasets/poloclub___diffusiondb/2m_first_1k/0.9.1/b3bc1e64570dc7149af62c4bac49ecfbce16b683dd4fee083292fae1afa95f7c)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8ed8be8b72c4eb4a3260ce5298b42f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load dataset\n",
    "dataset = am_data.DiffusionDB_Dataset(path=\"2m_first_1k\", batch_size=100) # data helper for the diffusionDB dataset; for the interactive prototype, we only use a random subset of 100 samples\n",
    "all_images, all_prompts, dataset_name = dataset.get_data()\n",
    "cache_name = 'diffusiondb_random_100' # path used to cache the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found cached embeddings for diffusiondb_random_100_CLIP_RN50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64ee66b9fef54cfea80bde42cdb3c16c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CLIPExplorerWidget(children=(VBox(children=(HBox(children=(Dropdown(description='Model: ', options=('CLIP', 'Câ€¦"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "# This widget consists of a similarity heatmap that shows the similarities between all latent embeddings, a 2-dimensional projection of the embeddings, and a hover view of the currently hovered instance.\n",
    "# It also allows user input for choosing the model, an option to manually close the modality gap, an option to cluster the matrix rows by their similarity, and settings for the 2-dimensional projection. \n",
    "# To learn more about the proper use for closing the modality gap, see the interactive article: https://jku-vds-lab.at/amumo.\n",
    "# You can specify a list of models to compare; there is a predefined set of models, but you can also pass your own model class.\n",
    "# The widget takes a list of images and a list of prompts of the same size as input. Images and texts with the same index are considered pairs.\n",
    "am_widgets.CLIPExplorerWidget(cache_name, all_images, all_prompts, models=[am_model.CLIPModel(), 'CyCLIP', 'CLOOB_LAION400M']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True [False, False, False] {'CLIP': <amumo.model.CLIPModel object at 0x000002DAD09ECD90>, 'CyCLIP': <amumo.model.CyCLIPModel object at 0x000002DA8AFA1430>, 'CLOOB_LAION400M': <amumo.model.CLOOB_LAION400M_Model object at 0x000002DA8F201EB0>}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (100,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# This widget shows similarity heatmaps for all models in the list for a better comparison between models.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mam_widgets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCLIPComparerWidget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_prompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mam_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCLIPModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCyCLIP\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCLOOB_LAION400M\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \n",
      "File \u001b[1;32m~\\anaconda3\\envs\\amumo_env\\lib\\site-packages\\amumo\\widgets.py:840\u001b[0m, in \u001b[0;36mCLIPComparerWidget.__init__\u001b[1;34m(self, dataset_name, all_images, all_prompts, models, close_modality_gap, zmin, zmax)\u001b[0m\n\u001b[0;32m    836\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(close_modality_gap) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(close_modality_gap) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\n\u001b[0;32m    837\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclose_modality_gap must be a bool or list of the same length as models\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    839\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_name \u001b[38;5;241m=\u001b[39m dataset_name\n\u001b[1;32m--> 840\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_images \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_images\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    841\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_prompts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(all_prompts)\n\u001b[0;32m    842\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_images)\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (100,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "# This widget shows similarity heatmaps for all models in the list for a better comparison between models.\n",
    "am_widgets.CLIPComparerWidget(cache_name, all_images, all_prompts, models=[am_model.CLIPModel(), 'CyCLIP', 'CLOOB_LAION400M']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use a Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "import numpy as np\n",
    "# You can use the CLIPModelInterface to create your own model class wrapper.\n",
    "class CustomModel(am_model.CLIPModelInterface):\n",
    "    available_models = clip.available_models()\n",
    "    model_name = 'MyCLIP'\n",
    "\n",
    "    def __init__(self, name='RN50', device='cpu') -> None:\n",
    "        super().__init__(name, device)\n",
    "        self.model, self.preprocess = clip.load(name, device=device)\n",
    "        self.model.eval()\n",
    "        self.logit_scale = self.model.logit_scale\n",
    "\n",
    "    def encode_image(self, images):\n",
    "        images = [self.preprocess(i) for i in images]\n",
    "        image_input = torch.tensor(np.stack(images)).to(self.device)\n",
    "        return self.model.encode_image(image_input).float().cpu()\n",
    "\n",
    "    def encode_text(self, texts):\n",
    "        text_tokens = clip.tokenize(texts, truncate = True).to(self.device)\n",
    "        return self.model.encode_text(text_tokens).float().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The widget takes any instance of type CLIPModelInterface.\n",
    "am_widgets.CLIPExplorerWidget(cache_name, all_images, all_prompts, models=[CustomModel()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
