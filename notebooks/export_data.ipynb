{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clipexplorer\n",
    "from clipexplorer import data as ce_data\n",
    "from clipexplorer import utils as ce_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Helpers\n",
    "def get_data_helper(dataset, filters=[], method=any):\n",
    "    all_images, all_prompts = dataset.get_filtered_data(filters, method=method)\n",
    "    print(len(all_images))\n",
    "\n",
    "    dataset_name = dataset.name\n",
    "    if len(filters) > 0:\n",
    "        dataset_name = dataset_name + '_filter-' + method.__name__ + '_' + '-'.join(filters)\n",
    "    else:\n",
    "        dataset_name = dataset_name + '_size-%i'%len(all_images)\n",
    "\n",
    "    return all_images, all_prompts, dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def create_dir_if_not_exists(dir):\n",
    "    if not os.path.exists(dir):\n",
    "        os.mkdir(dir)\n",
    "    return dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_directory = './exported_data_checkpoints/'\n",
    "\n",
    "def export_data(dataset_name, images, prompts):\n",
    "\n",
    "    # create folder structure\n",
    "    dataset_directory = create_dir_if_not_exists(export_directory + dataset_name)\n",
    "    images_dir = create_dir_if_not_exists(dataset_directory + '/images')\n",
    "    similarities_dir = create_dir_if_not_exists(dataset_directory + '/similarities')\n",
    "\n",
    "    # save images\n",
    "    for i in range(len(images)):\n",
    "        im = images[i]\n",
    "        im.resize((400,400))\n",
    "        im.save('%s/%i.jpg'%(images_dir,i))\n",
    "\n",
    "    # save texts\n",
    "    with open(dataset_directory + \"/prompts.txt\", \"w\") as file:\n",
    "        for prompt in prompts:\n",
    "            file.write(prompt + \"\\n\")\n",
    "\n",
    "    # export projections and similarities\n",
    "    import torch\n",
    "    from sklearn.decomposition import PCA\n",
    "    from openTSNE import TSNE\n",
    "    from umap import UMAP\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import json\n",
    "\n",
    "    projections_df = pd.DataFrame({'emb_id': list(np.arange(0,len(images),1))+list(np.arange(0,len(prompts),1)), 'data_type':['image']*len(images)+['text']*len(prompts)})\n",
    "\n",
    "\n",
    "    for model in ['CLIP', 'CyCLIP', 'CLOOB', 'CLOOB_LAION400M']:\n",
    "        # compute embeddings\n",
    "        image_embedding_gap, text_embedding_gap, logit_scale = ce_utils.get_embedding(model, dataset_name, images, prompts)\n",
    "        image_embedding_nogap, text_embedding_nogap = ce_utils.get_closed_modality_gap(image_embedding_gap, text_embedding_gap)\n",
    "        \n",
    "        for image_embedding, text_embedding, mode in [(image_embedding_gap, text_embedding_gap, ''), (image_embedding_nogap, text_embedding_nogap, '_nogap')]:\n",
    "            \n",
    "            # compute similarities\n",
    "            similarity_image_text, similarity = ce_utils.get_similarity(image_embedding, text_embedding)\n",
    "            np.savetxt('%s/%s%s.csv'%(similarities_dir,model,mode), similarity, delimiter=',')\n",
    "            \n",
    "            # compute meta information and similarity clustering\n",
    "            meta_info = {}\n",
    "            meta_info['gap_distance'] = float(ce_utils.get_modality_distance(image_embedding, text_embedding))\n",
    "            meta_info['loss'] = float(ce_utils.calculate_val_loss(image_embedding, text_embedding, logit_scale))\n",
    "\n",
    "            idcs, clusters, clusters_unsorted = ce_utils.get_cluster_sorting(similarity_image_text)\n",
    "            cluster_labels = []\n",
    "            cluster_sizes = []\n",
    "            for c in set(clusters):\n",
    "                cluster_size = int(np.count_nonzero(clusters==c))\n",
    "                cluster_label = ce_utils.get_textual_label_for_cluster(np.where(clusters_unsorted==c)[0], prompts)\n",
    "                cluster_labels.append(cluster_label)\n",
    "                cluster_sizes.append(cluster_size)\n",
    "\n",
    "            idcs_reverse = np.argsort(idcs)\n",
    "            meta_info['cluster_sort_idcs'] = idcs.tolist()\n",
    "            meta_info['cluster_sort_idcs_reverse'] = idcs_reverse.tolist()\n",
    "            meta_info['cluster_sizes'] = cluster_sizes\n",
    "            meta_info['cluster_labels'] = cluster_labels\n",
    "            # print(meta_info)\n",
    "\n",
    "            with open(\"%s/%s%s_meta_info.json\"%(similarities_dir, model, mode), \"w\") as file:\n",
    "                json.dump(meta_info, file)\n",
    "\n",
    "            # compute projections\n",
    "            embedding = np.array(torch.concatenate([image_embedding, text_embedding]))\n",
    "\n",
    "            projection_methods = {\n",
    "                'PCA': PCA,\n",
    "                'UMAP': UMAP,\n",
    "                'TSNE': TSNE\n",
    "            }\n",
    "            for method in projection_methods.keys():\n",
    "                if method == 'PCA':\n",
    "                    proj = projection_methods[method](n_components=2)\n",
    "                else:\n",
    "                    proj = projection_methods[method](n_components=2, metric='cosine', random_state=31415)\n",
    "                \n",
    "                if method == 'TSNE':\n",
    "                    low_dim_data = proj.fit(embedding)\n",
    "                else:\n",
    "                    low_dim_data = proj.fit_transform(embedding)\n",
    "                \n",
    "                projections_df['%s%s_%s_x'%(model, mode, method)] = low_dim_data[:,0]\n",
    "                projections_df['%s%s_%s_y'%(model, mode, method)] = low_dim_data[:,1]\n",
    "\n",
    "\n",
    "    projections_df.to_csv(dataset_directory + '/projections.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset diffusiondb (/Users/christina/.cache/huggingface/datasets/poloclub___diffusiondb/2m_first_1k/0.9.1/b3bc1e64570dc7149af62c4bac49ecfbce16b683dd4fee083292fae1afa95f7c)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee7cddffcfb54decbf2c0d2bdc5269d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "# dataset_mscoco_val = ce_data.MSCOCO_Val_Dataset(path='/Users/christina/Data/mscoco/validation/', batch_size=100) # TODO: update to a relative path\n",
    "# mscoco_val_images, mscoco_val_prompts, mscoco_val_dataset_name = get_data_helper(dataset_mscoco_val, filters=[], method=any)\n",
    "# export_data(mscoco_val_dataset_name, mscoco_val_images, mscoco_val_prompts)\n",
    "\n",
    "# subset of diffusionDB data\n",
    "dataset_diffusiondb = ce_data.DiffusionDB_Dataset(path=\"2m_first_1k\", batch_size=100)\n",
    "diffusiondb_images, diffusiondb_prompts, diffusiondb_dataset_name = get_data_helper(dataset_diffusiondb)\n",
    "# export_data(diffusiondb_dataset_name, diffusiondb_images, diffusiondb_prompts)\n",
    "\n",
    "# Analyse filtered subset\n",
    "# dataset_mscoco_val = ce_data.MSCOCO_Val_Dataset(path='/Users/christina/Data/mscoco/validation/', batch_size=100) # TODO: update to a relative path\n",
    "# mscoco_val_images_dogs, mscoco_val_prompts_dogs, mscoco_val_dataset_dogs_name = get_data_helper(dataset_mscoco_val, filters=['dog'], method=any) \n",
    "# export_data(mscoco_val_dataset_dogs_name, mscoco_val_images_dogs, mscoco_val_prompts_dogs)\n",
    "\n",
    "example_image_dir = create_dir_if_not_exists(export_directory + 'example_images/')\n",
    "for img_id in range(10):\n",
    "    thumb = diffusiondb_images[img_id].copy()\n",
    "    thumb.thumbnail((100,100))\n",
    "    thumb.save(example_image_dir+str(img_id)+'.jpg')\n",
    "    \n",
    "    # # Analyse rotated image\n",
    "    # dataset_rotated = ce_data.Rotate_Dataset(diffusiondb_images[img_id], diffusiondb_prompts[img_id], id=img_id)\n",
    "    # rotated_images, rotated_prompts, rotated_dataset_name = get_data_helper(dataset_rotated)\n",
    "    # export_data(rotated_dataset_name, rotated_images, rotated_prompts)\n",
    "\n",
    "    # # Analyze noisy image\n",
    "    # dataset_noise = ce_data.Noise_Dataset(diffusiondb_images[img_id], diffusiondb_prompts[img_id], id=img_id)\n",
    "    # noise_images, noise_prompts, noise_dataset_name = get_data_helper(dataset_noise)\n",
    "    # export_data(noise_dataset_name, noise_images, noise_prompts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.04s)\n",
      "creating index...\n",
      "index created!\n",
      "5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'MSCOCO-Val_size-5000'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# full set of mscoco validation data (5000 samples)\n",
    "dataset_mscoco_val_large = ce_data.MSCOCO_Val_Dataset(path='/Users/christina/Data/mscoco/validation/', batch_size=None) # TODO: update to a relative path\n",
    "mscoco_val_images_large, mscoco_val_prompts_large, mscoco_val_dataset_large_name = get_data_helper(dataset_mscoco_val_large, filters=[], method=any)\n",
    "mscoco_val_dataset_large_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder structure\n",
    "export_directory = './exported_data_checkpoints/'\n",
    "\n",
    "dataset_directory = create_dir_if_not_exists(export_directory + mscoco_val_dataset_large_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found cached embeddings for MSCOCO-Val_size-5000_CLIP_RN50\n",
      "found cached embeddings for MSCOCO-Val_size-5000_CyCLIP_RN50\n"
     ]
    }
   ],
   "source": [
    "# export loss landscape of 5000 sample dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "import json\n",
    "\n",
    "\n",
    "for clip_model in ['CLIP', 'CyCLIP']:\n",
    "\n",
    "    image_embedding, text_embedding, logit_scale = ce_utils.get_embedding(clip_model, mscoco_val_dataset_large_name, mscoco_val_images_large, mscoco_val_prompts_large)\n",
    "\n",
    "    # loss difference\n",
    "    modality_distance = ce_utils.get_modality_distance(image_embedding, text_embedding)\n",
    "    loss = ce_utils.calculate_val_loss(image_embedding, text_embedding, logit_scale.exp())\n",
    "\n",
    "    image_embedding_closed, text_embedding_closed = ce_utils.get_closed_modality_gap(image_embedding, text_embedding)\n",
    "    modified_modality_distance = ce_utils.get_modality_distance(image_embedding_closed, text_embedding_closed)\n",
    "    modified_loss = ce_utils.calculate_val_loss(image_embedding_closed, text_embedding_closed, logit_scale.exp())\n",
    "\n",
    "    loss_landscape = {'original_distance': modality_distance, 'original_loss': loss, 'closed_distance': modified_modality_distance, 'closed_loss': modified_loss, 'loss_difference': modified_loss-loss}\n",
    "    \n",
    "    # compute loss landscape\n",
    "    modality_gap = ce_utils.get_modality_gap_normed(image_embedding, text_embedding)\n",
    "    \n",
    "    distance_lst = []\n",
    "    loss_lst = []\n",
    "    for delta in np.arange(-5.0, 5.0, 0.25): \n",
    "        modified_text_features = ce_utils.l2_norm(text_embedding) + 0.5 * delta * modality_gap\n",
    "        modified_text_features = ce_utils.l2_norm(modified_text_features)\n",
    "\n",
    "        modified_image_features = ce_utils.l2_norm(image_embedding) - 0.5 * delta * modality_gap\n",
    "        modified_image_features = ce_utils.l2_norm(modified_image_features)\n",
    "\n",
    "        avg_val_loss = ce_utils.calculate_val_loss(modified_image_features, modified_text_features, logit_scale = logit_scale.exp())\n",
    "\n",
    "        pca = PCA(n_components=6)\n",
    "        pca.fit(np.concatenate((image_embedding, text_embedding), axis=0))\n",
    "\n",
    "        gap_direction = ce_utils.get_gap_direction(modified_image_features, modified_text_features, pca)\n",
    "\n",
    "        loss_lst.append(avg_val_loss)\n",
    "\n",
    "        # Euclidean distance between mass centers\n",
    "        distance_lst.append(\n",
    "            ce_utils.get_modality_distance(modified_image_features, modified_text_features) * gap_direction\n",
    "        )\n",
    "\n",
    "    loss_landscape['distances'] = distance_lst\n",
    "    loss_landscape['losses'] = loss_lst\n",
    "\n",
    "    with open(\"%s/%s_loss_landscape.json\"%(dataset_directory, clip_model), \"w\") as file:\n",
    "        json.dump(loss_landscape, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
