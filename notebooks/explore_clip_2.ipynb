{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need ipykernel > 6\n",
    "# ! pip install ipykernel==6.23.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clipexplorer\n",
    "from importlib import reload\n",
    "# reload(clipexplorer.widgets)\n",
    "# reload(clipexplorer.model)\n",
    "# reload(clipexplorer.data)\n",
    "# reload(clipexplorer.utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clipexplorer import model\n",
    "from clipexplorer import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = data.MSCOCO_Dataset(path='/Users/christina/Data/mscoco/')\n",
    "dataset = data.DiffusionDB_Dataset(path=\"2m_first_1k\")\n",
    "# dataset = data.MSCOCO_Val_Dataset(path='/Users/christina/Data/mscoco/validation/')\n",
    "\n",
    "# all_images, all_prompts = dataset.get_data()\n",
    "# dataset = data.RandomAugmentation_Dataset(all_images[0], all_prompts[0])\n",
    "# dataset = data.Rotate_Dataset(all_images[0], all_prompts[0])\n",
    "# dataset = data.Noise_Dataset(all_images[0], all_prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_helper(dataset, filters=[], method=any):\n",
    "    all_images, all_prompts = dataset.get_filtered_data(filters, method=method)\n",
    "    print(len(all_images))\n",
    "\n",
    "    dataset_name = dataset.name\n",
    "    if len(filters) > 0:\n",
    "        dataset_name = dataset_name + '_filter-' + method.__name__ + '_' + '-'.join(filters)\n",
    "    else:\n",
    "        dataset_name = dataset_name + '_size-%i'%len(all_images)\n",
    "\n",
    "    return all_images, all_prompts, dataset_name\n",
    "\n",
    "all_images, all_prompts, dataset_name = get_data_helper(dataset, filters=[], method=any) # filters = [\"dog\"], method=all\n",
    "dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clipexplorer.widgets import CLIPExplorerWidget\n",
    "\n",
    "\n",
    "clipexplorer_widget = CLIPExplorerWidget(dataset_name, all_images, all_prompts)\n",
    "clipexplorer_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clipexplorer import model\n",
    "model.available_CLIP_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clipexplorer.widgets import CLIPComparerWidget\n",
    "clip_comparer = CLIPComparerWidget(dataset_name, all_images, all_prompts, models=['CLIP', 'CyCLIP', 'CLOOB', 'CLOOB_LAION400M'])\n",
    "clip_comparer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clipexplorer.widgets import CLIPComparerWidget\n",
    "clip_comparer = CLIPComparerWidget(dataset_name, all_images, all_prompts, models=['CLIP', 'CyCLIP', 'CLOOB', 'CLOOB_LAION400M'], close_modality_gap=True)\n",
    "clip_comparer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot similarities and gap-closed similarities next to each other\n",
    "from clipexplorer.widgets import CLIPComparerWidget\n",
    "clip_comparer = CLIPComparerWidget(dataset_name, all_images, all_prompts, models=['CLIP', 'CLIP', 'CyCLIP', 'CyCLIP', 'CLOOB', 'CLOOB', 'CLOOB_LAION400M', 'CLOOB_LAION400M'], close_modality_gap=[False, True]*4)\n",
    "clip_comparer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate metrics for larger amounts of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clipexplorer import data\n",
    "from clipexplorer.utils import get_embedding, get_modality_distance, calculate_val_loss, get_closed_modality_gap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = data.MSCOCO_Dataset(path='/Users/christina/Data/mscoco/')\n",
    "# dataset = data.DiffusionDB_Dataset(path=\"2m_first_10k\", batch_size=None)\n",
    "\n",
    "all_images, all_prompts, dataset_name = get_data_helper(dataset, filters=[], method=any) # filters = [\"dog\"], method=all\n",
    "dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clip_model in ['CLIP', 'OpenCLIP', 'CyCLIP', 'CLOOB', 'CLOOB_LAION400M']:\n",
    "    print('---model: %s---'%clip_model)\n",
    "    image_embedding, text_embedding, logit_scaling = get_embedding(clip_model, dataset_name, all_images, all_prompts)\n",
    "    modality_distance = get_modality_distance(image_embedding, text_embedding)\n",
    "    val_loss = calculate_val_loss(image_embedding, text_embedding, logit_scaling.exp())\n",
    "    print('modality distance: %.2f | validation loss: %.2f'%(modality_distance, val_loss))\n",
    "\n",
    "    image_embedding, text_embedding = get_closed_modality_gap(image_embedding, text_embedding)\n",
    "    modified_modality_distance = get_modality_distance(image_embedding, text_embedding)\n",
    "    modified_val_loss = calculate_val_loss(image_embedding, text_embedding, logit_scaling.exp())\n",
    "    print('modified modality distance: %.2f | modified validation loss: %.2f'%(modified_modality_distance, modified_val_loss))\n",
    "\n",
    "    print('loss difference:', modified_val_loss-val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.MSCOCO_Dataset(path='/Users/christina/Data/mscoco/', batch_size=10000)\n",
    "# dataset = data.DiffusionDB_Dataset(path=\"2m_first_10k\", batch_size=None)\n",
    "all_images, all_prompts, dataset_name = get_data_helper(dataset, filters=[], method=any) # filters = [\"dog\"], method=all\n",
    "dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clip_model in ['CLIP', 'OpenCLIP', 'CyCLIP', 'CLOOB', 'CLOOB_LAION400M']:\n",
    "    print('---model: %s---'%clip_model)\n",
    "    image_embedding, text_embedding, logit_scaling = get_embedding(clip_model, dataset_name, all_images, all_prompts)\n",
    "    modality_distance = get_modality_distance(image_embedding, text_embedding)\n",
    "    val_loss = calculate_val_loss(image_embedding, text_embedding, logit_scaling.exp())\n",
    "    print('modality distance: %.2f | validation loss: %.2f'%(modality_distance, val_loss))\n",
    "\n",
    "    image_embedding, text_embedding = get_closed_modality_gap(image_embedding, text_embedding)\n",
    "    modified_modality_distance = get_modality_distance(image_embedding, text_embedding)\n",
    "    modified_val_loss = calculate_val_loss(image_embedding, text_embedding, logit_scaling.exp())\n",
    "    print('modified modality distance: %.2f | modified validation loss: %.2f'%(modified_modality_distance, modified_val_loss))\n",
    "\n",
    "    print('loss difference:', modified_val_loss-val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.MSCOCO_Val_Dataset(path='/Users/christina/Data/mscoco/validation/', batch_size=5000)\n",
    "all_images, all_prompts, dataset_name = get_data_helper(dataset, filters=[], method=any) # filters = [\"dog\"], method=all\n",
    "dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clip_model in ['CLIP', 'OpenCLIP', 'CyCLIP', 'CLOOB', 'CLOOB_LAION400M']:\n",
    "    print('---model: %s---'%clip_model)\n",
    "    image_embedding, text_embedding, logit_scaling = get_embedding(clip_model, dataset_name, all_images, all_prompts)\n",
    "    modality_distance = get_modality_distance(image_embedding, text_embedding)\n",
    "    val_loss = calculate_val_loss(image_embedding, text_embedding, logit_scaling.exp())\n",
    "    print('modality distance: %.2f | validation loss: %.2f'%(modality_distance, val_loss))\n",
    "\n",
    "    image_embedding, text_embedding = get_closed_modality_gap(image_embedding, text_embedding)\n",
    "    modified_modality_distance = get_modality_distance(image_embedding, text_embedding)\n",
    "    modified_val_loss = calculate_val_loss(image_embedding, text_embedding, logit_scaling.exp())\n",
    "    print('modified modality distance: %.2f | modified validation loss: %.2f'%(modified_modality_distance, modified_val_loss))\n",
    "\n",
    "    print('loss difference:', modified_val_loss-val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \n",
    "raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n",
    "\n",
    "# conditional image captioning\n",
    "text = \"a photography of\"\n",
    "inputs = processor(raw_image, text, return_tensors=\"pt\")\n",
    "\n",
    "out = model.generate(**inputs)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))\n",
    "\n",
    "# unconditional image captioning\n",
    "inputs = processor(raw_image, return_tensors=\"pt\")\n",
    "\n",
    "out = model.generate(**inputs)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[raw_image, raw_image]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BlipImageProcessor, BlipModel, BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BlipModel.from_pretrained(\"Salesforce/blip-image-captioning-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = BlipImageProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('Salesforce/blip-image-captioning-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = processor(raw_image, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "processor(list(np.array([raw_image, raw_image])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input['pixel_values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(['a photography of a woman and her dog on the beach', \"and her dog on the beach\"], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_image_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_image_features(**input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_text_features(**tokens).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import AutoProcessor, BlipModel\n",
    "\n",
    "model = BlipModel.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "inputs = processor(\n",
    "    text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True\n",
    ")\n",
    "\n",
    "outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.image_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.text_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
