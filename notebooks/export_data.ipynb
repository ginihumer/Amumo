{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clipexplorer\n",
    "from clipexplorer import data as ce_data\n",
    "from clipexplorer import utils as ce_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Helpers\n",
    "def get_data_helper(dataset, filters=[], method=any):\n",
    "    all_images, all_prompts = dataset.get_filtered_data(filters, method=method)\n",
    "    print(len(all_images))\n",
    "\n",
    "    dataset_name = dataset.name\n",
    "    if len(filters) > 0:\n",
    "        dataset_name = dataset_name + '_filter-' + method.__name__ + '_' + '-'.join(filters)\n",
    "    else:\n",
    "        dataset_name = dataset_name + '_size-%i'%len(all_images)\n",
    "\n",
    "    return all_images, all_prompts, dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def create_dir_if_not_exists(dir):\n",
    "    if not os.path.exists(dir):\n",
    "        os.mkdir(dir)\n",
    "    return dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_directory = './exported_data_checkpoints/'\n",
    "\n",
    "def export_data(dataset_name, images, prompts):\n",
    "\n",
    "    # create folder structure\n",
    "    dataset_directory = create_dir_if_not_exists(export_directory + dataset_name)\n",
    "    images_dir = create_dir_if_not_exists(dataset_directory + '/images')\n",
    "    similarities_dir = create_dir_if_not_exists(dataset_directory + '/similarities')\n",
    "\n",
    "    # save images\n",
    "    for i in range(len(images)):\n",
    "        im = images[i]\n",
    "        im.resize((400,400))\n",
    "        im.save('%s/%i.jpg'%(images_dir,i))\n",
    "\n",
    "    # save texts\n",
    "    with open(dataset_directory + \"/prompts.txt\", \"w\") as file:\n",
    "        for prompt in prompts:\n",
    "            file.write(prompt + \"\\n\")\n",
    "\n",
    "    # export projections and similarities\n",
    "    import torch\n",
    "    from sklearn.decomposition import PCA\n",
    "    from openTSNE import TSNE\n",
    "    from umap import UMAP\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import json\n",
    "\n",
    "    projections_df = pd.DataFrame({'emb_id': list(np.arange(0,len(images),1))+list(np.arange(0,len(prompts),1)), 'data_type':['image']*len(images)+['text']*len(prompts)})\n",
    "\n",
    "\n",
    "    for model in ['CLIP', 'CyCLIP', 'CLOOB', 'CLOOB_LAION400M']:\n",
    "        # compute embeddings\n",
    "        image_embedding_gap, text_embedding_gap, logit_scale = ce_utils.get_embedding(model, dataset_name, images, prompts)\n",
    "        image_embedding_nogap, text_embedding_nogap = ce_utils.get_closed_modality_gap(image_embedding_gap, text_embedding_gap)\n",
    "        \n",
    "        for image_embedding, text_embedding, mode in [(image_embedding_gap, text_embedding_gap, ''), (image_embedding_nogap, text_embedding_nogap, '_nogap')]:\n",
    "            \n",
    "            # compute similarities\n",
    "            similarity_image_text, similarity = ce_utils.get_similarity(image_embedding, text_embedding)\n",
    "            np.savetxt('%s/%s%s.csv'%(similarities_dir,model,mode), similarity, delimiter=',')\n",
    "            \n",
    "            # compute meta information and similarity clustering\n",
    "            meta_info = {}\n",
    "            meta_info['gap_distance'] = float(ce_utils.get_modality_distance(image_embedding, text_embedding))\n",
    "            meta_info['loss'] = float(ce_utils.calculate_val_loss(image_embedding, text_embedding, logit_scale.exp()))\n",
    "\n",
    "            idcs, clusters, clusters_unsorted = ce_utils.get_cluster_sorting(similarity_image_text)\n",
    "            cluster_labels = []\n",
    "            cluster_sizes = []\n",
    "            for c in set(clusters):\n",
    "                cluster_size = int(np.count_nonzero(clusters==c))\n",
    "                cluster_label = ce_utils.get_textual_label_for_cluster(np.where(clusters_unsorted==c)[0], prompts)\n",
    "                cluster_labels.append(cluster_label)\n",
    "                cluster_sizes.append(cluster_size)\n",
    "\n",
    "            idcs_reverse = np.argsort(idcs)\n",
    "            meta_info['cluster_sort_idcs'] = idcs.tolist()\n",
    "            meta_info['cluster_sort_idcs_reverse'] = idcs_reverse.tolist()\n",
    "            meta_info['cluster_sizes'] = cluster_sizes\n",
    "            meta_info['cluster_labels'] = cluster_labels\n",
    "            # print(meta_info)\n",
    "\n",
    "            with open(\"%s/%s%s_meta_info.json\"%(similarities_dir, model, mode), \"w\") as file:\n",
    "                json.dump(meta_info, file)\n",
    "\n",
    "            # compute projections\n",
    "            embedding = np.array(torch.concatenate([image_embedding, text_embedding]))\n",
    "\n",
    "            projection_methods = {\n",
    "                'PCA': PCA,\n",
    "                'UMAP': UMAP,\n",
    "                'TSNE': TSNE\n",
    "            }\n",
    "            for method in projection_methods.keys():\n",
    "                if method == 'PCA':\n",
    "                    proj = projection_methods[method](n_components=2)\n",
    "                else:\n",
    "                    proj = projection_methods[method](n_components=2, metric='cosine', random_state=31415)\n",
    "                \n",
    "                if method == 'TSNE':\n",
    "                    low_dim_data = proj.fit(embedding)\n",
    "                else:\n",
    "                    low_dim_data = proj.fit_transform(embedding)\n",
    "                \n",
    "                projections_df['%s%s_%s_x'%(model, mode, method)] = low_dim_data[:,0]\n",
    "                projections_df['%s%s_%s_y'%(model, mode, method)] = low_dim_data[:,1]\n",
    "\n",
    "\n",
    "    projections_df.to_csv(dataset_directory + '/projections.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.07s)\n",
      "creating index...\n",
      "index created!\n",
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christina/anaconda3/envs/myenv3/lib/python3.9/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/Users/christina/anaconda3/envs/myenv3/lib/python3.9/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/Users/christina/anaconda3/envs/myenv3/lib/python3.9/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/Users/christina/anaconda3/envs/myenv3/lib/python3.9/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "2023-06-26 13:51:31.152567: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found cached embeddings for MSCOCO-Val_size-100_CLIP_RN50\n",
      "found cached embeddings for MSCOCO-Val_size-100_CyCLIP_RN50\n",
      "Loading model from /Users/christina/Workspace/CLIP-explorer/clipexplorer/CLOOB_local/training/model_configs/RN50.json\n",
      "found cached embeddings for MSCOCO-Val_size-100_CLOOB_RN50\n",
      "found cached embeddings for MSCOCO-Val_size-100_CLOOB-LAION400M_ViT-B-16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset diffusiondb (/Users/christina/.cache/huggingface/datasets/poloclub___diffusiondb/2m_first_1k/0.9.1/b3bc1e64570dc7149af62c4bac49ecfbce16b683dd4fee083292fae1afa95f7c)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8586197d29d44a2ca3bdb66388c36d55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "found cached embeddings for DiffusionDB_size-100_CLIP_RN50\n",
      "found cached embeddings for DiffusionDB_size-100_CyCLIP_RN50\n",
      "Loading model from /Users/christina/Workspace/CLIP-explorer/clipexplorer/CLOOB_local/training/model_configs/RN50.json\n",
      "found cached embeddings for DiffusionDB_size-100_CLOOB_RN50\n",
      "found cached embeddings for DiffusionDB_size-100_CLOOB-LAION400M_ViT-B-16\n",
      "loading annotations into memory...\n",
      "Done (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "100\n",
      "found cached embeddings for MSCOCO-Val_filter-any_dog_CLIP_RN50\n",
      "found cached embeddings for MSCOCO-Val_filter-any_dog_CyCLIP_RN50\n",
      "Loading model from /Users/christina/Workspace/CLIP-explorer/clipexplorer/CLOOB_local/training/model_configs/RN50.json\n",
      "found cached embeddings for MSCOCO-Val_filter-any_dog_CLOOB_RN50\n",
      "found cached embeddings for MSCOCO-Val_filter-any_dog_CLOOB-LAION400M_ViT-B-16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christina/Workspace/CLIP-explorer/clipexplorer/data.py:208: FutureWarning: The input object of type 'Image' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Image', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  self.all_images = np.array(images)\n",
      "/Users/christina/Workspace/CLIP-explorer/clipexplorer/data.py:208: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.all_images = np.array(images)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "found cached embeddings for Rotated-0_size-100_CLIP_RN50\n",
      "found cached embeddings for Rotated-0_size-100_CyCLIP_RN50\n",
      "Loading model from /Users/christina/Workspace/CLIP-explorer/clipexplorer/CLOOB_local/training/model_configs/RN50.json\n",
      "found cached embeddings for Rotated-0_size-100_CLOOB_RN50\n",
      "found cached embeddings for Rotated-0_size-100_CLOOB-LAION400M_ViT-B-16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christina/Workspace/CLIP-explorer/clipexplorer/data.py:278: FutureWarning: The input object of type 'Image' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Image', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  self.all_images = np.array(images)\n",
      "/Users/christina/Workspace/CLIP-explorer/clipexplorer/data.py:278: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.all_images = np.array(images)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "found cached embeddings for Noisy-0_size-100_CLIP_RN50\n",
      "found cached embeddings for Noisy-0_size-100_CyCLIP_RN50\n",
      "Loading model from /Users/christina/Workspace/CLIP-explorer/clipexplorer/CLOOB_local/training/model_configs/RN50.json\n",
      "found cached embeddings for Noisy-0_size-100_CLOOB_RN50\n",
      "found cached embeddings for Noisy-0_size-100_CLOOB-LAION400M_ViT-B-16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christina/Workspace/CLIP-explorer/clipexplorer/data.py:208: FutureWarning: The input object of type 'Image' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Image', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  self.all_images = np.array(images)\n",
      "/Users/christina/Workspace/CLIP-explorer/clipexplorer/data.py:208: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.all_images = np.array(images)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "found cached embeddings for Rotated-1_size-100_CLIP_RN50\n",
      "found cached embeddings for Rotated-1_size-100_CyCLIP_RN50\n",
      "Loading model from /Users/christina/Workspace/CLIP-explorer/clipexplorer/CLOOB_local/training/model_configs/RN50.json\n",
      "found cached embeddings for Rotated-1_size-100_CLOOB_RN50\n",
      "found cached embeddings for Rotated-1_size-100_CLOOB-LAION400M_ViT-B-16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christina/Workspace/CLIP-explorer/clipexplorer/data.py:278: FutureWarning: The input object of type 'Image' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Image', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  self.all_images = np.array(images)\n",
      "/Users/christina/Workspace/CLIP-explorer/clipexplorer/data.py:278: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.all_images = np.array(images)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "found cached embeddings for Noisy-1_size-100_CLIP_RN50\n",
      "found cached embeddings for Noisy-1_size-100_CyCLIP_RN50\n",
      "Loading model from /Users/christina/Workspace/CLIP-explorer/clipexplorer/CLOOB_local/training/model_configs/RN50.json\n",
      "found cached embeddings for Noisy-1_size-100_CLOOB_RN50\n",
      "found cached embeddings for Noisy-1_size-100_CLOOB-LAION400M_ViT-B-16\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m dataset_noise \u001b[39m=\u001b[39m ce_data\u001b[39m.\u001b[39mNoise_Dataset(diffusiondb_images[img_id], diffusiondb_prompts[img_id], \u001b[39mid\u001b[39m\u001b[39m=\u001b[39mimg_id)\n\u001b[1;32m     29\u001b[0m noise_images, noise_prompts, noise_dataset_name \u001b[39m=\u001b[39m get_data_helper(dataset_noise)\n\u001b[0;32m---> 30\u001b[0m export_data(noise_dataset_name, noise_images, noise_prompts)\n",
      "Cell \u001b[0;32mIn[4], line 85\u001b[0m, in \u001b[0;36mexport_data\u001b[0;34m(dataset_name, images, prompts)\u001b[0m\n\u001b[1;32m     83\u001b[0m     low_dim_data \u001b[39m=\u001b[39m proj\u001b[39m.\u001b[39mfit(embedding)\n\u001b[1;32m     84\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 85\u001b[0m     low_dim_data \u001b[39m=\u001b[39m proj\u001b[39m.\u001b[39;49mfit_transform(embedding)\n\u001b[1;32m     87\u001b[0m projections_df[\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m_x\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%\u001b[39m(model, mode, method)] \u001b[39m=\u001b[39m low_dim_data[:,\u001b[39m0\u001b[39m]\n\u001b[1;32m     88\u001b[0m projections_df[\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m_y\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%\u001b[39m(model, mode, method)] \u001b[39m=\u001b[39m low_dim_data[:,\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv3/lib/python3.9/site-packages/umap/umap_.py:2772\u001b[0m, in \u001b[0;36mUMAP.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   2742\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit_transform\u001b[39m(\u001b[39mself\u001b[39m, X, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   2743\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Fit X into an embedded space and return that transformed\u001b[39;00m\n\u001b[1;32m   2744\u001b[0m \u001b[39m    output.\u001b[39;00m\n\u001b[1;32m   2745\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2770\u001b[0m \u001b[39m        Local radii of data points in the embedding (log-transformed).\u001b[39;00m\n\u001b[1;32m   2771\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2772\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, y)\n\u001b[1;32m   2773\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39membedding\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   2774\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_dens:\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv3/lib/python3.9/site-packages/umap/umap_.py:2684\u001b[0m, in \u001b[0;36mUMAP.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   2681\u001b[0m     \u001b[39mprint\u001b[39m(ts(), \u001b[39m\"\u001b[39m\u001b[39mConstruct embedding\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   2683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39membedding\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m-> 2684\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_, aux_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_embed_data(\n\u001b[1;32m   2685\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raw_data[index],\n\u001b[1;32m   2686\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_epochs,\n\u001b[1;32m   2687\u001b[0m         init,\n\u001b[1;32m   2688\u001b[0m         random_state,  \u001b[39m# JH why raw data?\u001b[39;49;00m\n\u001b[1;32m   2689\u001b[0m     )\n\u001b[1;32m   2690\u001b[0m     \u001b[39m# Assign any points that are fully disconnected from our manifold(s) to have embedding\u001b[39;00m\n\u001b[1;32m   2691\u001b[0m     \u001b[39m# coordinates of np.nan.  These will be filtered by our plotting functions automatically.\u001b[39;00m\n\u001b[1;32m   2692\u001b[0m     \u001b[39m# They also prevent users from being deceived a distance query to one of these points.\u001b[39;00m\n\u001b[1;32m   2693\u001b[0m     \u001b[39m# Might be worth moving this into simplicial_set_embedding or _fit_embed_data\u001b[39;00m\n\u001b[1;32m   2694\u001b[0m     disconnected_vertices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgraph_\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mflatten() \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv3/lib/python3.9/site-packages/umap/umap_.py:2717\u001b[0m, in \u001b[0;36mUMAP._fit_embed_data\u001b[0;34m(self, X, n_epochs, init, random_state)\u001b[0m\n\u001b[1;32m   2713\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fit_embed_data\u001b[39m(\u001b[39mself\u001b[39m, X, n_epochs, init, random_state):\n\u001b[1;32m   2714\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"A method wrapper for simplicial_set_embedding that can be\u001b[39;00m\n\u001b[1;32m   2715\u001b[0m \u001b[39m    replaced by subclasses.\u001b[39;00m\n\u001b[1;32m   2716\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2717\u001b[0m     \u001b[39mreturn\u001b[39;00m simplicial_set_embedding(\n\u001b[1;32m   2718\u001b[0m         X,\n\u001b[1;32m   2719\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgraph_,\n\u001b[1;32m   2720\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_components,\n\u001b[1;32m   2721\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initial_alpha,\n\u001b[1;32m   2722\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_a,\n\u001b[1;32m   2723\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_b,\n\u001b[1;32m   2724\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrepulsion_strength,\n\u001b[1;32m   2725\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnegative_sample_rate,\n\u001b[1;32m   2726\u001b[0m         n_epochs,\n\u001b[1;32m   2727\u001b[0m         init,\n\u001b[1;32m   2728\u001b[0m         random_state,\n\u001b[1;32m   2729\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_input_distance_func,\n\u001b[1;32m   2730\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_metric_kwds,\n\u001b[1;32m   2731\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdensmap,\n\u001b[1;32m   2732\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_densmap_kwds,\n\u001b[1;32m   2733\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_dens,\n\u001b[1;32m   2734\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_output_distance_func,\n\u001b[1;32m   2735\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_output_metric_kwds,\n\u001b[1;32m   2736\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_metric \u001b[39min\u001b[39;49;00m (\u001b[39m\"\u001b[39;49m\u001b[39meuclidean\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39ml2\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   2737\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   2738\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m   2739\u001b[0m         tqdm_kwds\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtqdm_kwds,\n\u001b[1;32m   2740\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv3/lib/python3.9/site-packages/umap/umap_.py:1156\u001b[0m, in \u001b[0;36msimplicial_set_embedding\u001b[0;34m(data, graph, n_components, initial_alpha, a, b, gamma, negative_sample_rate, n_epochs, init, random_state, metric, metric_kwds, densmap, densmap_kwds, output_dens, output_metric, output_metric_kwds, euclidean_output, parallel, verbose, tqdm_kwds)\u001b[0m\n\u001b[1;32m   1149\u001b[0m embedding \u001b[39m=\u001b[39m (\n\u001b[1;32m   1150\u001b[0m     \u001b[39m10.0\u001b[39m\n\u001b[1;32m   1151\u001b[0m     \u001b[39m*\u001b[39m (embedding \u001b[39m-\u001b[39m np\u001b[39m.\u001b[39mmin(embedding, \u001b[39m0\u001b[39m))\n\u001b[1;32m   1152\u001b[0m     \u001b[39m/\u001b[39m (np\u001b[39m.\u001b[39mmax(embedding, \u001b[39m0\u001b[39m) \u001b[39m-\u001b[39m np\u001b[39m.\u001b[39mmin(embedding, \u001b[39m0\u001b[39m))\n\u001b[1;32m   1153\u001b[0m )\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32, order\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mC\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1155\u001b[0m \u001b[39mif\u001b[39;00m euclidean_output:\n\u001b[0;32m-> 1156\u001b[0m     embedding \u001b[39m=\u001b[39m optimize_layout_euclidean(\n\u001b[1;32m   1157\u001b[0m         embedding,\n\u001b[1;32m   1158\u001b[0m         embedding,\n\u001b[1;32m   1159\u001b[0m         head,\n\u001b[1;32m   1160\u001b[0m         tail,\n\u001b[1;32m   1161\u001b[0m         n_epochs,\n\u001b[1;32m   1162\u001b[0m         n_vertices,\n\u001b[1;32m   1163\u001b[0m         epochs_per_sample,\n\u001b[1;32m   1164\u001b[0m         a,\n\u001b[1;32m   1165\u001b[0m         b,\n\u001b[1;32m   1166\u001b[0m         rng_state,\n\u001b[1;32m   1167\u001b[0m         gamma,\n\u001b[1;32m   1168\u001b[0m         initial_alpha,\n\u001b[1;32m   1169\u001b[0m         negative_sample_rate,\n\u001b[1;32m   1170\u001b[0m         parallel\u001b[39m=\u001b[39;49mparallel,\n\u001b[1;32m   1171\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   1172\u001b[0m         densmap\u001b[39m=\u001b[39;49mdensmap,\n\u001b[1;32m   1173\u001b[0m         densmap_kwds\u001b[39m=\u001b[39;49mdensmap_kwds,\n\u001b[1;32m   1174\u001b[0m         tqdm_kwds\u001b[39m=\u001b[39;49mtqdm_kwds,\n\u001b[1;32m   1175\u001b[0m         move_other\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1176\u001b[0m     )\n\u001b[1;32m   1177\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1178\u001b[0m     embedding \u001b[39m=\u001b[39m optimize_layout_generic(\n\u001b[1;32m   1179\u001b[0m         embedding,\n\u001b[1;32m   1180\u001b[0m         embedding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1196\u001b[0m         move_other\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   1197\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv3/lib/python3.9/site-packages/umap/layouts.py:369\u001b[0m, in \u001b[0;36moptimize_layout_euclidean\u001b[0;34m(head_embedding, tail_embedding, head, tail, n_epochs, n_vertices, epochs_per_sample, a, b, rng_state, gamma, initial_alpha, negative_sample_rate, parallel, verbose, densmap, densmap_kwds, tqdm_kwds, move_other)\u001b[0m\n\u001b[1;32m    366\u001b[0m         dens_re_mean \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    367\u001b[0m         dens_re_cov \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 369\u001b[0m     optimize_fn(\n\u001b[1;32m    370\u001b[0m         head_embedding,\n\u001b[1;32m    371\u001b[0m         tail_embedding,\n\u001b[1;32m    372\u001b[0m         head,\n\u001b[1;32m    373\u001b[0m         tail,\n\u001b[1;32m    374\u001b[0m         n_vertices,\n\u001b[1;32m    375\u001b[0m         epochs_per_sample,\n\u001b[1;32m    376\u001b[0m         a,\n\u001b[1;32m    377\u001b[0m         b,\n\u001b[1;32m    378\u001b[0m         rng_state,\n\u001b[1;32m    379\u001b[0m         gamma,\n\u001b[1;32m    380\u001b[0m         dim,\n\u001b[1;32m    381\u001b[0m         move_other,\n\u001b[1;32m    382\u001b[0m         alpha,\n\u001b[1;32m    383\u001b[0m         epochs_per_negative_sample,\n\u001b[1;32m    384\u001b[0m         epoch_of_next_negative_sample,\n\u001b[1;32m    385\u001b[0m         epoch_of_next_sample,\n\u001b[1;32m    386\u001b[0m         n,\n\u001b[1;32m    387\u001b[0m         densmap_flag,\n\u001b[1;32m    388\u001b[0m         dens_phi_sum,\n\u001b[1;32m    389\u001b[0m         dens_re_sum,\n\u001b[1;32m    390\u001b[0m         dens_re_cov,\n\u001b[1;32m    391\u001b[0m         dens_re_std,\n\u001b[1;32m    392\u001b[0m         dens_re_mean,\n\u001b[1;32m    393\u001b[0m         dens_lambda,\n\u001b[1;32m    394\u001b[0m         dens_R,\n\u001b[1;32m    395\u001b[0m         dens_mu,\n\u001b[1;32m    396\u001b[0m         dens_mu_tot,\n\u001b[1;32m    397\u001b[0m     )\n\u001b[1;32m    399\u001b[0m     alpha \u001b[39m=\u001b[39m initial_alpha \u001b[39m*\u001b[39m (\u001b[39m1.0\u001b[39m \u001b[39m-\u001b[39m (\u001b[39mfloat\u001b[39m(n) \u001b[39m/\u001b[39m \u001b[39mfloat\u001b[39m(n_epochs)))\n\u001b[1;32m    401\u001b[0m \u001b[39mreturn\u001b[39;00m head_embedding\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# subset of mscoco val dataset\n",
    "dataset_mscoco_val = ce_data.MSCOCO_Val_Dataset(path='/Users/christina/Data/mscoco/validation/', batch_size=100) # TODO: update to a relative path\n",
    "mscoco_val_images, mscoco_val_prompts, mscoco_val_dataset_name = get_data_helper(dataset_mscoco_val, filters=[], method=any)\n",
    "export_data(mscoco_val_dataset_name, mscoco_val_images, mscoco_val_prompts)\n",
    "\n",
    "# subset of diffusionDB data\n",
    "dataset_diffusiondb = ce_data.DiffusionDB_Dataset(path=\"2m_first_1k\", batch_size=100)\n",
    "diffusiondb_images, diffusiondb_prompts, diffusiondb_dataset_name = get_data_helper(dataset_diffusiondb)\n",
    "export_data(diffusiondb_dataset_name, diffusiondb_images, diffusiondb_prompts)\n",
    "\n",
    "# Analyse filtered subset\n",
    "dataset_mscoco_val = ce_data.MSCOCO_Val_Dataset(path='/Users/christina/Data/mscoco/validation/', batch_size=100) # TODO: update to a relative path\n",
    "mscoco_val_images_dogs, mscoco_val_prompts_dogs, mscoco_val_dataset_dogs_name = get_data_helper(dataset_mscoco_val, filters=['dog'], method=any) \n",
    "export_data(mscoco_val_dataset_dogs_name, mscoco_val_images_dogs, mscoco_val_prompts_dogs)\n",
    "\n",
    "example_image_dir = create_dir_if_not_exists(export_directory + 'example_images/')\n",
    "for img_id in range(10):\n",
    "    # thumb = diffusiondb_images[img_id].copy()\n",
    "    # thumb.thumbnail((100,100))\n",
    "    # thumb.save(example_image_dir+str(img_id)+'.jpg')\n",
    "    \n",
    "    # Analyse rotated image\n",
    "    dataset_rotated = ce_data.Rotate_Dataset(diffusiondb_images[img_id], diffusiondb_prompts[img_id], id=img_id)\n",
    "    rotated_images, rotated_prompts, rotated_dataset_name = get_data_helper(dataset_rotated)\n",
    "    export_data(rotated_dataset_name, rotated_images, rotated_prompts)\n",
    "\n",
    "    # Analyze noisy image\n",
    "    dataset_noise = ce_data.Noise_Dataset(diffusiondb_images[img_id], diffusiondb_prompts[img_id], id=img_id)\n",
    "    noise_images, noise_prompts, noise_dataset_name = get_data_helper(dataset_noise)\n",
    "    export_data(noise_dataset_name, noise_images, noise_prompts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.04s)\n",
      "creating index...\n",
      "index created!\n",
      "5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'MSCOCO-Val_size-5000'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# full set of mscoco validation data (5000 samples)\n",
    "dataset_mscoco_val_large = ce_data.MSCOCO_Val_Dataset(path='/Users/christina/Data/mscoco/validation/', batch_size=None) # TODO: update to a relative path\n",
    "mscoco_val_images_large, mscoco_val_prompts_large, mscoco_val_dataset_large_name = get_data_helper(dataset_mscoco_val_large, filters=[], method=any)\n",
    "mscoco_val_dataset_large_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder structure\n",
    "export_directory = './exported_data_checkpoints/'\n",
    "\n",
    "dataset_directory = create_dir_if_not_exists(export_directory + mscoco_val_dataset_large_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found cached embeddings for MSCOCO-Val_size-5000_CLIP_RN50\n",
      "found cached embeddings for MSCOCO-Val_size-5000_CyCLIP_RN50\n"
     ]
    }
   ],
   "source": [
    "# export loss landscape of 5000 sample dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "import json\n",
    "\n",
    "\n",
    "for clip_model in ['CLIP', 'CyCLIP']:\n",
    "\n",
    "    image_embedding, text_embedding, logit_scale = ce_utils.get_embedding(clip_model, mscoco_val_dataset_large_name, mscoco_val_images_large, mscoco_val_prompts_large)\n",
    "\n",
    "    # loss difference\n",
    "    modality_distance = ce_utils.get_modality_distance(image_embedding, text_embedding)\n",
    "    loss = ce_utils.calculate_val_loss(image_embedding, text_embedding, logit_scale.exp())\n",
    "\n",
    "    image_embedding_closed, text_embedding_closed = ce_utils.get_closed_modality_gap(image_embedding, text_embedding)\n",
    "    modified_modality_distance = ce_utils.get_modality_distance(image_embedding_closed, text_embedding_closed)\n",
    "    modified_loss = ce_utils.calculate_val_loss(image_embedding_closed, text_embedding_closed, logit_scale.exp())\n",
    "\n",
    "    loss_landscape = {'original_distance': modality_distance, 'original_loss': loss, 'closed_distance': modified_modality_distance, 'closed_loss': modified_loss, 'loss_difference': modified_loss-loss}\n",
    "    \n",
    "    # compute loss landscape\n",
    "    modality_gap = ce_utils.get_modality_gap_normed(image_embedding, text_embedding)\n",
    "    \n",
    "    distance_lst = []\n",
    "    loss_lst = []\n",
    "    for delta in np.arange(-5.0, 5.0, 0.25): \n",
    "        modified_text_features = ce_utils.l2_norm(text_embedding) + 0.5 * delta * modality_gap\n",
    "        modified_text_features = ce_utils.l2_norm(modified_text_features)\n",
    "\n",
    "        modified_image_features = ce_utils.l2_norm(image_embedding) - 0.5 * delta * modality_gap\n",
    "        modified_image_features = ce_utils.l2_norm(modified_image_features)\n",
    "\n",
    "        avg_val_loss = ce_utils.calculate_val_loss(modified_image_features, modified_text_features, logit_scale = logit_scale.exp())\n",
    "\n",
    "        pca = PCA(n_components=6)\n",
    "        pca.fit(np.concatenate((image_embedding, text_embedding), axis=0))\n",
    "\n",
    "        gap_direction = ce_utils.get_gap_direction(modified_image_features, modified_text_features, pca)\n",
    "\n",
    "        loss_lst.append(avg_val_loss)\n",
    "\n",
    "        # Euclidean distance between mass centers\n",
    "        distance_lst.append(\n",
    "            ce_utils.get_modality_distance(modified_image_features, modified_text_features) * gap_direction\n",
    "        )\n",
    "\n",
    "    loss_landscape['distances'] = distance_lst\n",
    "    loss_landscape['losses'] = loss_lst\n",
    "\n",
    "    with open(\"%s/%s_loss_landscape.json\"%(dataset_directory, clip_model), \"w\") as file:\n",
    "        json.dump(loss_landscape, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
