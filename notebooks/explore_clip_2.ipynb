{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need ipykernel > 6\n",
    "# ! pip install ipykernel==6.23.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'clipexplorer.utils' from '/Users/christina/Workspace/CLIP-explorer/clipexplorer/utils.py'>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import clipexplorer\n",
    "from importlib import reload\n",
    "reload(clipexplorer.data)\n",
    "reload(clipexplorer.model)\n",
    "reload(clipexplorer.widgets)\n",
    "reload(clipexplorer.utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clipexplorer import model\n",
    "from clipexplorer import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.04s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christina/Workspace/CLIP-explorer/clipexplorer/data.py:95: FutureWarning:\n",
      "\n",
      "The input object of type 'Image' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Image', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "\n",
      "/Users/christina/Workspace/CLIP-explorer/clipexplorer/data.py:95: VisibleDeprecationWarning:\n",
      "\n",
      "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dataset = data.MSCOCO_Dataset(path='/Users/christina/Data/mscoco/')\n",
    "# dataset = data.DiffusionDB_Dataset(path=\"2m_first_1k\")\n",
    "dataset = data.MSCOCO_Val_Dataset(path='/Users/christina/Data/mscoco/validation/')\n",
    "\n",
    "# all_images, all_prompts = dataset.get_data()\n",
    "# dataset = data.RandomAugmentation_Dataset(all_images[0], all_prompts[0])\n",
    "# dataset = data.Rotate_Dataset(all_images[0], all_prompts[0])\n",
    "# dataset = data.Noise_Dataset(all_images[0], all_prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'MSCOCO-Val_size-100'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_data_helper(dataset, filters=[], method=any):\n",
    "    all_images, all_prompts = dataset.get_filtered_data(filters, method=method)\n",
    "    print(len(all_images))\n",
    "\n",
    "    dataset_name = dataset.name\n",
    "    if len(filters) > 0:\n",
    "        dataset_name = dataset_name + '_filter-' + method.__name__ + '_' + '-'.join(filters)\n",
    "    else:\n",
    "        dataset_name = dataset_name + '_size-%i'%len(all_images)\n",
    "\n",
    "    return all_images, all_prompts, dataset_name\n",
    "\n",
    "all_images, all_prompts, dataset_name = get_data_helper(dataset, filters=[], method=any) # filters = [\"dog\"], method=all\n",
    "dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found cached embeddings for MSCOCO-Val_size-100_CLIP_RN50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d17ded41bcb4aa4a12a6af02b38658a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CLIPExplorerWidget(children=(VBox(children=(HBox(children=(Dropdown(description='Model: ', options=('CLIP', 'O…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Salesforce/blip-image-captioning-base were not used when initializing BlipModel: ['text_decoder.bert.encoder.layer.8.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.7.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.0.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.2.attention.self.value.bias', 'text_decoder.bert.encoder.layer.6.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.1.attention.self.key.bias', 'text_decoder.bert.encoder.layer.7.attention.self.key.weight', 'text_decoder.bert.encoder.layer.4.attention.self.query.bias', 'text_decoder.bert.encoder.layer.11.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.1.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.11.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.9.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.5.output.dense.bias', 'text_decoder.bert.encoder.layer.9.attention.self.value.bias', 'text_decoder.bert.encoder.layer.4.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.2.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.7.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.5.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.6.attention.self.key.weight', 'text_decoder.bert.encoder.layer.2.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.4.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.5.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.9.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.6.attention.self.query.weight', 'text_decoder.bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.7.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.9.attention.self.key.weight', 'text_decoder.bert.encoder.layer.11.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.6.attention.self.query.bias', 'text_decoder.bert.encoder.layer.1.attention.self.query.weight', 'text_decoder.bert.encoder.layer.7.output.dense.bias', 'text_decoder.bert.encoder.layer.1.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.5.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.9.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.5.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.6.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.1.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.3.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.2.attention.self.query.bias', 'text_decoder.bert.encoder.layer.10.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.8.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.8.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.3.attention.self.value.bias', 'text_decoder.bert.encoder.layer.2.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.10.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.0.attention.self.query.weight', 'text_decoder.bert.encoder.layer.0.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.3.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.7.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.5.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.7.attention.self.query.weight', 'text_decoder.bert.encoder.layer.5.intermediate.dense.weight', 'text_decoder.bert.embeddings.position_embeddings.weight', 'text_decoder.bert.encoder.layer.2.attention.self.value.weight', 'text_decoder.bert.encoder.layer.3.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.0.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.5.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.3.attention.self.query.bias', 'text_decoder.bert.encoder.layer.10.output.dense.bias', 'text_decoder.bert.encoder.layer.9.attention.self.query.weight', 'text_decoder.bert.encoder.layer.9.attention.self.key.bias', 'text_decoder.bert.encoder.layer.5.attention.self.key.bias', 'text_decoder.bert.encoder.layer.7.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.6.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.10.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.9.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.8.attention.self.query.bias', 'text_decoder.bert.encoder.layer.10.attention.self.query.bias', 'text_decoder.bert.encoder.layer.10.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.0.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.11.attention.self.key.weight', 'text_decoder.bert.encoder.layer.4.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.7.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.2.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.6.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.7.attention.self.key.bias', 'text_decoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.10.attention.self.query.weight', 'text_decoder.bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.8.attention.self.key.bias', 'text_decoder.bert.encoder.layer.8.attention.self.query.weight', 'text_decoder.bert.encoder.layer.8.intermediate.dense.bias', 'text_decoder.cls.predictions.decoder.weight', 'text_decoder.bert.encoder.layer.3.output.dense.bias', 'text_decoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.1.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.11.output.dense.weight', 'text_decoder.bert.encoder.layer.2.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.8.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.6.attention.self.value.weight', 'text_decoder.bert.encoder.layer.10.output.dense.weight', 'text_decoder.bert.encoder.layer.9.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.11.attention.self.query.bias', 'text_decoder.bert.encoder.layer.9.output.dense.bias', 'text_decoder.bert.encoder.layer.6.output.dense.weight', 'text_decoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.4.attention.self.value.weight', 'text_decoder.bert.encoder.layer.1.attention.self.value.bias', 'text_decoder.bert.encoder.layer.0.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.5.attention.self.query.bias', 'text_decoder.bert.encoder.layer.11.output.dense.bias', 'text_decoder.cls.predictions.transform.LayerNorm.bias', 'text_decoder.bert.encoder.layer.8.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.9.crossattention.self.query.bias', 'text_decoder.bert.embeddings.position_ids', 'text_decoder.bert.encoder.layer.1.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.5.attention.self.key.weight', 'text_decoder.bert.encoder.layer.8.output.dense.bias', 'text_decoder.cls.predictions.decoder.bias', 'text_decoder.bert.encoder.layer.3.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.8.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.6.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.11.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.11.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.1.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.3.attention.self.value.weight', 'text_decoder.bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.0.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.7.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.7.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.6.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.2.output.dense.bias', 'text_decoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.4.output.dense.bias', 'text_decoder.bert.encoder.layer.0.attention.self.value.bias', 'text_decoder.bert.encoder.layer.8.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.0.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.6.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.5.attention.self.value.bias', 'text_decoder.bert.encoder.layer.10.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.6.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.3.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.5.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.9.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.6.attention.self.value.bias', 'text_decoder.bert.encoder.layer.10.attention.self.value.bias', 'text_decoder.bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.2.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.8.output.dense.weight', 'text_decoder.bert.encoder.layer.2.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.3.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.7.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.1.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.11.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.4.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.4.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.3.output.dense.weight', 'text_decoder.bert.encoder.layer.4.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.7.attention.self.query.bias', 'text_decoder.bert.encoder.layer.2.attention.self.key.weight', 'text_decoder.bert.encoder.layer.4.attention.self.value.bias', 'text_decoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.2.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.9.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.0.attention.self.value.weight', 'text_decoder.bert.encoder.layer.9.output.dense.weight', 'text_decoder.bert.encoder.layer.5.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.10.attention.output.dense.bias', 'text_decoder.bert.embeddings.LayerNorm.bias', 'text_decoder.bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.0.output.dense.bias', 'text_decoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.5.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.7.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.3.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.3.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.9.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.4.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.0.attention.self.key.weight', 'text_decoder.bert.encoder.layer.4.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.4.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.3.attention.self.key.weight', 'text_decoder.bert.encoder.layer.7.attention.self.value.bias', 'text_decoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.5.attention.self.value.weight', 'text_decoder.bert.encoder.layer.10.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.10.intermediate.dense.bias', 'text_decoder.cls.predictions.bias', 'text_decoder.bert.encoder.layer.4.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.11.attention.self.value.weight', 'text_decoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.6.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.8.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.5.attention.self.query.weight', 'text_decoder.bert.encoder.layer.6.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.0.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.3.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.4.attention.self.key.weight', 'text_decoder.bert.encoder.layer.1.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.1.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.11.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.11.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.8.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.10.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.0.attention.self.query.bias', 'text_decoder.bert.encoder.layer.8.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.7.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.11.attention.self.key.bias', 'text_decoder.cls.predictions.transform.dense.weight', 'text_decoder.bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.11.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.11.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.8.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.0.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.10.attention.self.key.weight', 'text_decoder.bert.encoder.layer.2.attention.self.query.weight', 'text_decoder.bert.encoder.layer.4.attention.self.query.weight', 'text_decoder.bert.encoder.layer.4.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'text_decoder.bert.embeddings.LayerNorm.weight', 'text_decoder.bert.encoder.layer.4.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.9.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.3.attention.self.query.weight', 'text_decoder.bert.encoder.layer.7.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.11.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.9.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.11.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.6.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.8.attention.self.value.bias', 'text_decoder.bert.encoder.layer.1.output.dense.bias', 'text_decoder.bert.encoder.layer.2.attention.self.key.bias', 'text_decoder.bert.encoder.layer.1.attention.self.query.bias', 'text_decoder.bert.encoder.layer.11.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.10.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.10.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.9.attention.self.query.bias', 'text_decoder.bert.encoder.layer.3.attention.self.key.bias', 'text_decoder.bert.encoder.layer.11.attention.self.value.bias', 'text_decoder.bert.encoder.layer.7.attention.self.value.weight', 'text_decoder.bert.encoder.layer.0.output.dense.weight', 'text_decoder.bert.encoder.layer.2.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.5.output.dense.weight', 'text_decoder.bert.encoder.layer.8.attention.self.value.weight', 'text_decoder.bert.encoder.layer.10.crossattention.self.query.bias', 'text_decoder.cls.predictions.transform.LayerNorm.weight', 'text_decoder.bert.encoder.layer.3.attention.output.dense.bias', 'text_decoder.cls.predictions.transform.dense.bias', 'text_decoder.bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.7.output.dense.weight', 'text_decoder.bert.encoder.layer.2.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.3.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.2.output.dense.weight', 'text_decoder.bert.encoder.layer.11.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.6.output.dense.bias', 'text_decoder.bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.8.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.6.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.0.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.4.attention.self.key.bias', 'text_decoder.bert.encoder.layer.5.crossattention.self.query.bias', 'text_decoder.bert.embeddings.word_embeddings.weight', 'text_decoder.bert.encoder.layer.7.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.4.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.2.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.1.attention.self.key.weight', 'text_decoder.bert.encoder.layer.10.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.5.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.1.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.1.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.1.output.dense.weight', 'text_decoder.bert.encoder.layer.0.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.3.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.5.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.2.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.8.attention.self.key.weight', 'text_decoder.bert.encoder.layer.10.attention.self.value.weight', 'text_decoder.bert.encoder.layer.0.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.2.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.2.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.6.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.3.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.1.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.10.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.9.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.8.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.1.attention.self.value.weight', 'text_decoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.1.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.3.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.9.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.11.attention.self.query.weight', 'text_decoder.bert.encoder.layer.4.output.dense.weight', 'text_decoder.bert.encoder.layer.10.attention.self.key.bias', 'text_decoder.bert.encoder.layer.0.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.0.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.5.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.4.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.6.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.9.attention.self.value.weight', 'text_decoder.bert.encoder.layer.1.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.6.attention.self.key.bias', 'text_decoder.bert.encoder.layer.9.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.0.attention.self.key.bias', 'text_decoder.bert.encoder.layer.7.crossattention.output.dense.weight']\n",
      "- This IS expected if you are initializing BlipModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BlipModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BlipModel were not initialized from the model checkpoint at Salesforce/blip-image-captioning-base and are newly initialized: ['text_model.encoder.layer.4.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.3.output.LayerNorm.weight', 'text_model.encoder.layer.7.crossattention.self.key.bias', 'text_model.encoder.layer.11.crossattention.self.value.weight', 'text_model.encoder.layer.3.attention.output.dense.weight', 'text_model.encoder.layer.6.attention.self.key.bias', 'text_model.encoder.layer.8.attention.self.query.weight', 'text_model.encoder.layer.5.attention.output.dense.bias', 'text_model.encoder.layer.2.crossattention.self.value.bias', 'text_model.encoder.layer.4.output.dense.weight', 'text_model.encoder.layer.10.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.8.crossattention.self.query.bias', 'text_model.encoder.layer.7.output.dense.bias', 'text_model.encoder.layer.3.crossattention.output.dense.weight', 'text_model.encoder.layer.5.crossattention.self.key.bias', 'text_model.encoder.layer.9.crossattention.self.query.weight', 'text_model.encoder.layer.9.attention.output.dense.bias', 'text_model.encoder.layer.2.crossattention.self.query.weight', 'text_model.encoder.layer.4.attention.output.dense.weight', 'text_model.encoder.layer.9.output.dense.bias', 'text_model.encoder.layer.2.crossattention.self.value.weight', 'text_model.encoder.layer.11.crossattention.self.key.bias', 'text_model.encoder.layer.9.crossattention.self.value.weight', 'text_model.encoder.layer.7.crossattention.self.value.weight', 'text_model.encoder.layer.1.crossattention.self.key.bias', 'text_model.encoder.layer.0.intermediate.dense.weight', 'text_model.encoder.layer.5.crossattention.self.value.bias', 'text_model.encoder.layer.6.attention.output.LayerNorm.bias', 'text_model.encoder.layer.7.crossattention.self.query.weight', 'text_model.encoder.layer.10.attention.self.query.weight', 'text_model.encoder.layer.0.crossattention.self.value.weight', 'text_model.encoder.layer.5.output.LayerNorm.bias', 'text_model.encoder.layer.8.attention.self.key.bias', 'text_model.encoder.layer.11.output.LayerNorm.bias', 'text_model.encoder.layer.11.attention.self.value.weight', 'text_model.encoder.layer.11.output.LayerNorm.weight', 'text_model.encoder.layer.9.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.10.output.LayerNorm.weight', 'text_model.encoder.layer.0.intermediate.dense.bias', 'text_model.encoder.layer.8.crossattention.output.dense.bias', 'text_model.encoder.layer.7.attention.output.dense.bias', 'text_model.encoder.layer.4.attention.self.key.weight', 'text_model.encoder.layer.5.attention.output.dense.weight', 'text_model.encoder.layer.9.attention.output.dense.weight', 'text_projection.weight', 'text_model.encoder.layer.3.attention.self.key.bias', 'text_model.encoder.layer.1.attention.self.key.weight', 'text_model.encoder.layer.5.attention.self.query.weight', 'text_model.encoder.layer.0.crossattention.self.key.bias', 'text_model.encoder.layer.5.output.dense.weight', 'text_model.embeddings.LayerNorm.weight', 'text_model.encoder.layer.4.crossattention.self.value.bias', 'text_model.encoder.layer.10.attention.output.dense.bias', 'text_model.encoder.layer.4.output.LayerNorm.weight', 'text_model.encoder.layer.8.intermediate.dense.bias', 'text_model.encoder.layer.4.crossattention.output.dense.weight', 'text_model.encoder.layer.11.crossattention.output.dense.weight', 'text_model.encoder.layer.5.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.7.crossattention.output.dense.bias', 'text_model.encoder.layer.1.attention.output.LayerNorm.bias', 'text_model.encoder.layer.8.attention.output.dense.bias', 'text_model.encoder.layer.10.crossattention.self.query.weight', 'text_model.encoder.layer.1.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.8.output.dense.bias', 'text_model.encoder.layer.6.crossattention.self.value.weight', 'text_model.encoder.layer.2.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.5.output.LayerNorm.weight', 'text_model.encoder.layer.11.attention.output.dense.bias', 'text_model.encoder.layer.3.attention.self.key.weight', 'text_model.encoder.layer.0.output.LayerNorm.weight', 'text_model.encoder.layer.1.crossattention.self.query.bias', 'text_model.encoder.layer.6.attention.output.dense.weight', 'text_model.encoder.layer.3.crossattention.output.dense.bias', 'text_model.encoder.layer.9.attention.self.query.bias', 'text_model.encoder.layer.3.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.1.crossattention.self.value.bias', 'text_model.encoder.layer.9.output.LayerNorm.weight', 'text_model.encoder.layer.1.intermediate.dense.weight', 'text_model.encoder.layer.11.crossattention.output.dense.bias', 'text_model.encoder.layer.8.output.LayerNorm.weight', 'text_model.encoder.layer.8.crossattention.self.key.weight', 'text_model.encoder.layer.0.crossattention.self.value.bias', 'text_model.encoder.layer.5.attention.self.query.bias', 'text_model.encoder.layer.2.output.LayerNorm.weight', 'text_model.encoder.layer.6.crossattention.self.key.bias', 'text_model.encoder.layer.1.attention.output.LayerNorm.weight', 'text_model.embeddings.LayerNorm.bias', 'text_model.encoder.layer.11.attention.self.query.weight', 'text_model.encoder.layer.4.crossattention.self.key.bias', 'text_model.encoder.layer.10.crossattention.output.dense.bias', 'text_model.encoder.layer.11.intermediate.dense.weight', 'text_model.encoder.layer.9.output.dense.weight', 'text_model.encoder.layer.6.crossattention.output.dense.weight', 'text_model.encoder.layer.5.crossattention.output.dense.bias', 'text_model.encoder.layer.3.attention.self.query.bias', 'text_model.encoder.layer.6.output.LayerNorm.weight', 'text_model.encoder.layer.2.attention.output.LayerNorm.bias', 'text_model.encoder.layer.10.attention.self.query.bias', 'text_model.encoder.layer.8.attention.self.query.bias', 'text_model.encoder.layer.1.output.dense.bias', 'text_model.encoder.layer.9.crossattention.output.dense.weight', 'text_model.encoder.layer.3.crossattention.self.key.weight', 'text_model.encoder.layer.2.attention.output.LayerNorm.weight', 'text_model.encoder.layer.4.intermediate.dense.weight', 'text_model.encoder.layer.5.attention.output.LayerNorm.weight', 'text_model.encoder.layer.7.output.dense.weight', 'text_model.encoder.layer.9.attention.self.key.bias', 'text_model.encoder.layer.3.attention.self.value.weight', 'text_model.encoder.layer.10.output.dense.weight', 'text_model.encoder.layer.3.output.LayerNorm.bias', 'text_model.encoder.layer.9.attention.output.LayerNorm.weight', 'text_model.encoder.layer.7.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.0.crossattention.self.key.weight', 'text_model.encoder.layer.4.crossattention.self.value.weight', 'text_model.encoder.layer.8.crossattention.output.dense.weight', 'text_model.encoder.layer.6.crossattention.output.dense.bias', 'text_model.encoder.layer.8.output.dense.weight', 'text_model.encoder.layer.9.attention.self.value.weight', 'text_model.encoder.layer.2.intermediate.dense.bias', 'text_model.encoder.layer.3.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.8.crossattention.self.value.weight', 'text_model.encoder.layer.3.attention.output.dense.bias', 'text_model.encoder.layer.7.intermediate.dense.weight', 'text_model.encoder.layer.7.attention.output.LayerNorm.bias', 'text_model.encoder.layer.0.crossattention.output.dense.weight', 'text_model.encoder.layer.6.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.5.crossattention.self.query.weight', 'text_model.encoder.layer.3.crossattention.self.query.weight', 'text_model.encoder.layer.10.attention.output.LayerNorm.weight', 'text_model.encoder.layer.5.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.4.crossattention.output.dense.bias', 'text_model.encoder.layer.9.intermediate.dense.weight', 'text_model.encoder.layer.11.attention.self.query.bias', 'text_model.encoder.layer.7.crossattention.self.query.bias', 'text_model.encoder.layer.10.attention.self.value.weight', 'text_model.encoder.layer.2.crossattention.output.dense.weight', 'text_model.encoder.layer.11.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.11.crossattention.self.query.bias', 'text_model.pooler.dense.weight', 'text_model.encoder.layer.11.crossattention.self.value.bias', 'text_model.encoder.layer.6.output.dense.weight', 'text_model.encoder.layer.1.attention.self.query.weight', 'text_model.encoder.layer.11.attention.self.key.bias', 'text_model.encoder.layer.9.crossattention.self.value.bias', 'text_model.encoder.layer.3.attention.output.LayerNorm.weight', 'text_model.encoder.layer.4.crossattention.self.query.weight', 'text_model.encoder.layer.4.crossattention.self.key.weight', 'text_model.encoder.layer.5.crossattention.output.dense.weight', 'text_model.encoder.layer.3.output.dense.weight', 'text_model.encoder.layer.6.crossattention.self.key.weight', 'text_model.encoder.layer.2.attention.self.query.bias', 'text_model.encoder.layer.7.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.0.attention.self.key.weight', 'text_model.encoder.layer.2.crossattention.output.dense.bias', 'text_model.encoder.layer.1.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.7.crossattention.output.dense.weight', 'text_model.encoder.layer.10.intermediate.dense.bias', 'text_model.encoder.layer.0.crossattention.self.query.weight', 'text_model.encoder.layer.0.attention.self.query.bias', 'text_model.encoder.layer.9.crossattention.self.query.bias', 'text_model.encoder.layer.5.intermediate.dense.weight', 'text_model.pooler.dense.bias', 'text_model.encoder.layer.1.crossattention.output.dense.bias', 'text_model.encoder.layer.2.attention.self.value.weight', 'text_model.encoder.layer.3.crossattention.self.key.bias', 'text_model.encoder.layer.8.output.LayerNorm.bias', 'text_model.embeddings.word_embeddings.weight', 'text_model.encoder.layer.5.intermediate.dense.bias', 'text_model.encoder.layer.0.attention.self.query.weight', 'text_model.encoder.layer.1.crossattention.self.query.weight', 'text_model.encoder.layer.9.crossattention.self.key.bias', 'text_model.encoder.layer.2.crossattention.self.key.bias', 'text_model.encoder.layer.9.attention.output.LayerNorm.bias', 'text_model.encoder.layer.6.attention.self.query.bias', 'text_model.encoder.layer.11.output.dense.bias', 'text_model.encoder.layer.4.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.0.attention.output.LayerNorm.weight', 'logit_scale', 'text_model.encoder.layer.1.output.dense.weight', 'text_model.encoder.layer.6.intermediate.dense.bias', 'text_model.encoder.layer.5.output.dense.bias', 'text_model.encoder.layer.6.attention.self.key.weight', 'text_model.encoder.layer.2.intermediate.dense.weight', 'text_model.encoder.layer.6.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.1.attention.self.value.bias', 'text_model.encoder.layer.0.crossattention.self.query.bias', 'text_model.encoder.layer.5.attention.self.key.weight', 'text_model.encoder.layer.8.attention.self.value.bias', 'text_model.encoder.layer.8.crossattention.self.value.bias', 'visual_projection.weight', 'text_model.encoder.layer.2.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.4.crossattention.self.query.bias', 'text_model.encoder.layer.6.crossattention.self.query.bias', 'text_model.encoder.layer.7.crossattention.self.key.weight', 'text_model.encoder.layer.8.attention.output.dense.weight', 'text_model.encoder.layer.7.attention.output.dense.weight', 'text_model.encoder.layer.11.crossattention.self.query.weight', 'text_model.encoder.layer.4.attention.output.dense.bias', 'text_model.encoder.layer.2.attention.output.dense.bias', 'text_model.encoder.layer.1.intermediate.dense.bias', 'text_model.encoder.layer.2.attention.output.dense.weight', 'text_model.encoder.layer.4.attention.output.LayerNorm.bias', 'text_model.encoder.layer.3.crossattention.self.query.bias', 'text_model.encoder.layer.9.crossattention.output.dense.bias', 'text_model.encoder.layer.2.output.LayerNorm.bias', 'text_model.encoder.layer.3.attention.output.LayerNorm.bias', 'text_model.encoder.layer.6.attention.output.dense.bias', 'text_model.encoder.layer.7.attention.output.LayerNorm.weight', 'text_model.encoder.layer.11.attention.output.LayerNorm.weight', 'text_model.encoder.layer.6.attention.output.LayerNorm.weight', 'text_model.encoder.layer.0.attention.output.dense.bias', 'text_model.encoder.layer.9.crossattention.self.key.weight', 'text_model.encoder.layer.10.attention.output.LayerNorm.bias', 'text_model.encoder.layer.4.attention.output.LayerNorm.weight', 'text_model.encoder.layer.7.attention.self.value.weight', 'text_model.encoder.layer.10.crossattention.output.dense.weight', 'text_model.encoder.layer.5.crossattention.self.key.weight', 'text_model.encoder.layer.2.crossattention.self.query.bias', 'text_model.encoder.layer.8.attention.output.LayerNorm.weight', 'text_model.encoder.layer.8.attention.output.LayerNorm.bias', 'text_model.encoder.layer.8.intermediate.dense.weight', 'text_model.encoder.layer.5.crossattention.self.query.bias', 'text_model.encoder.layer.5.crossattention.self.value.weight', 'text_model.encoder.layer.0.output.LayerNorm.bias', 'text_model.encoder.layer.3.crossattention.self.value.bias', 'text_model.encoder.layer.0.attention.output.dense.weight', 'text_model.encoder.layer.0.output.dense.bias', 'text_model.encoder.layer.7.attention.self.key.weight', 'text_model.encoder.layer.4.attention.self.query.bias', 'text_model.encoder.layer.0.crossattention.output.dense.bias', 'text_model.encoder.layer.10.attention.self.value.bias', 'text_model.encoder.layer.1.output.LayerNorm.weight', 'text_model.encoder.layer.2.attention.self.key.weight', 'text_model.encoder.layer.1.attention.output.dense.bias', 'text_model.encoder.layer.6.intermediate.dense.weight', 'text_model.encoder.layer.1.attention.self.key.bias', 'text_model.encoder.layer.4.intermediate.dense.bias', 'text_model.encoder.layer.0.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.6.crossattention.self.query.weight', 'text_model.encoder.layer.0.output.dense.weight', 'text_model.encoder.layer.9.attention.self.query.weight', 'text_model.encoder.layer.11.attention.self.key.weight', 'text_model.encoder.layer.2.attention.self.key.bias', 'text_model.encoder.layer.6.attention.self.query.weight', 'text_model.encoder.layer.5.attention.output.LayerNorm.bias', 'text_model.encoder.layer.0.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.10.attention.output.dense.weight', 'text_model.encoder.layer.8.crossattention.self.key.bias', 'text_model.encoder.layer.6.output.LayerNorm.bias', 'text_model.encoder.layer.8.attention.self.value.weight', 'text_model.encoder.layer.2.attention.self.query.weight', 'text_model.encoder.layer.7.attention.self.query.bias', 'text_model.encoder.layer.10.attention.self.key.weight', 'text_model.encoder.layer.0.attention.self.value.weight', 'text_model.encoder.layer.8.crossattention.self.query.weight', 'text_model.encoder.layer.1.output.LayerNorm.bias', 'text_model.encoder.layer.7.attention.self.key.bias', 'text_model.encoder.layer.6.crossattention.self.value.bias', 'text_model.encoder.layer.10.crossattention.self.key.weight', 'text_model.encoder.layer.3.attention.self.query.weight', 'text_model.encoder.layer.3.intermediate.dense.weight', 'text_model.encoder.layer.3.intermediate.dense.bias', 'text_model.encoder.layer.3.attention.self.value.bias', 'text_model.encoder.layer.4.output.LayerNorm.bias', 'text_model.encoder.layer.5.attention.self.value.weight', 'text_model.encoder.layer.8.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.9.intermediate.dense.bias', 'text_model.embeddings.position_embeddings.weight', 'text_model.encoder.layer.3.output.dense.bias', 'text_model.encoder.layer.10.intermediate.dense.weight', 'text_model.encoder.layer.1.attention.self.value.weight', 'text_model.encoder.layer.11.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.11.attention.output.LayerNorm.bias', 'text_model.encoder.layer.6.attention.self.value.bias', 'text_model.encoder.layer.7.crossattention.self.value.bias', 'text_model.encoder.layer.8.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.11.output.dense.weight', 'text_model.encoder.layer.9.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.0.attention.self.key.bias', 'text_model.encoder.layer.3.crossattention.self.value.weight', 'text_model.encoder.layer.7.output.LayerNorm.weight', 'text_model.encoder.layer.6.output.dense.bias', 'text_model.encoder.layer.10.crossattention.self.value.bias', 'text_model.encoder.layer.9.attention.self.key.weight', 'text_model.encoder.layer.1.crossattention.self.value.weight', 'text_model.encoder.layer.8.attention.self.key.weight', 'text_model.encoder.layer.1.crossattention.output.dense.weight', 'text_model.encoder.layer.4.attention.self.value.bias', 'text_model.encoder.layer.0.attention.self.value.bias', 'text_model.encoder.layer.10.crossattention.self.query.bias', 'text_model.encoder.layer.10.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.11.attention.self.value.bias', 'text_model.encoder.layer.1.crossattention.self.key.weight', 'text_model.encoder.layer.4.output.dense.bias', 'text_model.encoder.layer.4.attention.self.query.weight', 'text_model.encoder.layer.10.attention.self.key.bias', 'text_model.encoder.layer.2.crossattention.self.key.weight', 'text_model.encoder.layer.10.crossattention.self.key.bias', 'text_model.encoder.layer.5.attention.self.key.bias', 'text_model.encoder.layer.4.attention.self.key.bias', 'text_model.encoder.layer.7.attention.self.value.bias', 'text_model.encoder.layer.11.intermediate.dense.bias', 'text_model.encoder.layer.7.intermediate.dense.bias', 'text_model.encoder.layer.7.attention.self.query.weight', 'text_model.encoder.layer.5.attention.self.value.bias', 'text_model.encoder.layer.10.output.dense.bias', 'text_model.encoder.layer.10.crossattention.self.value.weight', 'text_model.encoder.layer.2.output.dense.weight', 'text_model.encoder.layer.1.attention.output.dense.weight', 'text_model.encoder.layer.11.attention.output.dense.weight', 'text_model.encoder.layer.6.attention.self.value.weight', 'text_model.encoder.layer.11.crossattention.self.key.weight', 'text_model.encoder.layer.7.output.LayerNorm.bias', 'text_model.encoder.layer.10.output.LayerNorm.bias', 'text_model.encoder.layer.1.attention.self.query.bias', 'text_model.encoder.layer.0.attention.output.LayerNorm.bias', 'text_model.encoder.layer.4.attention.self.value.weight', 'text_model.encoder.layer.9.output.LayerNorm.bias', 'text_model.encoder.layer.2.attention.self.value.bias', 'text_model.encoder.layer.9.attention.self.value.bias', 'text_model.encoder.layer.2.output.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found cached embeddings for MSCOCO-Val_size-100_BLIP_Vit-B\n"
     ]
    }
   ],
   "source": [
    "from clipexplorer.widgets import CLIPExplorerWidget\n",
    "\n",
    "\n",
    "clipexplorer_widget = CLIPExplorerWidget(dataset_name, all_images, all_prompts)\n",
    "clipexplorer_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CLIP': clipexplorer.model.CLIPModel,\n",
       " 'OpenCLIP': clipexplorer.model.OpenCLIPModel,\n",
       " 'BLIP': clipexplorer.model.BLIPModel,\n",
       " 'CyCLIP': clipexplorer.model.CyCLIPModel,\n",
       " 'CLOOB': clipexplorer.model.CLOOB_Model,\n",
       " 'CLOOB_LAION400M': clipexplorer.model.CLOOB_LAION400M_Model}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from clipexplorer import model\n",
    "model.available_CLIP_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christina/Workspace/CLIP-explorer/clipexplorer/widgets.py:500: FutureWarning:\n",
      "\n",
      "The input object of type 'PngImageFile' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'PngImageFile', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "\n",
      "/Users/christina/Workspace/CLIP-explorer/clipexplorer/widgets.py:500: VisibleDeprecationWarning:\n",
      "\n",
      "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found cached embeddings for DiffusionDB_size-100_CLIP_RN50\n",
      "found cached embeddings for DiffusionDB_size-100_CyCLIP_RN50\n",
      "Loading model from /Users/christina/Workspace/CLIP-explorer/clipexplorer/CLOOB_local/training/model_configs/RN50.json\n",
      "found cached embeddings for DiffusionDB_size-100_CLOOB_RN50\n",
      "found cached embeddings for DiffusionDB_size-100_CLOOB-LAION400M_ViT-B-16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "702bd3d459cd40ebbb58bfdbbcf74e65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CLIPComparerWidget(children=(HoverWidget(children=(VBox(children=(HTML(value='', layout=Layout(width='300px'))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from clipexplorer.widgets import CLIPComparerWidget\n",
    "clip_comparer = CLIPComparerWidget(dataset_name, all_images, all_prompts, models=['CLIP', 'CyCLIP', 'CLOOB', 'CLOOB_LAION400M'])\n",
    "clip_comparer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found cached embeddings for DiffusionDB_size-100_CLIP_RN50\n",
      "found cached embeddings for DiffusionDB_size-100_CyCLIP_RN50\n",
      "Loading model from /Users/christina/Workspace/CLIP-explorer/clipexplorer/CLOOB_local/training/model_configs/RN50.json\n",
      "found cached embeddings for DiffusionDB_size-100_CLOOB_RN50\n",
      "found cached embeddings for DiffusionDB_size-100_CLOOB-LAION400M_ViT-B-16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b03ff848e7504d0eac4d563420b00148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CLIPComparerWidget(children=(HoverWidget(children=(VBox(children=(HTML(value='', layout=Layout(width='300px'))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from clipexplorer.widgets import CLIPComparerWidget\n",
    "clip_comparer = CLIPComparerWidget(dataset_name, all_images, all_prompts, models=['CLIP', 'CyCLIP', 'CLOOB', 'CLOOB_LAION400M'], close_modality_gap=True)\n",
    "clip_comparer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found cached embeddings for DiffusionDB_size-100_CLIP_RN50\n",
      "found cached embeddings for DiffusionDB_size-100_CLIP_RN50\n",
      "found cached embeddings for DiffusionDB_size-100_CyCLIP_RN50\n",
      "found cached embeddings for DiffusionDB_size-100_CyCLIP_RN50\n",
      "Loading model from /Users/christina/Workspace/CLIP-explorer/clipexplorer/CLOOB_local/training/model_configs/RN50.json\n",
      "found cached embeddings for DiffusionDB_size-100_CLOOB_RN50\n",
      "Loading model from /Users/christina/Workspace/CLIP-explorer/clipexplorer/CLOOB_local/training/model_configs/RN50.json\n",
      "found cached embeddings for DiffusionDB_size-100_CLOOB_RN50\n",
      "found cached embeddings for DiffusionDB_size-100_CLOOB-LAION400M_ViT-B-16\n",
      "found cached embeddings for DiffusionDB_size-100_CLOOB-LAION400M_ViT-B-16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93662da34364445c9623e5bd1386ac76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CLIPComparerWidget(children=(HoverWidget(children=(VBox(children=(HTML(value='', layout=Layout(width='300px'))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot similarities and gap-closed similarities next to each other\n",
    "from clipexplorer.widgets import CLIPComparerWidget\n",
    "clip_comparer = CLIPComparerWidget(dataset_name, all_images, all_prompts, models=['CLIP', 'CLIP', 'CyCLIP', 'CyCLIP', 'CLOOB', 'CLOOB', 'CLOOB_LAION400M', 'CLOOB_LAION400M'], close_modality_gap=[False, True]*4)\n",
    "clip_comparer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate metrics for larger amounts of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clipexplorer import data\n",
    "from clipexplorer.utils import get_embedding, get_modality_distance, calculate_val_loss, get_closed_modality_gap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset diffusiondb (/Users/christina/.cache/huggingface/datasets/poloclub___diffusiondb/2m_first_10k/0.9.1/b3bc1e64570dc7149af62c4bac49ecfbce16b683dd4fee083292fae1afa95f7c)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8954dcf3ff6343f0ab0a17774160c91e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dataset = data.MSCOCO_Dataset(path='/Users/christina/Data/mscoco/')\n",
    "# dataset = data.DiffusionDB_Dataset(path=\"2m_first_10k\", batch_size=None)\n",
    "\n",
    "all_images, all_prompts, dataset_name = get_data_helper(dataset, filters=[], method=any) # filters = [\"dog\"], method=all\n",
    "dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---model: CLIP\n",
      "found cached embeddings for DiffusionDB_size-10000_CLIP_RN50\n",
      "modality distance: 0.74 | validation loss: 1.00\n",
      "modified modality distance: 0.07 | modified validation loss: 1.69\n",
      "loss difference: 0.6850291558443011\n",
      "---model: OpenCLIP\n",
      "found cached embeddings for DiffusionDB_size-10000_OpenCLIP_RN50\n",
      "modality distance: 0.74 | validation loss: 1.00\n",
      "modified modality distance: 0.07 | modified validation loss: 1.69\n",
      "loss difference: 0.6850291577559193\n",
      "---model: CyCLIP\n",
      "found cached embeddings for DiffusionDB_size-10000_CyCLIP_RN50\n",
      "modality distance: 0.87 | validation loss: 2.88\n",
      "modified modality distance: 0.00 | modified validation loss: 3.03\n",
      "loss difference: 0.14437451954356906\n",
      "---model: CLOOB\n",
      "Loading model from /Users/christina/Workspace/CLIP-explorer/clipexplorer/CLOOB_local/training/model_configs/RN50.json\n",
      "found cached embeddings for DiffusionDB_size-10000_CLOOB_RN50\n",
      "modality distance: 0.34 | validation loss: 2.36\n",
      "modified modality distance: 0.00 | modified validation loss: 2.37\n",
      "loss difference: 0.013824568450892638\n",
      "---model: CLOOB_LAION400M\n",
      "found cached embeddings for DiffusionDB_size-10000_CLOOB-LAION400M_ViT-B-16\n",
      "modality distance: 0.28 | validation loss: 1.08\n",
      "modified modality distance: 0.00 | modified validation loss: 1.09\n",
      "loss difference: 0.01249993286321871\n"
     ]
    }
   ],
   "source": [
    "for clip_model in ['CLIP', 'OpenCLIP', 'CyCLIP', 'CLOOB', 'CLOOB_LAION400M']:\n",
    "    print('---model: %s---'%clip_model)\n",
    "    image_embedding, text_embedding, logit_scaling = get_embedding(clip_model, dataset_name, all_images, all_prompts)\n",
    "    modality_distance = get_modality_distance(image_embedding, text_embedding)\n",
    "    val_loss = calculate_val_loss(image_embedding, text_embedding, logit_scaling.exp())\n",
    "    print('modality distance: %.2f | validation loss: %.2f'%(modality_distance, val_loss))\n",
    "\n",
    "    image_embedding, text_embedding = get_closed_modality_gap(image_embedding, text_embedding)\n",
    "    modified_modality_distance = get_modality_distance(image_embedding, text_embedding)\n",
    "    modified_val_loss = calculate_val_loss(image_embedding, text_embedding, logit_scaling.exp())\n",
    "    print('modified modality distance: %.2f | modified validation loss: %.2f'%(modified_modality_distance, modified_val_loss))\n",
    "\n",
    "    print('loss difference:', modified_val_loss-val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christina/Workspace/CLIP-explorer/clipexplorer/data.py:68: FutureWarning:\n",
      "\n",
      "The input object of type 'Image' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Image', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christina/Workspace/CLIP-explorer/clipexplorer/data.py:68: VisibleDeprecationWarning:\n",
      "\n",
      "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'MSCOCO_size-10000'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = data.MSCOCO_Dataset(path='/Users/christina/Data/mscoco/', batch_size=10000)\n",
    "# dataset = data.DiffusionDB_Dataset(path=\"2m_first_10k\", batch_size=None)\n",
    "all_images, all_prompts, dataset_name = get_data_helper(dataset, filters=[], method=any) # filters = [\"dog\"], method=all\n",
    "dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---model: CLIP---\n",
      "found cached embeddings for MSCOCO_size-10000_CLIP_RN50\n",
      "modality distance: 0.85 | validation loss: 0.35\n",
      "modified modality distance: 0.02 | modified validation loss: 1.27\n",
      "loss difference: 0.9294637683411111\n",
      "---model: OpenCLIP---\n",
      "found cached embeddings for MSCOCO_size-10000_OpenCLIP_RN50\n",
      "modality distance: 0.85 | validation loss: 0.35\n",
      "modified modality distance: 0.02 | modified validation loss: 1.27\n",
      "loss difference: 0.9294637649345995\n",
      "---model: CyCLIP---\n",
      "found cached embeddings for MSCOCO_size-10000_CyCLIP_RN50\n",
      "modality distance: 0.88 | validation loss: 0.91\n",
      "modified modality distance: 0.01 | modified validation loss: 0.94\n",
      "loss difference: 0.03208349469931815\n",
      "---model: CLOOB---\n",
      "Loading model from /Users/christina/Workspace/CLIP-explorer/clipexplorer/CLOOB_local/training/model_configs/RN50.json\n",
      "found cached embeddings for MSCOCO_size-10000_CLOOB_RN50\n",
      "modality distance: 0.56 | validation loss: 0.70\n",
      "modified modality distance: 0.03 | modified validation loss: 0.66\n",
      "loss difference: -0.03811188028038093\n",
      "---model: CLOOB_LAION400M---\n",
      "found cached embeddings for MSCOCO_size-10000_CLOOB-LAION400M_ViT-B-16\n",
      "modality distance: 0.32 | validation loss: 0.52\n",
      "modified modality distance: 0.01 | modified validation loss: 0.52\n",
      "loss difference: 0.0032941417479991353\n"
     ]
    }
   ],
   "source": [
    "for clip_model in ['CLIP', 'OpenCLIP', 'CyCLIP', 'CLOOB', 'CLOOB_LAION400M']:\n",
    "    print('---model: %s---'%clip_model)\n",
    "    image_embedding, text_embedding, logit_scaling = get_embedding(clip_model, dataset_name, all_images, all_prompts)\n",
    "    modality_distance = get_modality_distance(image_embedding, text_embedding)\n",
    "    val_loss = calculate_val_loss(image_embedding, text_embedding, logit_scaling.exp())\n",
    "    print('modality distance: %.2f | validation loss: %.2f'%(modality_distance, val_loss))\n",
    "\n",
    "    image_embedding, text_embedding = get_closed_modality_gap(image_embedding, text_embedding)\n",
    "    modified_modality_distance = get_modality_distance(image_embedding, text_embedding)\n",
    "    modified_val_loss = calculate_val_loss(image_embedding, text_embedding, logit_scaling.exp())\n",
    "    print('modified modality distance: %.2f | modified validation loss: %.2f'%(modified_modality_distance, modified_val_loss))\n",
    "\n",
    "    print('loss difference:', modified_val_loss-val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.03s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christina/Workspace/CLIP-explorer/clipexplorer/data.py:95: FutureWarning:\n",
      "\n",
      "The input object of type 'Image' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Image', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'MSCOCO-Val_size-5000'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = data.MSCOCO_Val_Dataset(path='/Users/christina/Data/mscoco/validation/', batch_size=5000)\n",
    "all_images, all_prompts, dataset_name = get_data_helper(dataset, filters=[], method=any) # filters = [\"dog\"], method=all\n",
    "dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---model: CLIP---\n",
      "found cached embeddings for MSCOCO-Val_size-5000_CLIP_RN50\n",
      "modality distance: 0.82 | validation loss: 0.36\n",
      "modified modality distance: 0.04 | modified validation loss: 1.12\n",
      "loss difference: 0.7694095196212225\n",
      "---model: OpenCLIP---\n",
      "found cached embeddings for MSCOCO-Val_size-5000_OpenCLIP_RN50\n",
      "modality distance: 0.82 | validation loss: 0.36\n",
      "modified modality distance: 0.04 | modified validation loss: 1.12\n",
      "loss difference: 0.76940950766899\n",
      "---model: CyCLIP---\n",
      "found cached embeddings for MSCOCO-Val_size-5000_CyCLIP_RN50\n",
      "modality distance: 0.87 | validation loss: 0.76\n",
      "modified modality distance: 0.00 | modified validation loss: 0.85\n",
      "loss difference: 0.08543419265256569\n",
      "---model: CLOOB---\n",
      "Loading model from /Users/christina/Workspace/CLIP-explorer/clipexplorer/CLOOB_local/training/model_configs/RN50.json\n",
      "found cached embeddings for MSCOCO-Val_size-5000_CLOOB_RN50\n",
      "modality distance: 0.23 | validation loss: 0.76\n",
      "modified modality distance: 0.01 | modified validation loss: 0.73\n",
      "loss difference: -0.02833876246845146\n",
      "---model: CLOOB_LAION400M---\n",
      "found cached embeddings for MSCOCO-Val_size-5000_CLOOB-LAION400M_ViT-B-16\n",
      "modality distance: 0.29 | validation loss: 0.48\n",
      "modified modality distance: 0.00 | modified validation loss: 0.48\n",
      "loss difference: 0.0008154865469777728\n"
     ]
    }
   ],
   "source": [
    "for clip_model in ['CLIP', 'OpenCLIP', 'CyCLIP', 'CLOOB', 'CLOOB_LAION400M']:\n",
    "    print('---model: %s---'%clip_model)\n",
    "    image_embedding, text_embedding, logit_scaling = get_embedding(clip_model, dataset_name, all_images, all_prompts)\n",
    "    modality_distance = get_modality_distance(image_embedding, text_embedding)\n",
    "    val_loss = calculate_val_loss(image_embedding, text_embedding, logit_scaling.exp())\n",
    "    print('modality distance: %.2f | validation loss: %.2f'%(modality_distance, val_loss))\n",
    "\n",
    "    image_embedding, text_embedding = get_closed_modality_gap(image_embedding, text_embedding)\n",
    "    modified_modality_distance = get_modality_distance(image_embedding, text_embedding)\n",
    "    modified_val_loss = calculate_val_loss(image_embedding, text_embedding, logit_scaling.exp())\n",
    "    print('modified modality distance: %.2f | modified validation loss: %.2f'%(modified_modality_distance, modified_val_loss))\n",
    "\n",
    "    print('loss difference:', modified_val_loss-val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from transformers) (3.12.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from transformers) (2023.5.5)\n",
      "Requirement already satisfied: requests in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
      "  Downloading tokenizers-0.13.3-cp39-cp39-macosx_10_11_x86_64.whl (4.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests->transformers) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests->transformers) (2023.5.7)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "Successfully installed tokenizers-0.13.3 transformers-4.29.2\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0832e3184d154ae08607e29784effe9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)rocessor_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7714e4ceb9864e0e8c4b62c436f036a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/506 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "242d62c060f34d778252bdd2bda6dad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85c78f0b17064bfdb1105a3ca052c36d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "792fca1460144c908f0c850282001244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "646779f87a2849cba8bb0a346e8fd1a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/4.56k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8fcde0a940744a6a6613981c66922cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christina/anaconda3/envs/myenv/lib/python3.9/site-packages/transformers/generation/utils.py:1346: UserWarning:\n",
      "\n",
      "Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a photography of a woman and her dog on the beach\n",
      "a woman sitting on the beach with her dog\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \n",
    "raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n",
    "\n",
    "# conditional image captioning\n",
    "text = \"a photography of\"\n",
    "inputs = processor(raw_image, text, return_tensors=\"pt\")\n",
    "\n",
    "out = model.generate(**inputs)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))\n",
    "\n",
    "# unconditional image captioning\n",
    "inputs = processor(raw_image, return_tensors=\"pt\")\n",
    "\n",
    "out = model.generate(**inputs)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<PIL.Image.Image image mode=RGB size=2048x1365>,\n",
       " <PIL.Image.Image image mode=RGB size=2048x1365>]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[raw_image, raw_image]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BlipProcessor:\n",
       "- image_processor: BlipImageProcessor {\n",
       "  \"do_convert_rgb\": true,\n",
       "  \"do_normalize\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_mean\": [\n",
       "    0.48145466,\n",
       "    0.4578275,\n",
       "    0.40821073\n",
       "  ],\n",
       "  \"image_processor_type\": \"BlipImageProcessor\",\n",
       "  \"image_std\": [\n",
       "    0.26862954,\n",
       "    0.26130258,\n",
       "    0.27577711\n",
       "  ],\n",
       "  \"processor_class\": \"BlipProcessor\",\n",
       "  \"resample\": 3,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"height\": 384,\n",
       "    \"width\": 384\n",
       "  }\n",
       "}\n",
       "\n",
       "- tokenizer: BertTokenizerFast(name_or_path='Salesforce/blip-image-captioning-base', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BlipImageProcessor, BlipModel, BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Salesforce/blip-image-captioning-base were not used when initializing BlipModel: ['text_decoder.bert.encoder.layer.8.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.7.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.0.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.2.attention.self.value.bias', 'text_decoder.bert.encoder.layer.6.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.1.attention.self.key.bias', 'text_decoder.bert.encoder.layer.7.attention.self.key.weight', 'text_decoder.bert.encoder.layer.4.attention.self.query.bias', 'text_decoder.bert.encoder.layer.11.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.1.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.11.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.9.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.5.output.dense.bias', 'text_decoder.bert.encoder.layer.9.attention.self.value.bias', 'text_decoder.bert.encoder.layer.4.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.2.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.7.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.5.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.6.attention.self.key.weight', 'text_decoder.bert.encoder.layer.2.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.4.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.5.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.9.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.6.attention.self.query.weight', 'text_decoder.bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.7.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.9.attention.self.key.weight', 'text_decoder.bert.encoder.layer.11.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.6.attention.self.query.bias', 'text_decoder.bert.encoder.layer.1.attention.self.query.weight', 'text_decoder.bert.encoder.layer.7.output.dense.bias', 'text_decoder.bert.encoder.layer.1.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.5.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.9.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.5.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.6.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.1.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.3.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.2.attention.self.query.bias', 'text_decoder.bert.encoder.layer.10.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.8.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.8.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.3.attention.self.value.bias', 'text_decoder.bert.encoder.layer.2.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.10.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.0.attention.self.query.weight', 'text_decoder.bert.encoder.layer.0.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.3.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.7.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.5.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.7.attention.self.query.weight', 'text_decoder.bert.encoder.layer.5.intermediate.dense.weight', 'text_decoder.bert.embeddings.position_embeddings.weight', 'text_decoder.bert.encoder.layer.2.attention.self.value.weight', 'text_decoder.bert.encoder.layer.3.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.0.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.5.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.3.attention.self.query.bias', 'text_decoder.bert.encoder.layer.10.output.dense.bias', 'text_decoder.bert.encoder.layer.9.attention.self.query.weight', 'text_decoder.bert.encoder.layer.9.attention.self.key.bias', 'text_decoder.bert.encoder.layer.5.attention.self.key.bias', 'text_decoder.bert.encoder.layer.7.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.6.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.10.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.9.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.8.attention.self.query.bias', 'text_decoder.bert.encoder.layer.10.attention.self.query.bias', 'text_decoder.bert.encoder.layer.10.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.0.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.11.attention.self.key.weight', 'text_decoder.bert.encoder.layer.4.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.7.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.2.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.6.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.7.attention.self.key.bias', 'text_decoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.10.attention.self.query.weight', 'text_decoder.bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.8.attention.self.key.bias', 'text_decoder.bert.encoder.layer.8.attention.self.query.weight', 'text_decoder.bert.encoder.layer.8.intermediate.dense.bias', 'text_decoder.cls.predictions.decoder.weight', 'text_decoder.bert.encoder.layer.3.output.dense.bias', 'text_decoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.1.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.11.output.dense.weight', 'text_decoder.bert.encoder.layer.2.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.8.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.6.attention.self.value.weight', 'text_decoder.bert.encoder.layer.10.output.dense.weight', 'text_decoder.bert.encoder.layer.9.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.11.attention.self.query.bias', 'text_decoder.bert.encoder.layer.9.output.dense.bias', 'text_decoder.bert.encoder.layer.6.output.dense.weight', 'text_decoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.4.attention.self.value.weight', 'text_decoder.bert.encoder.layer.1.attention.self.value.bias', 'text_decoder.bert.encoder.layer.0.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.5.attention.self.query.bias', 'text_decoder.bert.encoder.layer.11.output.dense.bias', 'text_decoder.cls.predictions.transform.LayerNorm.bias', 'text_decoder.bert.encoder.layer.8.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.9.crossattention.self.query.bias', 'text_decoder.bert.embeddings.position_ids', 'text_decoder.bert.encoder.layer.1.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.5.attention.self.key.weight', 'text_decoder.bert.encoder.layer.8.output.dense.bias', 'text_decoder.cls.predictions.decoder.bias', 'text_decoder.bert.encoder.layer.3.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.8.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.6.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.11.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.11.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.1.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.3.attention.self.value.weight', 'text_decoder.bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.0.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.7.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.7.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.6.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.2.output.dense.bias', 'text_decoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.4.output.dense.bias', 'text_decoder.bert.encoder.layer.0.attention.self.value.bias', 'text_decoder.bert.encoder.layer.8.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.0.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.6.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.5.attention.self.value.bias', 'text_decoder.bert.encoder.layer.10.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.6.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.3.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.5.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.9.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.6.attention.self.value.bias', 'text_decoder.bert.encoder.layer.10.attention.self.value.bias', 'text_decoder.bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.2.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.8.output.dense.weight', 'text_decoder.bert.encoder.layer.2.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.3.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.7.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.1.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.11.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.4.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.4.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.3.output.dense.weight', 'text_decoder.bert.encoder.layer.4.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.7.attention.self.query.bias', 'text_decoder.bert.encoder.layer.2.attention.self.key.weight', 'text_decoder.bert.encoder.layer.4.attention.self.value.bias', 'text_decoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.2.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.9.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.0.attention.self.value.weight', 'text_decoder.bert.encoder.layer.9.output.dense.weight', 'text_decoder.bert.encoder.layer.5.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.10.attention.output.dense.bias', 'text_decoder.bert.embeddings.LayerNorm.bias', 'text_decoder.bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.0.output.dense.bias', 'text_decoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.5.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.7.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.3.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.3.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.9.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.4.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.0.attention.self.key.weight', 'text_decoder.bert.encoder.layer.4.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.4.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.3.attention.self.key.weight', 'text_decoder.bert.encoder.layer.7.attention.self.value.bias', 'text_decoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.5.attention.self.value.weight', 'text_decoder.bert.encoder.layer.10.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.10.intermediate.dense.bias', 'text_decoder.cls.predictions.bias', 'text_decoder.bert.encoder.layer.4.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.11.attention.self.value.weight', 'text_decoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.6.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.8.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.5.attention.self.query.weight', 'text_decoder.bert.encoder.layer.6.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.0.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.3.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.4.attention.self.key.weight', 'text_decoder.bert.encoder.layer.1.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.1.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.11.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.11.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.8.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.10.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.0.attention.self.query.bias', 'text_decoder.bert.encoder.layer.8.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.7.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.11.attention.self.key.bias', 'text_decoder.cls.predictions.transform.dense.weight', 'text_decoder.bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.11.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.11.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.8.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.0.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.10.attention.self.key.weight', 'text_decoder.bert.encoder.layer.2.attention.self.query.weight', 'text_decoder.bert.encoder.layer.4.attention.self.query.weight', 'text_decoder.bert.encoder.layer.4.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'text_decoder.bert.embeddings.LayerNorm.weight', 'text_decoder.bert.encoder.layer.4.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.9.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.3.attention.self.query.weight', 'text_decoder.bert.encoder.layer.7.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.11.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.9.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.11.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.6.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.8.attention.self.value.bias', 'text_decoder.bert.encoder.layer.1.output.dense.bias', 'text_decoder.bert.encoder.layer.2.attention.self.key.bias', 'text_decoder.bert.encoder.layer.1.attention.self.query.bias', 'text_decoder.bert.encoder.layer.11.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.10.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.10.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.9.attention.self.query.bias', 'text_decoder.bert.encoder.layer.3.attention.self.key.bias', 'text_decoder.bert.encoder.layer.11.attention.self.value.bias', 'text_decoder.bert.encoder.layer.7.attention.self.value.weight', 'text_decoder.bert.encoder.layer.0.output.dense.weight', 'text_decoder.bert.encoder.layer.2.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.5.output.dense.weight', 'text_decoder.bert.encoder.layer.8.attention.self.value.weight', 'text_decoder.bert.encoder.layer.10.crossattention.self.query.bias', 'text_decoder.cls.predictions.transform.LayerNorm.weight', 'text_decoder.bert.encoder.layer.3.attention.output.dense.bias', 'text_decoder.cls.predictions.transform.dense.bias', 'text_decoder.bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.7.output.dense.weight', 'text_decoder.bert.encoder.layer.2.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.3.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.2.output.dense.weight', 'text_decoder.bert.encoder.layer.11.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.6.output.dense.bias', 'text_decoder.bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.8.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.6.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.0.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.4.attention.self.key.bias', 'text_decoder.bert.encoder.layer.5.crossattention.self.query.bias', 'text_decoder.bert.embeddings.word_embeddings.weight', 'text_decoder.bert.encoder.layer.7.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.4.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.2.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.1.attention.self.key.weight', 'text_decoder.bert.encoder.layer.10.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.5.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.1.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.1.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.1.output.dense.weight', 'text_decoder.bert.encoder.layer.0.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.3.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.5.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.2.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.8.attention.self.key.weight', 'text_decoder.bert.encoder.layer.10.attention.self.value.weight', 'text_decoder.bert.encoder.layer.0.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.2.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.2.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.6.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.3.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.1.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.10.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.9.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.8.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.1.attention.self.value.weight', 'text_decoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.1.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.3.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.9.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.11.attention.self.query.weight', 'text_decoder.bert.encoder.layer.4.output.dense.weight', 'text_decoder.bert.encoder.layer.10.attention.self.key.bias', 'text_decoder.bert.encoder.layer.0.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.0.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.5.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.4.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.6.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.9.attention.self.value.weight', 'text_decoder.bert.encoder.layer.1.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.6.attention.self.key.bias', 'text_decoder.bert.encoder.layer.9.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.0.attention.self.key.bias', 'text_decoder.bert.encoder.layer.7.crossattention.output.dense.weight']\n",
      "- This IS expected if you are initializing BlipModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BlipModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BlipModel were not initialized from the model checkpoint at Salesforce/blip-image-captioning-base and are newly initialized: ['text_model.encoder.layer.4.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.3.output.LayerNorm.weight', 'text_model.encoder.layer.7.crossattention.self.key.bias', 'text_model.encoder.layer.11.crossattention.self.value.weight', 'text_model.encoder.layer.3.attention.output.dense.weight', 'text_model.encoder.layer.6.attention.self.key.bias', 'text_model.encoder.layer.8.attention.self.query.weight', 'text_model.encoder.layer.5.attention.output.dense.bias', 'text_model.encoder.layer.2.crossattention.self.value.bias', 'text_model.encoder.layer.4.output.dense.weight', 'text_model.encoder.layer.10.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.8.crossattention.self.query.bias', 'text_model.encoder.layer.7.output.dense.bias', 'text_model.encoder.layer.3.crossattention.output.dense.weight', 'text_model.encoder.layer.5.crossattention.self.key.bias', 'text_model.encoder.layer.9.crossattention.self.query.weight', 'text_model.encoder.layer.9.attention.output.dense.bias', 'text_model.encoder.layer.2.crossattention.self.query.weight', 'text_model.encoder.layer.4.attention.output.dense.weight', 'text_model.encoder.layer.9.output.dense.bias', 'text_model.encoder.layer.2.crossattention.self.value.weight', 'text_model.encoder.layer.11.crossattention.self.key.bias', 'text_model.encoder.layer.9.crossattention.self.value.weight', 'text_model.encoder.layer.7.crossattention.self.value.weight', 'text_model.encoder.layer.1.crossattention.self.key.bias', 'text_model.encoder.layer.0.intermediate.dense.weight', 'text_model.encoder.layer.5.crossattention.self.value.bias', 'text_model.encoder.layer.6.attention.output.LayerNorm.bias', 'text_model.encoder.layer.7.crossattention.self.query.weight', 'text_model.encoder.layer.10.attention.self.query.weight', 'text_model.encoder.layer.0.crossattention.self.value.weight', 'text_model.encoder.layer.5.output.LayerNorm.bias', 'text_model.encoder.layer.8.attention.self.key.bias', 'text_model.encoder.layer.11.output.LayerNorm.bias', 'text_model.encoder.layer.11.attention.self.value.weight', 'text_model.encoder.layer.11.output.LayerNorm.weight', 'text_model.encoder.layer.9.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.10.output.LayerNorm.weight', 'text_model.encoder.layer.0.intermediate.dense.bias', 'text_model.encoder.layer.8.crossattention.output.dense.bias', 'text_model.encoder.layer.7.attention.output.dense.bias', 'text_model.encoder.layer.4.attention.self.key.weight', 'text_model.encoder.layer.5.attention.output.dense.weight', 'text_model.encoder.layer.9.attention.output.dense.weight', 'text_projection.weight', 'text_model.encoder.layer.3.attention.self.key.bias', 'text_model.encoder.layer.1.attention.self.key.weight', 'text_model.encoder.layer.5.attention.self.query.weight', 'text_model.encoder.layer.0.crossattention.self.key.bias', 'text_model.encoder.layer.5.output.dense.weight', 'text_model.embeddings.LayerNorm.weight', 'text_model.encoder.layer.4.crossattention.self.value.bias', 'text_model.encoder.layer.10.attention.output.dense.bias', 'text_model.encoder.layer.4.output.LayerNorm.weight', 'text_model.encoder.layer.8.intermediate.dense.bias', 'text_model.encoder.layer.4.crossattention.output.dense.weight', 'text_model.encoder.layer.11.crossattention.output.dense.weight', 'text_model.encoder.layer.5.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.7.crossattention.output.dense.bias', 'text_model.encoder.layer.1.attention.output.LayerNorm.bias', 'text_model.encoder.layer.8.attention.output.dense.bias', 'text_model.encoder.layer.10.crossattention.self.query.weight', 'text_model.encoder.layer.1.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.8.output.dense.bias', 'text_model.encoder.layer.6.crossattention.self.value.weight', 'text_model.encoder.layer.2.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.5.output.LayerNorm.weight', 'text_model.encoder.layer.11.attention.output.dense.bias', 'text_model.encoder.layer.3.attention.self.key.weight', 'text_model.encoder.layer.0.output.LayerNorm.weight', 'text_model.encoder.layer.1.crossattention.self.query.bias', 'text_model.encoder.layer.6.attention.output.dense.weight', 'text_model.encoder.layer.3.crossattention.output.dense.bias', 'text_model.encoder.layer.9.attention.self.query.bias', 'text_model.encoder.layer.3.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.1.crossattention.self.value.bias', 'text_model.encoder.layer.9.output.LayerNorm.weight', 'text_model.encoder.layer.1.intermediate.dense.weight', 'text_model.encoder.layer.11.crossattention.output.dense.bias', 'text_model.encoder.layer.8.output.LayerNorm.weight', 'text_model.encoder.layer.8.crossattention.self.key.weight', 'text_model.encoder.layer.0.crossattention.self.value.bias', 'text_model.encoder.layer.5.attention.self.query.bias', 'text_model.encoder.layer.2.output.LayerNorm.weight', 'text_model.encoder.layer.6.crossattention.self.key.bias', 'text_model.encoder.layer.1.attention.output.LayerNorm.weight', 'text_model.embeddings.LayerNorm.bias', 'text_model.encoder.layer.11.attention.self.query.weight', 'text_model.encoder.layer.4.crossattention.self.key.bias', 'text_model.encoder.layer.10.crossattention.output.dense.bias', 'text_model.encoder.layer.11.intermediate.dense.weight', 'text_model.encoder.layer.9.output.dense.weight', 'text_model.encoder.layer.6.crossattention.output.dense.weight', 'text_model.encoder.layer.5.crossattention.output.dense.bias', 'text_model.encoder.layer.3.attention.self.query.bias', 'text_model.encoder.layer.6.output.LayerNorm.weight', 'text_model.encoder.layer.2.attention.output.LayerNorm.bias', 'text_model.encoder.layer.10.attention.self.query.bias', 'text_model.encoder.layer.8.attention.self.query.bias', 'text_model.encoder.layer.1.output.dense.bias', 'text_model.encoder.layer.9.crossattention.output.dense.weight', 'text_model.encoder.layer.3.crossattention.self.key.weight', 'text_model.encoder.layer.2.attention.output.LayerNorm.weight', 'text_model.encoder.layer.4.intermediate.dense.weight', 'text_model.encoder.layer.5.attention.output.LayerNorm.weight', 'text_model.encoder.layer.7.output.dense.weight', 'text_model.encoder.layer.9.attention.self.key.bias', 'text_model.encoder.layer.3.attention.self.value.weight', 'text_model.encoder.layer.10.output.dense.weight', 'text_model.encoder.layer.3.output.LayerNorm.bias', 'text_model.encoder.layer.9.attention.output.LayerNorm.weight', 'text_model.encoder.layer.7.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.0.crossattention.self.key.weight', 'text_model.encoder.layer.4.crossattention.self.value.weight', 'text_model.encoder.layer.8.crossattention.output.dense.weight', 'text_model.encoder.layer.6.crossattention.output.dense.bias', 'text_model.encoder.layer.8.output.dense.weight', 'text_model.encoder.layer.9.attention.self.value.weight', 'text_model.encoder.layer.2.intermediate.dense.bias', 'text_model.encoder.layer.3.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.8.crossattention.self.value.weight', 'text_model.encoder.layer.3.attention.output.dense.bias', 'text_model.encoder.layer.7.intermediate.dense.weight', 'text_model.encoder.layer.7.attention.output.LayerNorm.bias', 'text_model.encoder.layer.0.crossattention.output.dense.weight', 'text_model.encoder.layer.6.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.5.crossattention.self.query.weight', 'text_model.encoder.layer.3.crossattention.self.query.weight', 'text_model.encoder.layer.10.attention.output.LayerNorm.weight', 'text_model.encoder.layer.5.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.4.crossattention.output.dense.bias', 'text_model.encoder.layer.9.intermediate.dense.weight', 'text_model.encoder.layer.11.attention.self.query.bias', 'text_model.encoder.layer.7.crossattention.self.query.bias', 'text_model.encoder.layer.10.attention.self.value.weight', 'text_model.encoder.layer.2.crossattention.output.dense.weight', 'text_model.encoder.layer.11.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.11.crossattention.self.query.bias', 'text_model.pooler.dense.weight', 'text_model.encoder.layer.11.crossattention.self.value.bias', 'text_model.encoder.layer.6.output.dense.weight', 'text_model.encoder.layer.1.attention.self.query.weight', 'text_model.encoder.layer.11.attention.self.key.bias', 'text_model.encoder.layer.9.crossattention.self.value.bias', 'text_model.encoder.layer.3.attention.output.LayerNorm.weight', 'text_model.encoder.layer.4.crossattention.self.query.weight', 'text_model.encoder.layer.4.crossattention.self.key.weight', 'text_model.encoder.layer.5.crossattention.output.dense.weight', 'text_model.encoder.layer.3.output.dense.weight', 'text_model.encoder.layer.6.crossattention.self.key.weight', 'text_model.encoder.layer.2.attention.self.query.bias', 'text_model.encoder.layer.7.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.0.attention.self.key.weight', 'text_model.encoder.layer.2.crossattention.output.dense.bias', 'text_model.encoder.layer.1.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.7.crossattention.output.dense.weight', 'text_model.encoder.layer.10.intermediate.dense.bias', 'text_model.encoder.layer.0.crossattention.self.query.weight', 'text_model.encoder.layer.0.attention.self.query.bias', 'text_model.encoder.layer.9.crossattention.self.query.bias', 'text_model.encoder.layer.5.intermediate.dense.weight', 'text_model.pooler.dense.bias', 'text_model.encoder.layer.1.crossattention.output.dense.bias', 'text_model.encoder.layer.2.attention.self.value.weight', 'text_model.encoder.layer.3.crossattention.self.key.bias', 'text_model.encoder.layer.8.output.LayerNorm.bias', 'text_model.embeddings.word_embeddings.weight', 'text_model.encoder.layer.5.intermediate.dense.bias', 'text_model.encoder.layer.0.attention.self.query.weight', 'text_model.encoder.layer.1.crossattention.self.query.weight', 'text_model.encoder.layer.9.crossattention.self.key.bias', 'text_model.encoder.layer.2.crossattention.self.key.bias', 'text_model.encoder.layer.9.attention.output.LayerNorm.bias', 'text_model.encoder.layer.6.attention.self.query.bias', 'text_model.encoder.layer.11.output.dense.bias', 'text_model.encoder.layer.4.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.0.attention.output.LayerNorm.weight', 'logit_scale', 'text_model.encoder.layer.1.output.dense.weight', 'text_model.encoder.layer.6.intermediate.dense.bias', 'text_model.encoder.layer.5.output.dense.bias', 'text_model.encoder.layer.6.attention.self.key.weight', 'text_model.encoder.layer.2.intermediate.dense.weight', 'text_model.encoder.layer.6.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.1.attention.self.value.bias', 'text_model.encoder.layer.0.crossattention.self.query.bias', 'text_model.encoder.layer.5.attention.self.key.weight', 'text_model.encoder.layer.8.attention.self.value.bias', 'text_model.encoder.layer.8.crossattention.self.value.bias', 'visual_projection.weight', 'text_model.encoder.layer.2.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.4.crossattention.self.query.bias', 'text_model.encoder.layer.6.crossattention.self.query.bias', 'text_model.encoder.layer.7.crossattention.self.key.weight', 'text_model.encoder.layer.8.attention.output.dense.weight', 'text_model.encoder.layer.7.attention.output.dense.weight', 'text_model.encoder.layer.11.crossattention.self.query.weight', 'text_model.encoder.layer.4.attention.output.dense.bias', 'text_model.encoder.layer.2.attention.output.dense.bias', 'text_model.encoder.layer.1.intermediate.dense.bias', 'text_model.encoder.layer.2.attention.output.dense.weight', 'text_model.encoder.layer.4.attention.output.LayerNorm.bias', 'text_model.encoder.layer.3.crossattention.self.query.bias', 'text_model.encoder.layer.9.crossattention.output.dense.bias', 'text_model.encoder.layer.2.output.LayerNorm.bias', 'text_model.encoder.layer.3.attention.output.LayerNorm.bias', 'text_model.encoder.layer.6.attention.output.dense.bias', 'text_model.encoder.layer.7.attention.output.LayerNorm.weight', 'text_model.encoder.layer.11.attention.output.LayerNorm.weight', 'text_model.encoder.layer.6.attention.output.LayerNorm.weight', 'text_model.encoder.layer.0.attention.output.dense.bias', 'text_model.encoder.layer.9.crossattention.self.key.weight', 'text_model.encoder.layer.10.attention.output.LayerNorm.bias', 'text_model.encoder.layer.4.attention.output.LayerNorm.weight', 'text_model.encoder.layer.7.attention.self.value.weight', 'text_model.encoder.layer.10.crossattention.output.dense.weight', 'text_model.encoder.layer.5.crossattention.self.key.weight', 'text_model.encoder.layer.2.crossattention.self.query.bias', 'text_model.encoder.layer.8.attention.output.LayerNorm.weight', 'text_model.encoder.layer.8.attention.output.LayerNorm.bias', 'text_model.encoder.layer.8.intermediate.dense.weight', 'text_model.encoder.layer.5.crossattention.self.query.bias', 'text_model.encoder.layer.5.crossattention.self.value.weight', 'text_model.encoder.layer.0.output.LayerNorm.bias', 'text_model.encoder.layer.3.crossattention.self.value.bias', 'text_model.encoder.layer.0.attention.output.dense.weight', 'text_model.encoder.layer.0.output.dense.bias', 'text_model.encoder.layer.7.attention.self.key.weight', 'text_model.encoder.layer.4.attention.self.query.bias', 'text_model.encoder.layer.0.crossattention.output.dense.bias', 'text_model.encoder.layer.10.attention.self.value.bias', 'text_model.encoder.layer.1.output.LayerNorm.weight', 'text_model.encoder.layer.2.attention.self.key.weight', 'text_model.encoder.layer.1.attention.output.dense.bias', 'text_model.encoder.layer.6.intermediate.dense.weight', 'text_model.encoder.layer.1.attention.self.key.bias', 'text_model.encoder.layer.4.intermediate.dense.bias', 'text_model.encoder.layer.0.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.6.crossattention.self.query.weight', 'text_model.encoder.layer.0.output.dense.weight', 'text_model.encoder.layer.9.attention.self.query.weight', 'text_model.encoder.layer.11.attention.self.key.weight', 'text_model.encoder.layer.2.attention.self.key.bias', 'text_model.encoder.layer.6.attention.self.query.weight', 'text_model.encoder.layer.5.attention.output.LayerNorm.bias', 'text_model.encoder.layer.0.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.10.attention.output.dense.weight', 'text_model.encoder.layer.8.crossattention.self.key.bias', 'text_model.encoder.layer.6.output.LayerNorm.bias', 'text_model.encoder.layer.8.attention.self.value.weight', 'text_model.encoder.layer.2.attention.self.query.weight', 'text_model.encoder.layer.7.attention.self.query.bias', 'text_model.encoder.layer.10.attention.self.key.weight', 'text_model.encoder.layer.0.attention.self.value.weight', 'text_model.encoder.layer.8.crossattention.self.query.weight', 'text_model.encoder.layer.1.output.LayerNorm.bias', 'text_model.encoder.layer.7.attention.self.key.bias', 'text_model.encoder.layer.6.crossattention.self.value.bias', 'text_model.encoder.layer.10.crossattention.self.key.weight', 'text_model.encoder.layer.3.attention.self.query.weight', 'text_model.encoder.layer.3.intermediate.dense.weight', 'text_model.encoder.layer.3.intermediate.dense.bias', 'text_model.encoder.layer.3.attention.self.value.bias', 'text_model.encoder.layer.4.output.LayerNorm.bias', 'text_model.encoder.layer.5.attention.self.value.weight', 'text_model.encoder.layer.8.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.9.intermediate.dense.bias', 'text_model.embeddings.position_embeddings.weight', 'text_model.encoder.layer.3.output.dense.bias', 'text_model.encoder.layer.10.intermediate.dense.weight', 'text_model.encoder.layer.1.attention.self.value.weight', 'text_model.encoder.layer.11.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.11.attention.output.LayerNorm.bias', 'text_model.encoder.layer.6.attention.self.value.bias', 'text_model.encoder.layer.7.crossattention.self.value.bias', 'text_model.encoder.layer.8.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.11.output.dense.weight', 'text_model.encoder.layer.9.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.0.attention.self.key.bias', 'text_model.encoder.layer.3.crossattention.self.value.weight', 'text_model.encoder.layer.7.output.LayerNorm.weight', 'text_model.encoder.layer.6.output.dense.bias', 'text_model.encoder.layer.10.crossattention.self.value.bias', 'text_model.encoder.layer.9.attention.self.key.weight', 'text_model.encoder.layer.1.crossattention.self.value.weight', 'text_model.encoder.layer.8.attention.self.key.weight', 'text_model.encoder.layer.1.crossattention.output.dense.weight', 'text_model.encoder.layer.4.attention.self.value.bias', 'text_model.encoder.layer.0.attention.self.value.bias', 'text_model.encoder.layer.10.crossattention.self.query.bias', 'text_model.encoder.layer.10.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.11.attention.self.value.bias', 'text_model.encoder.layer.1.crossattention.self.key.weight', 'text_model.encoder.layer.4.output.dense.bias', 'text_model.encoder.layer.4.attention.self.query.weight', 'text_model.encoder.layer.10.attention.self.key.bias', 'text_model.encoder.layer.2.crossattention.self.key.weight', 'text_model.encoder.layer.10.crossattention.self.key.bias', 'text_model.encoder.layer.5.attention.self.key.bias', 'text_model.encoder.layer.4.attention.self.key.bias', 'text_model.encoder.layer.7.attention.self.value.bias', 'text_model.encoder.layer.11.intermediate.dense.bias', 'text_model.encoder.layer.7.intermediate.dense.bias', 'text_model.encoder.layer.7.attention.self.query.weight', 'text_model.encoder.layer.5.attention.self.value.bias', 'text_model.encoder.layer.10.output.dense.bias', 'text_model.encoder.layer.10.crossattention.self.value.weight', 'text_model.encoder.layer.2.output.dense.weight', 'text_model.encoder.layer.1.attention.output.dense.weight', 'text_model.encoder.layer.11.attention.output.dense.weight', 'text_model.encoder.layer.6.attention.self.value.weight', 'text_model.encoder.layer.11.crossattention.self.key.weight', 'text_model.encoder.layer.7.output.LayerNorm.bias', 'text_model.encoder.layer.10.output.LayerNorm.bias', 'text_model.encoder.layer.1.attention.self.query.bias', 'text_model.encoder.layer.0.attention.output.LayerNorm.bias', 'text_model.encoder.layer.4.attention.self.value.weight', 'text_model.encoder.layer.9.output.LayerNorm.bias', 'text_model.encoder.layer.2.attention.self.value.bias', 'text_model.encoder.layer.9.attention.self.value.bias', 'text_model.encoder.layer.2.output.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BlipModel.from_pretrained(\"Salesforce/blip-image-captioning-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = BlipImageProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('Salesforce/blip-image-captioning-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = processor(raw_image, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/46/rz1kpyy146n3kk5pyv0pqdh80000gn/T/ipykernel_17580/348425520.py:2: FutureWarning:\n",
      "\n",
      "The input object of type 'Image' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Image', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "\n",
      "/var/folders/46/rz1kpyy146n3kk5pyv0pqdh80000gn/T/ipykernel_17580/348425520.py:2: VisibleDeprecationWarning:\n",
      "\n",
      "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'pixel_values': [array([[[ 0.8646511 ,  0.9230448 ,  0.93764323, ...,  1.7551551 ,\n",
       "          1.7551551 ,  1.7551551 ],\n",
       "        [ 0.9084464 ,  0.93764323,  0.95224166, ...,  1.7551551 ,\n",
       "          1.7551551 ,  1.7551551 ],\n",
       "        [ 0.93764323,  0.93764323,  0.9668401 , ...,  1.7551551 ,\n",
       "          1.7551551 ,  1.7551551 ],\n",
       "        ...,\n",
       "        [-0.78497106, -0.78497106, -0.72657734, ..., -0.31782144,\n",
       "         -0.2740262 , -0.36161673],\n",
       "        [-0.7557742 , -0.7557742 , -0.7411758 , ..., -0.31782144,\n",
       "         -0.36161673, -0.43460885],\n",
       "        [-0.7557742 , -0.7703726 , -0.78497106, ..., -0.36161673,\n",
       "         -0.43460885, -0.47840413]],\n",
       "\n",
       "       [[ 1.219441  ,  1.2494565 ,  1.2794721 , ...,  1.8947905 ,\n",
       "          1.8947905 ,  1.8947905 ],\n",
       "        [ 1.2344488 ,  1.2644643 ,  1.2944798 , ...,  1.8947905 ,\n",
       "          1.8947905 ,  1.8947905 ],\n",
       "        [ 1.2494565 ,  1.2794721 ,  1.3094876 , ...,  1.8947905 ,\n",
       "          1.8947905 ,  1.8947905 ],\n",
       "        ...,\n",
       "        [-0.5964989 , -0.5964989 , -0.55147564, ..., -0.40139794,\n",
       "         -0.32635912, -0.4164057 ],\n",
       "        [-0.5664834 , -0.5664834 , -0.55147564, ..., -0.38639018,\n",
       "         -0.4164057 , -0.49144456],\n",
       "        [-0.5664834 , -0.5814912 , -0.5964989 , ..., -0.4164057 ,\n",
       "         -0.4764368 , -0.53646785]],\n",
       "\n",
       "       [[ 1.2926931 ,  1.3211333 ,  1.3495734 , ...,  1.9752562 ,\n",
       "          1.9752562 ,  1.9752562 ],\n",
       "        [ 1.3211333 ,  1.3353534 ,  1.3637935 , ...,  1.9752562 ,\n",
       "          1.9752562 ,  1.9752562 ],\n",
       "        [ 1.3353534 ,  1.3495734 ,  1.3780135 , ...,  1.9752562 ,\n",
       "          1.9752562 ,  1.9752562 ],\n",
       "        ...,\n",
       "        [-0.35683453, -0.34261447, -0.2857342 , ..., -0.25729406,\n",
       "         -0.21463387, -0.2857342 ],\n",
       "        [-0.31417432, -0.29995427, -0.29995427, ..., -0.25729406,\n",
       "         -0.2857342 , -0.35683453],\n",
       "        [-0.31417432, -0.34261447, -0.35683453, ..., -0.29995427,\n",
       "         -0.35683453, -0.39949474]]], dtype=float32), array([[[ 0.8646511 ,  0.9230448 ,  0.93764323, ...,  1.7551551 ,\n",
       "          1.7551551 ,  1.7551551 ],\n",
       "        [ 0.9084464 ,  0.93764323,  0.95224166, ...,  1.7551551 ,\n",
       "          1.7551551 ,  1.7551551 ],\n",
       "        [ 0.93764323,  0.93764323,  0.9668401 , ...,  1.7551551 ,\n",
       "          1.7551551 ,  1.7551551 ],\n",
       "        ...,\n",
       "        [-0.78497106, -0.78497106, -0.72657734, ..., -0.31782144,\n",
       "         -0.2740262 , -0.36161673],\n",
       "        [-0.7557742 , -0.7557742 , -0.7411758 , ..., -0.31782144,\n",
       "         -0.36161673, -0.43460885],\n",
       "        [-0.7557742 , -0.7703726 , -0.78497106, ..., -0.36161673,\n",
       "         -0.43460885, -0.47840413]],\n",
       "\n",
       "       [[ 1.219441  ,  1.2494565 ,  1.2794721 , ...,  1.8947905 ,\n",
       "          1.8947905 ,  1.8947905 ],\n",
       "        [ 1.2344488 ,  1.2644643 ,  1.2944798 , ...,  1.8947905 ,\n",
       "          1.8947905 ,  1.8947905 ],\n",
       "        [ 1.2494565 ,  1.2794721 ,  1.3094876 , ...,  1.8947905 ,\n",
       "          1.8947905 ,  1.8947905 ],\n",
       "        ...,\n",
       "        [-0.5964989 , -0.5964989 , -0.55147564, ..., -0.40139794,\n",
       "         -0.32635912, -0.4164057 ],\n",
       "        [-0.5664834 , -0.5664834 , -0.55147564, ..., -0.38639018,\n",
       "         -0.4164057 , -0.49144456],\n",
       "        [-0.5664834 , -0.5814912 , -0.5964989 , ..., -0.4164057 ,\n",
       "         -0.4764368 , -0.53646785]],\n",
       "\n",
       "       [[ 1.2926931 ,  1.3211333 ,  1.3495734 , ...,  1.9752562 ,\n",
       "          1.9752562 ,  1.9752562 ],\n",
       "        [ 1.3211333 ,  1.3353534 ,  1.3637935 , ...,  1.9752562 ,\n",
       "          1.9752562 ,  1.9752562 ],\n",
       "        [ 1.3353534 ,  1.3495734 ,  1.3780135 , ...,  1.9752562 ,\n",
       "          1.9752562 ,  1.9752562 ],\n",
       "        ...,\n",
       "        [-0.35683453, -0.34261447, -0.2857342 , ..., -0.25729406,\n",
       "         -0.21463387, -0.2857342 ],\n",
       "        [-0.31417432, -0.29995427, -0.29995427, ..., -0.25729406,\n",
       "         -0.2857342 , -0.35683453],\n",
       "        [-0.31417432, -0.34261447, -0.35683453, ..., -0.29995427,\n",
       "         -0.35683453, -0.39949474]]], dtype=float32)]}"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "processor(list(np.array([raw_image, raw_image])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 384, 384])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input['pixel_values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1037, 5855, 1997, 1037, 2450, 1998, 2014, 3899, 2006, 1996, 3509,\n",
       "          102],\n",
       "        [ 101, 1998, 2014, 3899, 2006, 1996, 3509,  102,    0,    0,    0,    0,\n",
       "            0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer(['a photography of a woman and her dog on the beach', \"and her dog on the beach\"], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BlipModel.get_image_features of BlipModel(\n",
       "  (text_model): BlipTextModel(\n",
       "    (embeddings): BlipTextEmbeddings(\n",
       "      (word_embeddings): Embedding(30524, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): BlipTextEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BlipTextLayer(\n",
       "          (attention): BlipTextAttention(\n",
       "            (self): BlipTextSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): BlipTextSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): BlipTextAttention(\n",
       "            (self): BlipTextSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): BlipTextSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BlipTextIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BlipTextOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BlipTextPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (vision_model): BlipVisionModel(\n",
       "    (embeddings): BlipVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (encoder): BlipEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BlipEncoderLayer(\n",
       "          (self_attn): BlipAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): BlipMLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "  (text_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       ")>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_image_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_image_features(**input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_text_features(**tokens).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff3d4a8f85394c1caa188773ebf59df4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/6.96k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type blip-2 to instantiate a model of type blip. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5d6b32e9b1a4602b54e736597ae1cf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/122k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5e77d02da8246688b03aed794a2db99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c13e01af0a8e441798a9d7f55439bbb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00002.bin:   0%|          | 0.00/10.0G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdf1548e76064e789631dd001fe0aeb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00002.bin:   0%|          | 0.00/5.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6de5063f1a8443b6a73c84b905158ba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Salesforce/blip2-opt-2.7b were not used when initializing BlipModel: ['language_model.model.decoder.layers.8.self_attn.out_proj.weight', 'qformer.encoder.layer.7.attention.attention.key.weight', 'language_model.model.decoder.layers.6.final_layer_norm.weight', 'qformer.encoder.layer.11.output_query.dense.bias', 'qformer.encoder.layer.7.intermediate_query.dense.weight', 'qformer.encoder.layer.7.attention.attention.query.weight', 'qformer.encoder.layer.4.crossattention.attention.value.weight', 'language_model.model.decoder.layers.17.self_attn.k_proj.weight', 'language_model.model.decoder.layers.25.final_layer_norm.weight', 'language_model.model.decoder.layers.31.self_attn.q_proj.bias', 'language_model.model.decoder.layers.3.fc2.weight', 'language_model.model.decoder.layers.13.self_attn.v_proj.weight', 'language_model.model.decoder.layers.6.fc1.bias', 'language_model.model.decoder.layers.1.self_attn.q_proj.bias', 'language_model.model.decoder.layers.24.self_attn.q_proj.bias', 'qformer.encoder.layer.4.attention.attention.value.bias', 'qformer.encoder.layer.9.output_query.dense.weight', 'language_model.model.decoder.layers.16.self_attn.out_proj.weight', 'language_model.model.decoder.layers.10.fc1.bias', 'language_model.model.decoder.layers.18.self_attn.k_proj.weight', 'language_model.model.decoder.layers.12.self_attn.q_proj.weight', 'language_model.model.decoder.layers.12.self_attn.k_proj.weight', 'language_model.model.decoder.layers.2.final_layer_norm.weight', 'language_model.model.decoder.layers.1.self_attn.k_proj.weight', 'qformer.encoder.layer.6.crossattention.output.LayerNorm.weight', 'language_model.model.decoder.layers.29.self_attn.k_proj.weight', 'language_model.model.decoder.layers.28.self_attn.out_proj.bias', 'language_model.model.decoder.layers.10.final_layer_norm.bias', 'language_model.model.decoder.layers.26.fc1.bias', 'language_model.model.decoder.layers.29.fc1.bias', 'language_model.model.decoder.layers.13.fc2.weight', 'language_model.model.decoder.layers.10.final_layer_norm.weight', 'language_model.model.decoder.layers.24.self_attn.k_proj.bias', 'language_model.model.decoder.layers.4.self_attn_layer_norm.weight', 'language_model.model.decoder.layers.5.fc1.bias', 'qformer.encoder.layer.0.crossattention.output.LayerNorm.weight', 'qformer.encoder.layer.2.crossattention.attention.value.bias', 'qformer.encoder.layer.2.output_query.LayerNorm.weight', 'qformer.encoder.layer.10.output_query.LayerNorm.weight', 'language_model.model.decoder.layers.12.self_attn.out_proj.bias', 'language_model.model.decoder.layers.1.fc2.bias', 'language_model.model.decoder.layers.30.self_attn.q_proj.weight', 'language_model.model.decoder.layers.25.self_attn.v_proj.weight', 'qformer.encoder.layer.8.crossattention.attention.key.bias', 'qformer.encoder.layer.3.output_query.LayerNorm.bias', 'language_model.model.decoder.layers.8.fc2.bias', 'language_model.model.decoder.layers.9.fc2.bias', 'language_model.model.decoder.layers.23.self_attn.v_proj.bias', 'qformer.encoder.layer.6.attention.output.LayerNorm.weight', 'language_model.model.decoder.layers.14.self_attn.out_proj.bias', 'language_model.model.decoder.layers.11.self_attn.out_proj.bias', 'language_model.model.decoder.layers.6.self_attn.k_proj.bias', 'language_model.model.decoder.layers.12.self_attn.v_proj.bias', 'language_model.model.decoder.layers.6.self_attn.q_proj.bias', 'language_model.model.decoder.layers.21.self_attn.out_proj.weight', 'language_model.model.decoder.layers.7.self_attn_layer_norm.bias', 'qformer.encoder.layer.10.attention.attention.key.weight', 'language_model.model.decoder.layers.24.fc2.bias', 'language_model.model.decoder.layers.28.self_attn.k_proj.bias', 'qformer.encoder.layer.10.crossattention.attention.query.bias', 'qformer.encoder.layer.3.attention.output.LayerNorm.weight', 'qformer.encoder.layer.2.intermediate_query.dense.weight', 'language_model.model.decoder.layers.11.fc2.weight', 'language_model.model.decoder.layers.18.self_attn_layer_norm.bias', 'language_model.model.decoder.layers.20.self_attn.v_proj.weight', 'language_model.model.decoder.layers.10.fc1.weight', 'qformer.encoder.layer.2.crossattention.attention.key.bias', 'qformer.encoder.layer.1.attention.attention.key.bias', 'language_model.model.decoder.layers.8.fc1.weight', 'qformer.encoder.layer.11.intermediate_query.dense.bias', 'qformer.encoder.layer.5.attention.output.dense.weight', 'language_model.model.decoder.layers.10.self_attn.k_proj.bias', 'language_model.model.decoder.layers.14.self_attn.q_proj.weight', 'qformer.encoder.layer.7.output_query.dense.weight', 'language_model.model.decoder.layers.27.fc2.bias', 'language_model.model.decoder.layers.20.fc2.bias', 'language_model.model.decoder.layers.9.self_attn_layer_norm.bias', 'language_model.model.decoder.layers.7.self_attn.out_proj.bias', 'language_model.model.decoder.layers.2.fc1.weight', 'language_model.model.decoder.layers.20.self_attn_layer_norm.weight', 'language_model.model.decoder.layers.6.self_attn_layer_norm.weight', 'qformer.encoder.layer.0.crossattention.attention.query.bias', 'language_model.model.decoder.layers.16.self_attn.k_proj.bias', 'qformer.encoder.layer.2.attention.attention.query.bias', 'language_model.model.decoder.layers.9.self_attn.k_proj.weight', 'qformer.encoder.layer.3.output_query.dense.weight', 'language_model.model.decoder.layers.24.final_layer_norm.weight', 'qformer.encoder.layer.7.attention.attention.value.weight', 'qformer.encoder.layer.5.attention.attention.value.weight', 'language_model.model.decoder.layers.16.fc1.bias', 'language_model.model.decoder.layers.17.self_attn.q_proj.weight', 'qformer.encoder.layer.1.output_query.dense.bias', 'qformer.encoder.layer.0.attention.output.dense.weight', 'language_model.model.decoder.layers.17.self_attn.v_proj.weight', 'language_model.model.decoder.layers.16.final_layer_norm.weight', 'language_model.model.decoder.layers.20.fc2.weight', 'qformer.encoder.layer.0.attention.attention.key.weight', 'language_model.model.decoder.layers.0.self_attn.q_proj.weight', 'language_model.model.decoder.layers.13.self_attn.k_proj.weight', 'language_model.model.decoder.layers.29.self_attn.v_proj.bias', 'language_model.model.decoder.layers.23.self_attn_layer_norm.bias', 'language_model.model.decoder.layers.8.fc2.weight', 'language_model.model.decoder.layers.29.self_attn.q_proj.bias', 'qformer.encoder.layer.4.output_query.dense.bias', 'qformer.encoder.layer.0.attention.attention.query.weight', 'language_model.model.decoder.layers.15.self_attn.q_proj.bias', 'language_model.model.decoder.layers.27.self_attn.q_proj.weight', 'qformer.encoder.layer.10.attention.output.dense.weight', 'qformer.encoder.layer.4.attention.attention.query.bias', 'language_model.model.decoder.layers.18.final_layer_norm.bias', 'language_model.model.decoder.layers.22.self_attn.q_proj.weight', 'language_model.model.decoder.layers.25.fc1.bias', 'qformer.encoder.layer.9.attention.output.LayerNorm.weight', 'qformer.encoder.layer.10.output_query.dense.bias', 'language_model.model.decoder.layers.14.fc2.bias', 'qformer.encoder.layer.8.attention.attention.value.weight', 'qformer.encoder.layer.3.attention.attention.key.weight', 'language_model.model.decoder.layers.18.final_layer_norm.weight', 'qformer.encoder.layer.8.intermediate_query.dense.bias', 'language_model.model.decoder.layers.4.self_attn.q_proj.bias', 'language_model.model.decoder.layers.21.self_attn_layer_norm.weight', 'language_model.model.decoder.layers.0.final_layer_norm.weight', 'qformer.encoder.layer.10.attention.attention.query.bias', 'qformer.encoder.layer.6.crossattention.attention.key.bias', 'language_model.model.decoder.layers.24.self_attn.out_proj.weight', 'language_model.model.decoder.layers.31.self_attn.out_proj.bias', 'language_model.model.decoder.layers.11.fc1.weight', 'language_model.model.decoder.layers.12.self_attn.v_proj.weight', 'qformer.encoder.layer.4.crossattention.output.dense.bias', 'language_model.model.decoder.layers.10.self_attn_layer_norm.weight', 'qformer.encoder.layer.4.crossattention.output.LayerNorm.bias', 'qformer.encoder.layer.1.attention.output.dense.weight', 'qformer.encoder.layer.10.attention.attention.key.bias', 'language_model.model.decoder.layers.10.fc2.weight', 'language_model.model.decoder.layers.27.fc2.weight', 'language_model.model.decoder.layers.25.self_attn.v_proj.bias', 'language_model.model.decoder.layers.8.self_attn.k_proj.weight', 'qformer.encoder.layer.1.output_query.LayerNorm.weight', 'language_model.model.decoder.layers.10.self_attn.k_proj.weight', 'language_model.model.decoder.layers.27.self_attn.v_proj.bias', 'language_model.model.decoder.layers.5.self_attn_layer_norm.weight', 'qformer.encoder.layer.5.output_query.dense.bias', 'language_model.model.decoder.layers.28.final_layer_norm.bias', 'qformer.encoder.layer.0.attention.attention.key.bias', 'language_model.model.decoder.layers.23.fc1.bias', 'language_model.model.decoder.layers.10.self_attn.q_proj.weight', 'language_model.model.decoder.layers.24.self_attn_layer_norm.weight', 'language_model.model.decoder.layers.31.self_attn.k_proj.bias', 'language_model.model.decoder.layers.15.fc1.bias', 'qformer.encoder.layer.4.attention.output.LayerNorm.weight', 'language_model.model.decoder.layers.22.self_attn.out_proj.weight', 'language_model.model.decoder.layers.13.self_attn.v_proj.bias', 'language_model.model.decoder.layers.0.fc2.weight', 'language_model.model.decoder.layers.10.self_attn_layer_norm.bias', 'language_model.model.decoder.layers.22.self_attn.v_proj.weight', 'language_model.model.decoder.layers.2.self_attn.out_proj.bias', 'language_model.model.decoder.layers.30.self_attn.q_proj.bias', 'language_model.model.decoder.layers.5.fc2.weight', 'language_model.model.decoder.layers.25.self_attn.out_proj.bias', 'language_model.model.decoder.layers.3.self_attn.q_proj.bias', 'qformer.encoder.layer.5.attention.output.dense.bias', 'qformer.encoder.layer.4.attention.output.dense.weight', 'language_model.model.decoder.layers.8.self_attn_layer_norm.weight', 'language_model.model.decoder.layers.7.fc1.weight', 'qformer.encoder.layer.9.attention.output.dense.bias', 'qformer.encoder.layer.6.attention.output.LayerNorm.bias', 'qformer.encoder.layer.6.crossattention.attention.value.bias', 'language_model.model.decoder.layers.3.self_attn.out_proj.weight', 'language_model.model.decoder.layers.16.final_layer_norm.bias', 'language_model.model.decoder.layers.30.self_attn_layer_norm.weight', 'qformer.encoder.layer.9.attention.attention.value.weight', 'language_model.model.decoder.layers.18.self_attn.v_proj.weight', 'language_model.model.decoder.layers.15.self_attn.v_proj.weight', 'language_model.model.decoder.layers.0.self_attn.q_proj.bias', 'language_model.model.decoder.layers.23.self_attn.out_proj.bias', 'language_model.model.decoder.layers.23.self_attn_layer_norm.weight', 'qformer.encoder.layer.5.attention.attention.key.bias', 'language_model.model.decoder.layers.16.fc2.weight', 'qformer.encoder.layer.7.attention.attention.query.bias', 'qformer.encoder.layer.2.attention.attention.key.bias', 'qformer.encoder.layer.8.attention.attention.query.weight', 'language_model.model.decoder.layers.0.fc1.weight', 'language_model.model.decoder.layers.28.self_attn.k_proj.weight', 'language_model.model.decoder.layers.29.self_attn.v_proj.weight', 'qformer.encoder.layer.10.crossattention.attention.query.weight', 'qformer.encoder.layer.8.attention.attention.key.weight', 'language_model.model.decoder.layers.11.self_attn.k_proj.weight', 'language_model.model.decoder.layers.6.self_attn.out_proj.weight', 'language_model.model.decoder.layers.10.self_attn.out_proj.bias', 'qformer.encoder.layer.11.attention.attention.key.weight', 'language_model.model.decoder.layers.26.self_attn_layer_norm.weight', 'language_model.model.decoder.layers.21.self_attn.k_proj.bias', 'language_model.model.decoder.layers.13.fc2.bias', 'qformer.encoder.layer.7.attention.attention.key.bias', 'language_model.model.decoder.layers.20.fc1.weight', 'language_model.model.decoder.layers.0.fc2.bias', 'language_model.model.decoder.layers.15.self_attn_layer_norm.weight', 'language_model.model.decoder.layers.5.self_attn.out_proj.bias', 'qformer.encoder.layer.11.attention.attention.value.weight', 'language_model.model.decoder.layers.19.fc1.weight', 'language_model.model.decoder.layers.15.self_attn.k_proj.weight', 'language_model.model.decoder.layers.17.self_attn.v_proj.bias', 'language_model.model.decoder.layers.19.self_attn.k_proj.bias', 'language_model.model.decoder.layers.4.fc1.bias', 'qformer.encoder.layer.11.attention.output.LayerNorm.bias', 'qformer.encoder.layer.11.output_query.LayerNorm.bias', 'language_model.model.decoder.layers.0.self_attn.k_proj.bias', 'language_model.model.decoder.layers.5.final_layer_norm.weight', 'language_model.model.decoder.layers.5.fc1.weight', 'qformer.encoder.layer.6.crossattention.attention.key.weight', 'language_model.model.decoder.layers.29.self_attn_layer_norm.bias', 'qformer.encoder.layer.0.crossattention.attention.key.weight', 'qformer.encoder.layer.6.output_query.dense.bias', 'language_model.model.decoder.layers.15.self_attn.out_proj.bias', 'qformer.encoder.layer.10.attention.output.LayerNorm.weight', 'qformer.encoder.layer.0.output_query.LayerNorm.bias', 'qformer.encoder.layer.5.attention.attention.value.bias', 'language_model.model.decoder.layers.27.final_layer_norm.weight', 'language_model.model.decoder.layers.27.fc1.weight', 'language_model.model.decoder.layers.28.self_attn.q_proj.weight', 'language_model.model.decoder.layers.3.self_attn.k_proj.weight', 'language_model.model.decoder.layers.11.final_layer_norm.bias', 'language_model.model.decoder.layers.22.self_attn.k_proj.weight', 'language_model.model.decoder.layers.23.self_attn.q_proj.bias', 'language_model.model.decoder.layers.27.self_attn.k_proj.bias', 'language_model.model.decoder.layers.18.fc2.weight', 'language_model.model.decoder.layers.9.self_attn.q_proj.bias', 'qformer.encoder.layer.4.crossattention.output.LayerNorm.weight', 'language_model.model.decoder.layers.12.fc2.weight', 'language_model.model.decoder.layers.6.self_attn.out_proj.bias', 'qformer.encoder.layer.5.intermediate_query.dense.weight', 'language_model.model.decoder.layers.2.self_attn.v_proj.bias', 'language_model.model.decoder.layers.14.self_attn_layer_norm.bias', 'language_model.model.decoder.layers.30.self_attn.out_proj.weight', 'language_model.model.decoder.layers.6.fc2.weight', 'language_model.model.decoder.layers.20.fc1.bias', 'qformer.encoder.layer.6.attention.attention.key.weight', 'language_model.model.decoder.layers.8.self_attn.v_proj.weight', 'language_model.model.decoder.layers.17.final_layer_norm.weight', 'language_model.model.decoder.layers.7.self_attn.v_proj.weight', 'language_model.model.decoder.layers.16.self_attn_layer_norm.weight', 'language_model.model.decoder.layers.23.self_attn.v_proj.weight', 'language_model.model.decoder.layers.7.final_layer_norm.weight', 'language_model.model.decoder.layers.2.self_attn.v_proj.weight', 'language_model.model.decoder.layers.1.self_attn_layer_norm.bias', 'language_model.model.decoder.layers.14.fc1.bias', 'language_model.model.decoder.layers.0.self_attn.k_proj.weight', 'language_model.model.decoder.layers.7.self_attn.q_proj.bias', 'language_model.model.decoder.layers.29.fc2.bias', 'qformer.encoder.layer.6.intermediate_query.dense.weight', 'language_model.model.decoder.layers.4.self_attn.k_proj.weight', 'language_model.model.decoder.layers.17.final_layer_norm.bias', 'language_model.model.decoder.layers.11.fc2.bias', 'language_model.model.decoder.layers.31.self_attn.v_proj.bias', 'qformer.encoder.layer.4.crossattention.output.dense.weight', 'qformer.encoder.layer.10.crossattention.output.dense.weight', 'qformer.encoder.layer.2.intermediate_query.dense.bias', 'qformer.encoder.layer.10.crossattention.attention.key.weight', 'qformer.encoder.layer.0.crossattention.attention.query.weight', 'qformer.encoder.layer.6.attention.output.dense.weight', 'qformer.encoder.layer.8.attention.attention.key.bias', 'language_model.model.decoder.layers.18.fc2.bias', 'language_model.model.decoder.layers.9.fc2.weight', 'language_model.model.decoder.layers.0.self_attn.v_proj.bias', 'language_model.model.decoder.layers.23.self_attn.q_proj.weight', 'language_model.model.decoder.layers.18.self_attn.q_proj.bias', 'language_model.model.decoder.layers.16.fc1.weight', 'language_model.model.decoder.layers.16.self_attn.v_proj.bias', 'qformer.encoder.layer.0.attention.output.LayerNorm.bias', 'language_model.model.decoder.layers.25.self_attn_layer_norm.weight', 'qformer.encoder.layer.7.attention.output.dense.weight', 'language_model.model.decoder.layers.27.self_attn_layer_norm.weight', 'language_model.model.decoder.layers.21.self_attn.out_proj.bias', 'language_model.model.decoder.layers.13.self_attn.out_proj.bias', 'language_model.model.decoder.layers.13.fc1.weight', 'qformer.encoder.layer.8.output_query.dense.bias', 'language_model.model.decoder.layers.28.self_attn.v_proj.bias', 'qformer.encoder.layer.3.attention.output.dense.weight', 'language_model.model.decoder.layers.7.self_attn_layer_norm.weight', 'language_model.model.decoder.layers.9.self_attn_layer_norm.weight', 'qformer.encoder.layer.0.output_query.dense.weight', 'language_model.model.decoder.layers.31.fc1.bias', 'language_model.model.decoder.layers.1.self_attn.out_proj.weight', 'language_model.model.decoder.layers.10.self_attn.out_proj.weight', 'language_model.model.decoder.layers.30.final_layer_norm.bias', 'language_model.model.decoder.layers.5.final_layer_norm.bias', 'qformer.encoder.layer.9.attention.attention.query.bias', 'language_model.model.decoder.layers.10.self_attn.v_proj.weight', 'qformer.encoder.layer.8.attention.attention.value.bias', 'language_model.model.decoder.layers.9.self_attn.q_proj.weight', 'qformer.encoder.layer.5.output_query.LayerNorm.bias', 'qformer.encoder.layer.6.output_query.dense.weight', 'language_model.model.decoder.layers.21.fc2.weight', 'qformer.encoder.layer.8.output_query.dense.weight', 'language_model.model.decoder.layers.17.self_attn_layer_norm.bias', 'qformer.encoder.layer.5.output_query.LayerNorm.weight', 'language_model.model.decoder.layers.25.self_attn_layer_norm.bias', 'language_model.model.decoder.layers.30.final_layer_norm.weight', 'language_model.model.decoder.layers.18.self_attn.out_proj.bias', 'qformer.encoder.layer.0.intermediate_query.dense.bias', 'language_model.model.decoder.layers.5.self_attn_layer_norm.bias', 'language_model.model.decoder.layers.11.fc1.bias', 'qformer.encoder.layer.4.output_query.dense.weight', 'language_model.model.decoder.layers.27.self_attn.k_proj.weight', 'qformer.encoder.layer.5.intermediate_query.dense.bias', 'language_model.model.decoder.layers.2.fc2.weight', 'language_model.model.decoder.layers.12.self_attn.k_proj.bias', 'qformer.encoder.layer.6.crossattention.attention.query.weight', 'qformer.encoder.layer.9.intermediate_query.dense.bias', 'language_model.model.decoder.layers.0.self_attn_layer_norm.weight', 'qformer.encoder.layer.10.crossattention.output.dense.bias', 'language_model.model.decoder.layers.1.fc1.bias', 'language_model.model.decoder.layers.24.final_layer_norm.bias', 'language_model.model.decoder.layers.7.self_attn.k_proj.weight', 'qformer.encoder.layer.0.attention.attention.value.bias', 'qformer.encoder.layer.3.attention.attention.value.weight', 'language_model.model.decoder.layers.21.self_attn.v_proj.weight', 'language_model.model.decoder.layers.10.self_attn.q_proj.bias', 'language_model.model.decoder.layers.29.self_attn.q_proj.weight', 'qformer.encoder.layer.7.output_query.LayerNorm.bias', 'qformer.encoder.layer.2.crossattention.attention.key.weight', 'qformer.encoder.layer.10.crossattention.output.LayerNorm.weight', 'language_model.model.decoder.layers.29.final_layer_norm.weight', 'qformer.layernorm.bias', 'qformer.encoder.layer.7.intermediate_query.dense.bias', 'language_model.model.decoder.layers.13.self_attn_layer_norm.weight', 'language_model.model.decoder.layers.26.self_attn.v_proj.weight', 'qformer.encoder.layer.9.output_query.dense.bias', 'language_model.model.decoder.layers.28.self_attn_layer_norm.bias', 'language_model.model.decoder.layers.11.self_attn_layer_norm.weight', 'language_model.model.decoder.layers.31.self_attn.v_proj.weight', 'language_model.model.decoder.layers.22.self_attn.q_proj.bias', 'qformer.encoder.layer.3.intermediate_query.dense.bias', 'language_model.model.decoder.layers.1.self_attn.k_proj.bias', 'language_model.model.decoder.layers.30.self_attn.out_proj.bias', 'language_model.model.decoder.layers.30.self_attn.k_proj.bias', 'qformer.encoder.layer.2.output_query.dense.weight', 'qformer.encoder.layer.3.attention.output.LayerNorm.bias', 'language_model.model.decoder.layers.23.fc2.bias', 'language_model.model.decoder.layers.20.self_attn.k_proj.bias', 'qformer.encoder.layer.6.crossattention.output.dense.bias', 'language_model.model.decoder.layers.10.self_attn.v_proj.bias', 'language_model.model.decoder.layers.26.self_attn.k_proj.weight', 'language_model.model.decoder.layers.25.self_attn.k_proj.bias', 'language_model.model.decoder.layers.2.final_layer_norm.bias', 'language_model.model.decoder.layers.20.self_attn.k_proj.weight', 'language_model.model.decoder.layers.29.fc1.weight', 'qformer.encoder.layer.5.attention.attention.query.weight', 'language_model.model.decoder.layers.18.fc1.weight', 'qformer.encoder.layer.7.output_query.dense.bias', 'language_model.model.decoder.layers.30.fc1.bias', 'language_model.model.decoder.layers.10.fc2.bias', 'language_model.model.decoder.layers.3.fc2.bias', 'language_model.model.decoder.layers.0.fc1.bias', 'language_model.model.decoder.layers.27.self_attn.q_proj.bias', 'language_model.model.decoder.layers.31.self_attn_layer_norm.weight', 'language_model.model.decoder.layers.25.fc2.bias', 'language_model.model.decoder.layers.21.fc1.bias', 'language_model.model.decoder.layers.13.self_attn_layer_norm.bias', 'language_model.model.decoder.layers.3.self_attn.v_proj.weight', 'language_model.model.decoder.layers.23.final_layer_norm.weight', 'language_model.model.decoder.layers.5.self_attn.out_proj.weight', 'language_model.model.decoder.layers.21.final_layer_norm.weight', 'qformer.encoder.layer.4.crossattention.attention.key.bias', 'language_model.model.decoder.layers.14.self_attn.k_proj.weight', 'language_model.model.decoder.layers.0.self_attn_layer_norm.bias', 'language_model.model.decoder.layers.4.self_attn.k_proj.bias', 'language_model.model.decoder.layers.27.self_attn_layer_norm.bias', 'language_model.model.decoder.layers.8.self_attn.v_proj.bias', 'language_model.model.decoder.layers.12.fc2.bias', 'qformer.encoder.layer.8.crossattention.attention.query.bias', 'language_model.model.decoder.layers.0.self_attn.out_proj.bias', 'language_model.model.decoder.layers.24.self_attn_layer_norm.bias', 'qformer.encoder.layer.0.crossattention.output.dense.bias', 'language_model.model.decoder.embed_positions.weight', 'qformer.encoder.layer.2.crossattention.attention.value.weight', 'language_model.model.decoder.layers.4.self_attn.v_proj.bias', 'qformer.encoder.layer.9.attention.attention.key.bias', 'language_model.model.decoder.layers.28.self_attn.v_proj.weight', 'language_model.model.decoder.layers.23.self_attn.k_proj.weight', 'language_model.model.decoder.layers.21.self_attn.k_proj.weight', 'language_model.model.decoder.layers.0.self_attn.out_proj.weight', 'qformer.encoder.layer.0.crossattention.attention.value.weight', 'language_model.model.decoder.layers.6.self_attn.v_proj.bias', 'language_model.model.decoder.layers.21.final_layer_norm.bias', 'qformer.encoder.layer.7.output_query.LayerNorm.weight', 'language_model.model.decoder.layers.16.self_attn.k_proj.weight', 'language_model.model.decoder.layers.21.self_attn.q_proj.bias', 'qformer.encoder.layer.3.attention.attention.value.bias', 'qformer.encoder.layer.11.output_query.LayerNorm.weight', 'qformer.encoder.layer.8.attention.attention.query.bias', 'language_model.model.decoder.layers.25.fc1.weight', 'qformer.encoder.layer.1.output_query.dense.weight', 'qformer.encoder.layer.0.crossattention.attention.value.bias', 'language_model.model.decoder.layers.22.self_attn_layer_norm.bias', 'language_model.model.decoder.layers.0.self_attn.v_proj.weight', 'language_model.model.decoder.layers.30.self_attn.k_proj.weight', 'qformer.encoder.layer.8.attention.output.dense.bias', 'language_model.model.decoder.layers.13.final_layer_norm.bias', 'language_model.model.decoder.layers.31.fc2.bias', 'qformer.encoder.layer.4.intermediate_query.dense.bias', 'language_projection.bias', 'language_model.model.decoder.layers.26.fc2.bias', 'language_model.model.decoder.layers.11.self_attn.out_proj.weight', 'qformer.encoder.layer.11.attention.output.LayerNorm.weight', 'language_model.model.decoder.layers.30.self_attn.v_proj.weight', 'qformer.encoder.layer.8.crossattention.output.dense.weight', 'qformer.encoder.layer.8.intermediate_query.dense.weight', 'qformer.encoder.layer.5.attention.attention.key.weight', 'qformer.encoder.layer.1.output_query.LayerNorm.bias', 'language_model.model.decoder.layers.11.self_attn.q_proj.weight', 'qformer.encoder.layer.8.crossattention.output.LayerNorm.bias', 'language_model.model.decoder.layers.20.final_layer_norm.bias', 'language_model.model.decoder.layers.23.self_attn.k_proj.bias', 'qformer.encoder.layer.11.attention.attention.value.bias', 'language_model.model.decoder.layers.14.fc1.weight', 'qformer.encoder.layer.11.intermediate_query.dense.weight', 'language_model.model.decoder.layers.4.fc2.bias', 'language_model.model.decoder.layers.6.self_attn.v_proj.weight', 'qformer.encoder.layer.4.output_query.LayerNorm.weight', 'qformer.encoder.layer.6.attention.attention.value.weight', 'language_model.model.decoder.layers.17.self_attn.q_proj.bias', 'language_model.model.decoder.layers.28.fc1.weight', 'language_model.model.decoder.layers.12.fc1.bias', 'qformer.encoder.layer.10.attention.output.LayerNorm.bias', 'qformer.encoder.layer.7.attention.output.dense.bias', 'qformer.encoder.layer.11.attention.output.dense.bias', 'qformer.encoder.layer.1.attention.attention.value.weight', 'language_model.model.decoder.layers.5.self_attn.v_proj.bias', 'qformer.encoder.layer.2.attention.output.dense.weight', 'language_model.model.decoder.layers.15.self_attn.out_proj.weight', 'qformer.encoder.layer.9.output_query.LayerNorm.weight', 'language_model.model.decoder.layers.5.self_attn.q_proj.bias', 'language_model.model.decoder.layers.28.self_attn_layer_norm.weight', 'language_model.model.decoder.layers.15.final_layer_norm.weight', 'language_model.model.decoder.layers.3.final_layer_norm.weight', 'language_model.model.decoder.layers.19.self_attn.k_proj.weight', 'language_model.lm_head.weight', 'language_model.model.decoder.layers.9.self_attn.v_proj.weight', 'language_model.model.decoder.layers.18.self_attn.q_proj.weight', 'language_model.model.decoder.layers.26.final_layer_norm.weight', 'language_model.model.decoder.layers.31.self_attn.q_proj.weight', 'language_model.model.decoder.layers.5.fc2.bias', 'qformer.encoder.layer.4.attention.output.dense.bias', 'qformer.encoder.layer.1.attention.attention.value.bias', 'qformer.encoder.layer.11.attention.attention.key.bias', 'language_model.model.decoder.layers.8.self_attn.q_proj.bias', 'language_model.model.decoder.layers.25.self_attn.out_proj.weight', 'language_model.model.decoder.layers.23.self_attn.out_proj.weight', 'language_model.model.decoder.layers.29.self_attn.out_proj.bias', 'language_model.model.decoder.final_layer_norm.bias', 'language_model.model.decoder.layers.20.self_attn.q_proj.weight', 'language_model.model.decoder.layers.30.self_attn_layer_norm.bias', 'qformer.encoder.layer.9.attention.attention.query.weight', 'language_model.model.decoder.layers.8.fc1.bias', 'qformer.encoder.layer.2.output_query.dense.bias', 'language_model.model.decoder.layers.15.self_attn.q_proj.weight', 'qformer.encoder.layer.4.attention.attention.key.weight', 'language_model.model.decoder.layers.7.final_layer_norm.bias', 'qformer.encoder.layer.6.attention.output.dense.bias', 'language_model.model.decoder.layers.20.self_attn.out_proj.bias', 'language_model.model.decoder.layers.19.fc1.bias', 'language_model.model.decoder.layers.17.fc1.weight', 'language_model.model.decoder.layers.7.self_attn.k_proj.bias', 'language_model.model.decoder.layers.14.self_attn.v_proj.bias', 'language_model.model.decoder.layers.9.self_attn.k_proj.bias', 'language_model.model.decoder.layers.26.self_attn.k_proj.bias', 'language_model.model.decoder.layers.27.self_attn.out_proj.bias', 'language_model.model.decoder.layers.21.self_attn.v_proj.bias', 'language_model.model.decoder.layers.6.self_attn.k_proj.weight', 'language_model.model.decoder.layers.13.final_layer_norm.weight', 'language_model.model.decoder.layers.29.final_layer_norm.bias', 'qformer.encoder.layer.8.crossattention.attention.query.weight', 'language_model.model.decoder.layers.30.fc2.bias', 'language_model.model.decoder.layers.8.self_attn.k_proj.bias', 'qformer.encoder.layer.5.output_query.dense.weight', 'qformer.encoder.layer.9.attention.output.dense.weight', 'language_model.model.decoder.layers.28.fc2.weight', 'language_model.model.decoder.layers.24.fc1.bias', 'language_model.model.decoder.layers.18.self_attn.out_proj.weight', 'qformer.encoder.layer.11.attention.attention.query.weight', 'language_model.model.decoder.layers.16.self_attn.out_proj.bias', 'language_model.model.decoder.layers.15.self_attn.v_proj.bias', 'language_model.model.decoder.layers.16.self_attn_layer_norm.bias', 'language_model.model.decoder.layers.22.self_attn.out_proj.bias', 'language_model.model.decoder.layers.24.fc1.weight', 'language_model.model.decoder.layers.12.self_attn_layer_norm.weight', 'language_model.model.decoder.layers.28.fc1.bias', 'qformer.encoder.layer.3.output_query.dense.bias', 'language_model.model.decoder.layers.15.self_attn.k_proj.bias', 'language_model.model.decoder.layers.3.self_attn.out_proj.bias', 'language_model.model.decoder.layers.21.fc1.weight', 'language_model.model.decoder.layers.25.fc2.weight', 'qformer.encoder.layer.11.attention.output.dense.weight', 'language_model.model.decoder.layers.8.self_attn.q_proj.weight', 'language_model.model.decoder.layers.7.fc1.bias', 'language_model.model.decoder.layers.9.final_layer_norm.bias', 'qformer.encoder.layer.6.crossattention.output.dense.weight', 'qformer.encoder.layer.10.attention.attention.query.weight', 'language_model.model.decoder.layers.21.fc2.bias', 'language_model.model.decoder.layers.12.self_attn.q_proj.bias', 'language_model.model.decoder.layers.6.self_attn.q_proj.weight', 'language_model.model.decoder.layers.27.fc1.bias', 'qformer.encoder.layer.2.attention.attention.value.bias', 'qformer.encoder.layer.8.crossattention.output.dense.bias', 'qformer.encoder.layer.9.intermediate_query.dense.weight', 'language_model.model.decoder.layers.30.fc1.weight', 'qformer.encoder.layer.10.output_query.dense.weight', 'language_model.model.decoder.layers.1.self_attn.v_proj.weight', 'language_model.model.decoder.layers.4.self_attn_layer_norm.bias', 'language_model.model.decoder.layers.19.final_layer_norm.weight', 'language_model.model.decoder.layers.5.self_attn.k_proj.bias', 'qformer.encoder.layer.8.crossattention.output.LayerNorm.weight', 'language_model.model.decoder.layers.6.final_layer_norm.bias', 'qformer.encoder.layer.1.intermediate_query.dense.bias', 'qformer.encoder.layer.0.crossattention.attention.key.bias', 'qformer.encoder.layer.6.attention.attention.key.bias', 'language_model.model.decoder.layers.15.fc2.weight', 'language_model.model.decoder.layers.4.final_layer_norm.weight', 'qformer.encoder.layer.8.crossattention.attention.key.weight', 'language_model.model.decoder.layers.8.self_attn_layer_norm.bias', 'qformer.encoder.layer.4.crossattention.attention.query.bias', 'qformer.encoder.layer.4.attention.attention.key.bias', 'language_model.model.decoder.layers.2.self_attn.q_proj.bias', 'language_model.model.decoder.layers.18.self_attn_layer_norm.weight', 'qformer.encoder.layer.11.output_query.dense.weight', 'language_model.model.decoder.layers.31.fc2.weight', 'language_model.model.decoder.layers.22.fc2.weight', 'language_model.model.decoder.layers.28.self_attn.out_proj.weight', 'language_model.model.decoder.layers.19.self_attn.v_proj.bias', 'language_model.model.decoder.layers.27.self_attn.v_proj.weight', 'language_model.model.decoder.layers.3.self_attn_layer_norm.weight', 'language_model.model.decoder.layers.8.self_attn.out_proj.bias', 'language_model.model.decoder.layers.14.self_attn_layer_norm.weight', 'language_model.model.decoder.layers.29.self_attn_layer_norm.weight', 'language_model.model.decoder.layers.20.final_layer_norm.weight', 'language_model.model.decoder.layers.29.self_attn.k_proj.bias', 'language_model.model.decoder.layers.16.self_attn.q_proj.bias', 'language_model.model.decoder.layers.17.self_attn_layer_norm.weight', 'language_model.model.decoder.layers.2.self_attn.k_proj.bias', 'qformer.encoder.layer.1.attention.output.LayerNorm.weight', 'qformer.encoder.layer.6.crossattention.attention.query.bias', 'language_model.model.decoder.layers.8.final_layer_norm.bias', 'qformer.layernorm.weight', 'language_model.model.decoder.layers.2.fc1.bias', 'qformer.encoder.layer.11.attention.attention.query.bias', 'qformer.encoder.layer.2.crossattention.output.dense.weight', 'language_model.model.decoder.layers.14.self_attn.out_proj.weight', 'language_model.model.decoder.layers.2.self_attn_layer_norm.weight', 'qformer.encoder.layer.2.attention.output.LayerNorm.bias', 'language_model.model.decoder.layers.8.final_layer_norm.weight', 'language_model.model.decoder.layers.4.final_layer_norm.bias', 'language_model.model.decoder.layers.29.fc2.weight', 'qformer.encoder.layer.0.output_query.LayerNorm.weight', 'language_model.model.decoder.layers.22.self_attn.k_proj.bias', 'qformer.encoder.layer.3.intermediate_query.dense.weight', 'language_model.model.decoder.layers.4.fc1.weight', 'language_model.model.decoder.layers.0.final_layer_norm.bias', 'language_model.model.decoder.layers.14.self_attn.q_proj.bias', 'language_model.model.decoder.layers.19.self_attn.out_proj.weight', 'qformer.encoder.layer.3.output_query.LayerNorm.weight', 'language_model.model.decoder.layers.11.self_attn_layer_norm.bias', 'qformer.encoder.layer.9.attention.attention.key.weight', 'language_model.model.decoder.layers.5.self_attn.k_proj.weight', 'language_model.model.decoder.layers.17.self_attn.k_proj.bias', 'qformer.encoder.layer.1.attention.attention.query.weight', 'qformer.encoder.layer.8.output_query.LayerNorm.weight', 'language_model.model.decoder.layers.11.self_attn.v_proj.bias', 'language_model.model.decoder.layers.3.self_attn_layer_norm.bias', 'language_model.model.decoder.layers.2.self_attn.out_proj.weight', 'language_model.model.decoder.layers.19.fc2.weight', 'qformer.encoder.layer.9.attention.output.LayerNorm.bias', 'qformer.encoder.layer.6.output_query.LayerNorm.weight', 'qformer.encoder.layer.2.crossattention.attention.query.bias', 'language_model.model.decoder.layers.5.self_attn.v_proj.weight', 'qformer.encoder.layer.0.intermediate_query.dense.weight', 'language_model.model.decoder.layers.24.self_attn.v_proj.weight', 'language_model.model.decoder.layers.6.self_attn_layer_norm.bias', 'language_model.model.decoder.layers.15.fc1.weight', 'language_model.model.decoder.layers.7.self_attn.v_proj.bias', 'qformer.encoder.layer.10.intermediate_query.dense.bias', 'language_model.model.decoder.layers.9.self_attn.out_proj.weight', 'language_model.model.decoder.layers.4.self_attn.out_proj.bias', 'language_model.model.decoder.layers.26.self_attn.v_proj.bias', 'language_model.model.decoder.layers.3.final_layer_norm.bias', 'language_model.model.decoder.layers.19.self_attn.out_proj.bias', 'language_model.model.decoder.layers.19.self_attn.q_proj.bias', 'language_model.model.decoder.layers.30.fc2.weight', 'language_model.model.decoder.layers.16.self_attn.q_proj.weight', 'language_model.model.decoder.layers.22.self_attn_layer_norm.weight', 'qformer.encoder.layer.7.attention.output.LayerNorm.bias', 'language_model.model.decoder.layers.31.final_layer_norm.bias', 'qformer.encoder.layer.2.attention.attention.key.weight', 'language_model.model.decoder.layers.24.self_attn.k_proj.weight', 'qformer.encoder.layer.10.output_query.LayerNorm.bias', 'language_model.model.decoder.layers.2.self_attn_layer_norm.bias', 'language_model.model.decoder.layers.1.final_layer_norm.bias', 'qformer.encoder.layer.10.crossattention.attention.value.weight', 'language_model.model.decoder.layers.22.self_attn.v_proj.bias', 'language_model.model.decoder.layers.13.self_attn.q_proj.bias', 'language_model.model.decoder.layers.28.self_attn.q_proj.bias', 'language_model.model.decoder.layers.1.self_attn.q_proj.weight', 'language_model.model.decoder.layers.26.self_attn.out_proj.bias', 'language_model.model.decoder.layers.1.self_attn.v_proj.bias', 'language_model.model.decoder.layers.20.self_attn.out_proj.weight', 'language_model.model.decoder.layers.31.self_attn.k_proj.weight', 'qformer.encoder.layer.2.crossattention.output.dense.bias', 'language_model.model.decoder.layers.25.self_attn.q_proj.bias', 'language_model.model.decoder.layers.26.self_attn.out_proj.weight', 'language_model.model.decoder.layers.19.fc2.bias', 'qformer.encoder.layer.8.attention.output.LayerNorm.bias', 'qformer.encoder.layer.7.attention.output.LayerNorm.weight', 'qformer.encoder.layer.9.output_query.LayerNorm.bias', 'language_model.model.decoder.layers.23.final_layer_norm.bias', 'language_model.model.decoder.layers.9.fc1.bias', 'language_model.model.decoder.layers.14.self_attn.v_proj.weight', 'language_model.model.decoder.layers.16.fc2.bias', 'language_model.model.decoder.layers.19.self_attn_layer_norm.weight', 'language_model.model.decoder.layers.31.self_attn_layer_norm.bias', 'language_model.model.decoder.layers.19.self_attn.q_proj.weight', 'qformer.encoder.layer.6.crossattention.attention.value.weight', 'language_model.model.decoder.layers.20.self_attn.v_proj.bias', 'qformer.encoder.layer.2.attention.attention.query.weight', 'language_model.model.decoder.layers.12.self_attn_layer_norm.bias', 'qformer.encoder.layer.8.attention.output.dense.weight', 'qformer.encoder.layer.0.attention.attention.query.bias', 'language_model.model.decoder.layers.12.final_layer_norm.weight', 'qformer.encoder.layer.5.attention.output.LayerNorm.weight', 'language_model.model.decoder.layers.17.fc2.bias', 'language_model.model.decoder.layers.1.fc1.weight', 'language_model.model.decoder.layers.26.self_attn.q_proj.bias', 'qformer.encoder.layer.2.output_query.LayerNorm.bias', 'language_model.model.decoder.layers.25.self_attn.k_proj.weight', 'language_model.model.decoder.layers.13.self_attn.out_proj.weight', 'qformer.encoder.layer.4.intermediate_query.dense.weight', 'qformer.encoder.layer.9.attention.attention.value.bias', 'language_model.model.decoder.layers.22.fc2.bias', 'query_tokens', 'language_model.model.decoder.layers.22.fc1.weight', 'language_model.model.decoder.layers.24.self_attn.v_proj.bias', 'qformer.encoder.layer.6.output_query.LayerNorm.bias', 'qformer.encoder.layer.10.attention.attention.value.bias', 'language_model.model.decoder.layers.22.final_layer_norm.weight', 'language_model.model.decoder.layers.26.self_attn.q_proj.weight', 'language_model.model.decoder.layers.23.fc1.weight', 'qformer.encoder.layer.2.crossattention.output.LayerNorm.bias', 'language_model.model.decoder.layers.18.fc1.bias', 'language_model.model.decoder.layers.1.self_attn_layer_norm.weight', 'qformer.encoder.layer.0.attention.output.dense.bias', 'language_model.model.decoder.layers.7.self_attn.q_proj.weight', 'language_model.model.decoder.layers.13.self_attn.k_proj.bias', 'qformer.encoder.layer.8.attention.output.LayerNorm.weight', 'language_model.model.decoder.layers.30.self_attn.v_proj.bias', 'qformer.encoder.layer.8.crossattention.attention.value.bias', 'language_model.model.decoder.layers.2.self_attn.q_proj.weight', 'language_model.model.decoder.layers.7.fc2.weight', 'language_projection.weight', 'language_model.model.decoder.layers.1.fc2.weight', 'language_model.model.decoder.layers.19.final_layer_norm.bias', 'language_model.model.decoder.layers.26.self_attn_layer_norm.bias', 'qformer.encoder.layer.2.crossattention.output.LayerNorm.weight', 'language_model.model.decoder.layers.13.fc1.bias', 'qformer.encoder.layer.3.attention.attention.query.bias', 'language_model.model.decoder.layers.3.self_attn.q_proj.weight', 'language_model.model.decoder.layers.18.self_attn.k_proj.bias', 'qformer.encoder.layer.1.attention.attention.key.weight', 'qformer.encoder.layer.10.attention.attention.value.weight', 'qformer.encoder.layer.0.attention.attention.value.weight', 'language_model.model.decoder.layers.9.fc1.weight', 'qformer.encoder.layer.4.crossattention.attention.query.weight', 'qformer.encoder.layer.3.attention.attention.key.bias', 'language_model.model.decoder.layers.24.self_attn.q_proj.weight', 'language_model.model.decoder.layers.19.self_attn_layer_norm.bias', 'qformer.encoder.layer.10.intermediate_query.dense.weight', 'language_model.model.decoder.layers.2.self_attn.k_proj.weight', 'language_model.model.decoder.layers.31.self_attn.out_proj.weight', 'language_model.model.decoder.layers.27.final_layer_norm.bias', 'language_model.model.decoder.layers.22.fc1.bias', 'qformer.encoder.layer.4.attention.attention.query.weight', 'language_model.model.decoder.layers.13.self_attn.q_proj.weight', 'language_model.model.decoder.layers.3.self_attn.k_proj.bias', 'language_model.model.decoder.embed_tokens.weight', 'qformer.encoder.layer.4.crossattention.attention.value.bias', 'language_model.model.decoder.layers.1.self_attn.out_proj.bias', 'language_model.model.decoder.layers.12.fc1.weight', 'language_model.model.decoder.layers.3.self_attn.v_proj.bias', 'qformer.encoder.layer.0.attention.output.LayerNorm.weight', 'language_model.model.decoder.layers.16.self_attn.v_proj.weight', 'qformer.encoder.layer.10.crossattention.output.LayerNorm.bias', 'language_model.model.decoder.layers.15.fc2.bias', 'language_model.model.decoder.layers.7.fc2.bias', 'language_model.model.decoder.final_layer_norm.weight', 'qformer.encoder.layer.5.attention.attention.query.bias', 'qformer.encoder.layer.7.attention.attention.value.bias', 'qformer.encoder.layer.1.attention.attention.query.bias', 'language_model.model.decoder.layers.31.fc1.weight', 'language_model.model.decoder.layers.26.fc2.weight', 'language_model.model.decoder.layers.5.self_attn.q_proj.weight', 'qformer.encoder.layer.1.attention.output.LayerNorm.bias', 'language_model.model.decoder.layers.20.self_attn_layer_norm.bias', 'language_model.model.decoder.layers.14.final_layer_norm.weight', 'language_model.model.decoder.layers.6.fc2.bias', 'language_model.model.decoder.layers.21.self_attn.q_proj.weight', 'language_model.model.decoder.layers.28.fc2.bias', 'language_model.model.decoder.layers.15.self_attn_layer_norm.bias', 'qformer.encoder.layer.4.attention.attention.value.weight', 'qformer.encoder.layer.10.attention.output.dense.bias', 'language_model.model.decoder.layers.2.fc2.bias', 'qformer.encoder.layer.4.crossattention.attention.key.weight', 'qformer.encoder.layer.6.attention.attention.value.bias', 'language_model.model.decoder.layers.27.self_attn.out_proj.weight', 'language_model.model.decoder.layers.17.fc1.bias', 'language_model.model.decoder.layers.12.final_layer_norm.bias', 'qformer.encoder.layer.6.crossattention.output.LayerNorm.bias', 'language_model.model.decoder.layers.3.fc1.bias', 'language_model.model.decoder.layers.23.fc2.weight', 'language_model.model.decoder.layers.28.final_layer_norm.weight', 'language_model.model.decoder.layers.4.self_attn.out_proj.weight', 'language_model.model.decoder.layers.19.self_attn.v_proj.weight', 'language_model.model.decoder.layers.29.self_attn.out_proj.weight', 'language_model.model.decoder.layers.22.final_layer_norm.bias', 'language_model.model.decoder.layers.11.self_attn.k_proj.bias', 'qformer.encoder.layer.2.attention.output.LayerNorm.weight', 'language_model.model.decoder.layers.9.self_attn.out_proj.bias', 'language_model.model.decoder.layers.17.self_attn.out_proj.bias', 'language_model.model.decoder.layers.26.fc1.weight', 'qformer.encoder.layer.6.attention.attention.query.weight', 'language_model.model.decoder.layers.11.final_layer_norm.weight', 'language_model.model.decoder.layers.25.final_layer_norm.bias', 'qformer.encoder.layer.6.intermediate_query.dense.bias', 'language_model.model.decoder.layers.24.fc2.weight', 'language_model.model.decoder.layers.7.self_attn.out_proj.weight', 'language_model.model.decoder.layers.14.fc2.weight', 'language_model.model.decoder.layers.4.fc2.weight', 'qformer.encoder.layer.0.output_query.dense.bias', 'qformer.encoder.layer.5.attention.output.LayerNorm.bias', 'qformer.encoder.layer.1.attention.output.dense.bias', 'qformer.encoder.layer.2.crossattention.attention.query.weight', 'language_model.model.decoder.layers.14.self_attn.k_proj.bias', 'language_model.model.decoder.layers.21.self_attn_layer_norm.bias', 'qformer.encoder.layer.2.attention.output.dense.bias', 'qformer.encoder.layer.4.output_query.LayerNorm.bias', 'language_model.model.decoder.layers.4.self_attn.q_proj.weight', 'language_model.model.decoder.layers.14.final_layer_norm.bias', 'language_model.model.decoder.layers.31.final_layer_norm.weight', 'qformer.encoder.layer.10.crossattention.attention.value.bias', 'qformer.encoder.layer.10.crossattention.attention.key.bias', 'language_model.model.decoder.layers.1.final_layer_norm.weight', 'qformer.encoder.layer.1.intermediate_query.dense.weight', 'qformer.encoder.layer.0.crossattention.output.dense.weight', 'qformer.encoder.layer.8.output_query.LayerNorm.bias', 'language_model.model.decoder.layers.17.fc2.weight', 'language_model.model.decoder.layers.17.self_attn.out_proj.weight', 'language_model.model.decoder.layers.6.fc1.weight', 'qformer.encoder.layer.3.attention.attention.query.weight', 'language_model.model.decoder.layers.11.self_attn.v_proj.weight', 'qformer.encoder.layer.0.crossattention.output.LayerNorm.bias', 'language_model.model.decoder.layers.18.self_attn.v_proj.bias', 'language_model.model.decoder.layers.11.self_attn.q_proj.bias', 'qformer.encoder.layer.3.attention.output.dense.bias', 'language_model.model.decoder.layers.4.self_attn.v_proj.weight', 'qformer.encoder.layer.2.attention.attention.value.weight', 'language_model.model.decoder.layers.12.self_attn.out_proj.weight', 'language_model.model.decoder.layers.25.self_attn.q_proj.weight', 'language_model.model.decoder.layers.3.fc1.weight', 'qformer.encoder.layer.6.attention.attention.query.bias', 'language_model.model.decoder.layers.24.self_attn.out_proj.bias', 'language_model.model.decoder.layers.20.self_attn.q_proj.bias', 'language_model.model.decoder.layers.26.final_layer_norm.bias', 'qformer.encoder.layer.4.attention.output.LayerNorm.bias', 'qformer.encoder.layer.8.crossattention.attention.value.weight', 'language_model.model.decoder.layers.9.self_attn.v_proj.bias', 'language_model.model.decoder.layers.9.final_layer_norm.weight', 'language_model.model.decoder.layers.15.final_layer_norm.bias']\n",
      "- This IS expected if you are initializing BlipModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BlipModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BlipModel were not initialized from the model checkpoint at Salesforce/blip2-opt-2.7b and are newly initialized: ['text_model.encoder.layer.30.attention.self.key.weight', 'text_model.encoder.layer.3.output.LayerNorm.weight', 'text_model.encoder.layer.31.attention.output.LayerNorm.bias', 'text_model.encoder.layer.26.intermediate.dense.bias', 'text_model.encoder.layer.30.attention.output.dense.weight', 'text_model.encoder.layer.3.attention.output.dense.weight', 'text_model.encoder.layer.16.attention.self.value.bias', 'text_model.encoder.layer.6.attention.self.key.bias', 'text_model.encoder.layer.8.attention.self.query.weight', 'text_model.encoder.layer.5.attention.output.dense.bias', 'text_model.encoder.layer.23.attention.output.LayerNorm.weight', 'text_model.encoder.layer.4.output.dense.weight', 'text_model.encoder.layer.13.attention.self.key.bias', 'text_model.encoder.layer.17.attention.self.query.bias', 'text_model.encoder.layer.7.output.dense.bias', 'text_model.encoder.layer.15.output.dense.weight', 'text_model.encoder.layer.18.attention.self.value.weight', 'text_model.encoder.layer.30.attention.self.query.bias', 'text_model.encoder.layer.16.intermediate.dense.bias', 'text_model.encoder.layer.18.attention.output.LayerNorm.weight', 'text_model.encoder.layer.20.attention.output.LayerNorm.weight', 'text_model.encoder.layer.13.output.dense.bias', 'text_model.encoder.layer.27.attention.output.dense.bias', 'text_model.encoder.layer.9.attention.output.dense.bias', 'text_model.encoder.layer.30.attention.output.dense.bias', 'text_model.encoder.layer.4.attention.output.dense.weight', 'text_model.encoder.layer.9.output.dense.bias', 'text_model.encoder.layer.28.attention.self.value.weight', 'text_model.encoder.layer.22.attention.output.LayerNorm.bias', 'text_model.encoder.layer.20.attention.self.value.bias', 'text_model.encoder.layer.19.attention.output.dense.weight', 'text_model.encoder.layer.22.attention.output.LayerNorm.weight', 'text_model.encoder.layer.0.intermediate.dense.weight', 'text_model.encoder.layer.12.attention.self.query.bias', 'text_model.encoder.layer.16.output.LayerNorm.weight', 'text_model.encoder.layer.6.attention.output.LayerNorm.bias', 'text_model.encoder.layer.29.attention.self.value.bias', 'text_model.encoder.layer.10.attention.self.query.weight', 'text_model.encoder.layer.24.attention.self.key.weight', 'text_model.encoder.layer.30.attention.self.key.bias', 'text_model.encoder.layer.29.attention.output.LayerNorm.bias', 'text_model.encoder.layer.24.attention.output.LayerNorm.weight', 'text_model.encoder.layer.5.output.LayerNorm.bias', 'text_model.encoder.layer.8.attention.self.key.bias', 'text_model.encoder.layer.11.output.LayerNorm.bias', 'text_model.encoder.layer.11.attention.self.value.weight', 'text_model.encoder.layer.11.output.LayerNorm.weight', 'text_model.encoder.layer.31.attention.output.LayerNorm.weight', 'text_model.encoder.layer.10.output.LayerNorm.weight', 'text_model.encoder.layer.0.intermediate.dense.bias', 'text_model.encoder.layer.21.intermediate.dense.weight', 'text_model.encoder.layer.7.attention.output.dense.bias', 'text_model.encoder.layer.4.attention.self.key.weight', 'text_model.encoder.layer.15.intermediate.dense.weight', 'text_model.encoder.layer.17.output.dense.bias', 'text_model.encoder.layer.5.attention.output.dense.weight', 'text_model.encoder.layer.29.attention.output.LayerNorm.weight', 'text_model.encoder.layer.26.attention.self.key.weight', 'text_model.encoder.layer.29.intermediate.dense.weight', 'text_model.encoder.layer.12.intermediate.dense.weight', 'text_model.encoder.layer.23.attention.self.query.bias', 'text_model.encoder.layer.27.attention.output.LayerNorm.bias', 'text_model.encoder.layer.14.intermediate.dense.bias', 'text_model.encoder.layer.9.attention.output.dense.weight', 'text_projection.weight', 'text_model.encoder.layer.3.attention.self.key.bias', 'text_model.encoder.layer.1.attention.self.key.weight', 'text_model.encoder.layer.27.attention.output.LayerNorm.weight', 'text_model.encoder.layer.5.attention.self.query.weight', 'text_model.encoder.layer.20.output.LayerNorm.weight', 'text_model.encoder.layer.22.attention.output.dense.bias', 'text_model.encoder.layer.15.attention.self.key.bias', 'text_model.encoder.layer.20.intermediate.dense.bias', 'text_model.encoder.layer.22.output.LayerNorm.weight', 'text_model.encoder.layer.5.output.dense.weight', 'text_model.embeddings.LayerNorm.weight', 'text_model.encoder.layer.24.output.LayerNorm.weight', 'text_model.encoder.layer.26.attention.self.key.bias', 'text_model.encoder.layer.10.attention.output.dense.bias', 'text_model.encoder.layer.31.attention.output.dense.weight', 'text_model.encoder.layer.4.output.LayerNorm.weight', 'text_model.encoder.layer.25.attention.output.LayerNorm.bias', 'text_model.encoder.layer.29.attention.self.key.bias', 'text_model.encoder.layer.8.intermediate.dense.bias', 'text_model.encoder.layer.17.attention.self.key.weight', 'text_model.encoder.layer.18.output.dense.bias', 'text_model.encoder.layer.1.attention.output.LayerNorm.bias', 'text_model.encoder.layer.8.attention.output.dense.bias', 'text_model.encoder.layer.25.attention.self.key.weight', 'text_model.encoder.layer.31.attention.self.key.bias', 'text_model.encoder.layer.8.output.dense.bias', 'text_model.encoder.layer.26.output.dense.bias', 'text_model.encoder.layer.22.attention.self.key.weight', 'text_model.encoder.layer.5.output.LayerNorm.weight', 'text_model.encoder.layer.11.attention.output.dense.bias', 'text_model.encoder.layer.3.attention.self.key.weight', 'text_model.encoder.layer.30.intermediate.dense.weight', 'text_model.encoder.layer.19.attention.self.key.weight', 'text_model.encoder.layer.0.output.LayerNorm.weight', 'text_model.encoder.layer.20.attention.output.dense.bias', 'text_model.encoder.layer.6.attention.output.dense.weight', 'text_model.encoder.layer.14.output.dense.weight', 'text_model.encoder.layer.31.attention.self.query.bias', 'text_model.encoder.layer.9.attention.self.query.bias', 'text_model.encoder.layer.9.output.LayerNorm.weight', 'text_model.encoder.layer.18.attention.self.query.bias', 'text_model.encoder.layer.21.output.dense.bias', 'text_model.encoder.layer.1.intermediate.dense.weight', 'text_model.encoder.layer.26.attention.output.LayerNorm.bias', 'text_model.encoder.layer.15.attention.self.value.weight', 'text_model.encoder.layer.8.output.LayerNorm.weight', 'text_model.encoder.layer.15.attention.self.value.bias', 'text_model.encoder.layer.19.intermediate.dense.weight', 'text_model.encoder.layer.26.attention.self.value.weight', 'text_model.encoder.layer.26.output.dense.weight', 'text_model.encoder.layer.28.attention.self.value.bias', 'text_model.encoder.layer.18.output.LayerNorm.weight', 'text_model.encoder.layer.5.attention.self.query.bias', 'text_model.encoder.layer.2.output.LayerNorm.weight', 'text_model.encoder.layer.16.attention.self.key.bias', 'text_model.encoder.layer.1.attention.output.LayerNorm.weight', 'text_model.encoder.layer.22.attention.self.query.weight', 'text_model.encoder.layer.23.attention.self.key.bias', 'text_model.embeddings.LayerNorm.bias', 'text_model.encoder.layer.11.attention.self.query.weight', 'text_model.encoder.layer.11.intermediate.dense.weight', 'text_model.encoder.layer.9.output.dense.weight', 'text_model.encoder.layer.20.attention.self.query.weight', 'text_model.encoder.layer.20.output.LayerNorm.bias', 'text_model.encoder.layer.3.attention.self.query.bias', 'text_model.encoder.layer.17.attention.self.value.weight', 'text_model.encoder.layer.6.output.LayerNorm.weight', 'text_model.encoder.layer.2.attention.output.LayerNorm.bias', 'text_model.encoder.layer.10.attention.self.query.bias', 'text_model.encoder.layer.21.attention.self.query.bias', 'text_model.encoder.layer.16.attention.output.dense.weight', 'text_model.encoder.layer.8.attention.self.query.bias', 'text_model.encoder.layer.1.output.dense.bias', 'text_model.encoder.layer.2.attention.output.LayerNorm.weight', 'text_model.encoder.layer.4.intermediate.dense.weight', 'text_model.encoder.layer.22.output.dense.bias', 'text_model.encoder.layer.5.attention.output.LayerNorm.weight', 'text_model.encoder.layer.7.output.dense.weight', 'text_model.encoder.layer.21.output.LayerNorm.bias', 'text_model.encoder.layer.9.attention.self.key.bias', 'text_model.encoder.layer.16.attention.self.query.weight', 'text_model.encoder.layer.19.output.dense.bias', 'text_model.encoder.layer.22.attention.self.value.weight', 'text_model.encoder.layer.3.attention.self.value.weight', 'text_model.encoder.layer.20.attention.output.dense.weight', 'text_model.encoder.layer.14.attention.self.key.weight', 'text_model.encoder.layer.10.output.dense.weight', 'text_model.encoder.layer.28.attention.output.dense.weight', 'text_model.encoder.layer.19.attention.self.query.weight', 'text_model.encoder.layer.3.output.LayerNorm.bias', 'text_model.encoder.layer.13.intermediate.dense.bias', 'text_model.encoder.layer.20.attention.self.query.bias', 'text_model.encoder.layer.9.attention.output.LayerNorm.weight', 'text_model.encoder.layer.12.output.LayerNorm.bias', 'text_model.encoder.layer.31.intermediate.dense.bias', 'text_model.encoder.layer.20.attention.self.key.weight', 'text_model.encoder.layer.26.output.LayerNorm.weight', 'text_model.encoder.layer.19.attention.self.value.weight', 'text_model.encoder.layer.8.output.dense.weight', 'text_model.encoder.layer.17.intermediate.dense.bias', 'text_model.encoder.layer.26.output.LayerNorm.bias', 'text_model.encoder.layer.9.attention.self.value.weight', 'text_model.encoder.layer.13.attention.output.dense.weight', 'text_model.encoder.layer.2.intermediate.dense.bias', 'text_model.encoder.layer.13.attention.output.LayerNorm.bias', 'text_model.encoder.layer.22.output.LayerNorm.bias', 'text_model.encoder.layer.14.attention.output.LayerNorm.weight', 'text_model.encoder.layer.30.output.dense.bias', 'text_model.encoder.layer.31.intermediate.dense.weight', 'text_model.encoder.layer.3.attention.output.dense.bias', 'text_model.encoder.layer.7.intermediate.dense.weight', 'text_model.encoder.layer.26.attention.output.dense.weight', 'text_model.encoder.layer.13.intermediate.dense.weight', 'text_model.encoder.layer.29.attention.output.dense.weight', 'text_model.encoder.layer.7.attention.output.LayerNorm.bias', 'text_model.encoder.layer.12.attention.self.query.weight', 'text_model.encoder.layer.19.attention.self.value.bias', 'text_model.encoder.layer.28.output.LayerNorm.bias', 'text_model.encoder.layer.15.intermediate.dense.bias', 'text_model.encoder.layer.28.attention.output.dense.bias', 'text_model.encoder.layer.25.output.LayerNorm.weight', 'text_model.encoder.layer.22.attention.self.value.bias', 'text_model.encoder.layer.30.output.dense.weight', 'text_model.encoder.layer.31.output.dense.bias', 'text_model.encoder.layer.10.attention.output.LayerNorm.weight', 'text_model.encoder.layer.24.attention.self.query.weight', 'text_model.encoder.layer.31.attention.self.key.weight', 'text_model.encoder.layer.9.intermediate.dense.weight', 'text_model.encoder.layer.11.attention.self.query.bias', 'text_model.encoder.layer.28.attention.self.query.weight', 'text_model.encoder.layer.16.attention.self.query.bias', 'text_model.encoder.layer.28.attention.output.LayerNorm.bias', 'text_model.encoder.layer.24.attention.output.dense.bias', 'text_model.encoder.layer.10.attention.self.value.weight', 'text_model.encoder.layer.17.output.LayerNorm.bias', 'text_model.encoder.layer.27.attention.self.value.weight', 'text_model.encoder.layer.26.attention.output.LayerNorm.weight', 'text_model.pooler.dense.weight', 'text_model.encoder.layer.19.attention.output.LayerNorm.weight', 'text_model.encoder.layer.26.intermediate.dense.weight', 'text_model.encoder.layer.24.attention.self.value.weight', 'text_model.encoder.layer.6.output.dense.weight', 'text_model.encoder.layer.1.attention.self.query.weight', 'text_model.encoder.layer.21.output.LayerNorm.weight', 'text_model.encoder.layer.11.attention.self.key.bias', 'text_model.encoder.layer.31.output.LayerNorm.weight', 'text_model.encoder.layer.13.attention.self.query.bias', 'text_model.encoder.layer.21.attention.self.query.weight', 'text_model.encoder.layer.12.attention.self.value.weight', 'text_model.encoder.layer.3.attention.output.LayerNorm.weight', 'text_model.encoder.layer.17.output.dense.weight', 'text_model.encoder.layer.29.output.LayerNorm.weight', 'text_model.encoder.layer.15.attention.self.query.weight', 'text_model.encoder.layer.12.attention.self.key.bias', 'text_model.encoder.layer.20.attention.self.value.weight', 'text_model.encoder.layer.14.output.LayerNorm.bias', 'text_model.encoder.layer.24.attention.self.query.bias', 'text_model.encoder.layer.16.attention.output.dense.bias', 'text_model.encoder.layer.17.attention.output.LayerNorm.weight', 'text_model.encoder.layer.19.output.LayerNorm.bias', 'text_model.encoder.layer.30.attention.output.LayerNorm.weight', 'text_model.encoder.layer.3.output.dense.weight', 'text_model.encoder.layer.2.attention.self.query.bias', 'text_model.encoder.layer.0.attention.self.key.weight', 'text_model.encoder.layer.27.output.LayerNorm.bias', 'text_model.encoder.layer.29.attention.self.query.weight', 'text_model.encoder.layer.12.output.dense.bias', 'text_model.encoder.layer.18.output.dense.weight', 'text_model.encoder.layer.14.attention.self.query.weight', 'text_model.encoder.layer.10.intermediate.dense.bias', 'text_model.encoder.layer.29.intermediate.dense.bias', 'text_model.encoder.layer.25.attention.self.key.bias', 'text_model.encoder.layer.12.intermediate.dense.bias', 'text_model.encoder.layer.25.attention.self.query.weight', 'text_model.encoder.layer.27.output.dense.weight', 'text_model.encoder.layer.0.attention.self.query.bias', 'text_model.encoder.layer.15.attention.self.query.bias', 'text_model.encoder.layer.29.output.dense.weight', 'text_model.encoder.layer.23.attention.output.dense.weight', 'text_model.encoder.layer.5.intermediate.dense.weight', 'text_model.pooler.dense.bias', 'text_model.encoder.layer.26.attention.output.dense.bias', 'text_model.encoder.layer.15.attention.output.dense.weight', 'text_model.encoder.layer.17.attention.self.value.bias', 'text_model.encoder.layer.2.attention.self.value.weight', 'text_model.encoder.layer.16.attention.output.LayerNorm.bias', 'text_model.encoder.layer.14.intermediate.dense.weight', 'text_model.encoder.layer.19.attention.self.key.bias', 'text_model.encoder.layer.8.output.LayerNorm.bias', 'text_model.embeddings.word_embeddings.weight', 'text_model.encoder.layer.14.attention.output.LayerNorm.bias', 'text_model.encoder.layer.5.intermediate.dense.bias', 'text_model.encoder.layer.25.intermediate.dense.bias', 'text_model.encoder.layer.15.attention.output.LayerNorm.weight', 'text_model.encoder.layer.0.attention.self.query.weight', 'text_model.encoder.layer.31.attention.self.value.weight', 'text_model.encoder.layer.28.attention.self.key.bias', 'text_model.encoder.layer.18.intermediate.dense.bias', 'text_model.encoder.layer.30.output.LayerNorm.bias', 'text_model.encoder.layer.15.output.LayerNorm.weight', 'text_model.encoder.layer.27.output.LayerNorm.weight', 'text_model.encoder.layer.22.intermediate.dense.weight', 'text_model.encoder.layer.27.intermediate.dense.bias', 'text_model.encoder.layer.19.output.LayerNorm.weight', 'text_model.encoder.layer.12.attention.output.dense.bias', 'text_model.encoder.layer.9.attention.output.LayerNorm.bias', 'text_model.encoder.layer.20.output.dense.bias', 'text_model.encoder.layer.19.attention.output.LayerNorm.bias', 'text_model.encoder.layer.6.attention.self.query.bias', 'text_model.encoder.layer.18.attention.output.dense.weight', 'text_model.encoder.layer.28.output.dense.weight', 'text_model.encoder.layer.24.output.dense.bias', 'text_model.encoder.layer.11.output.dense.bias', 'text_model.encoder.layer.24.output.dense.weight', 'text_model.encoder.layer.0.attention.output.LayerNorm.weight', 'text_model.encoder.layer.20.intermediate.dense.weight', 'logit_scale', 'text_model.encoder.layer.1.output.dense.weight', 'text_model.encoder.layer.17.attention.self.key.bias', 'text_model.encoder.layer.24.intermediate.dense.bias', 'text_model.encoder.layer.17.output.LayerNorm.weight', 'text_model.encoder.layer.6.intermediate.dense.bias', 'text_model.encoder.layer.5.output.dense.bias', 'text_model.encoder.layer.23.output.dense.bias', 'text_model.encoder.layer.6.attention.self.key.weight', 'text_model.encoder.layer.2.intermediate.dense.weight', 'text_model.encoder.layer.15.attention.output.LayerNorm.bias', 'text_model.encoder.layer.18.attention.self.key.bias', 'text_model.encoder.layer.25.attention.self.query.bias', 'text_model.encoder.layer.27.attention.output.dense.weight', 'text_model.encoder.layer.12.attention.self.key.weight', 'text_model.encoder.layer.1.attention.self.value.bias', 'text_model.encoder.layer.5.attention.self.key.weight', 'text_model.encoder.layer.25.attention.output.LayerNorm.weight', 'text_model.encoder.layer.30.attention.self.query.weight', 'text_model.encoder.layer.8.attention.self.value.bias', 'visual_projection.weight', 'text_model.encoder.layer.17.attention.output.dense.weight', 'text_model.encoder.layer.8.attention.output.dense.weight', 'text_model.encoder.layer.19.output.dense.weight', 'text_model.encoder.layer.12.attention.output.dense.weight', 'text_model.encoder.layer.28.attention.self.key.weight', 'text_model.encoder.layer.16.output.dense.bias', 'text_model.encoder.layer.7.attention.output.dense.weight', 'text_model.encoder.layer.22.intermediate.dense.bias', 'text_model.encoder.layer.25.output.dense.bias', 'text_model.encoder.layer.13.attention.self.value.bias', 'text_model.encoder.layer.4.attention.output.dense.bias', 'text_model.encoder.layer.28.intermediate.dense.bias', 'text_model.encoder.layer.2.attention.output.dense.bias', 'text_model.encoder.layer.1.intermediate.dense.bias', 'text_model.encoder.layer.16.attention.self.value.weight', 'text_model.encoder.layer.2.attention.output.dense.weight', 'text_model.encoder.layer.13.attention.output.LayerNorm.weight', 'text_model.encoder.layer.14.attention.self.value.weight', 'text_model.encoder.layer.4.attention.output.LayerNorm.bias', 'text_model.encoder.layer.14.output.LayerNorm.weight', 'text_model.encoder.layer.12.output.dense.weight', 'text_model.encoder.layer.31.output.dense.weight', 'text_model.encoder.layer.17.intermediate.dense.weight', 'text_model.encoder.layer.2.output.LayerNorm.bias', 'text_model.encoder.layer.3.attention.output.LayerNorm.bias', 'text_model.encoder.layer.6.attention.output.dense.bias', 'text_model.encoder.layer.7.attention.output.LayerNorm.weight', 'text_model.encoder.layer.11.attention.output.LayerNorm.weight', 'text_model.encoder.layer.12.output.LayerNorm.weight', 'text_model.encoder.layer.23.attention.self.key.weight', 'text_model.encoder.layer.6.attention.output.LayerNorm.weight', 'text_model.encoder.layer.0.attention.output.dense.bias', 'text_model.encoder.layer.13.output.dense.weight', 'text_model.encoder.layer.31.attention.self.query.weight', 'text_model.encoder.layer.16.intermediate.dense.weight', 'text_model.encoder.layer.23.attention.self.query.weight', 'text_model.encoder.layer.25.intermediate.dense.weight', 'text_model.encoder.layer.28.attention.output.LayerNorm.weight', 'text_model.encoder.layer.22.attention.self.key.bias', 'text_model.encoder.layer.10.attention.output.LayerNorm.bias', 'text_model.encoder.layer.4.attention.output.LayerNorm.weight', 'text_model.encoder.layer.28.output.LayerNorm.weight', 'text_model.encoder.layer.7.attention.self.value.weight', 'text_model.encoder.layer.26.attention.self.query.bias', 'text_model.encoder.layer.24.attention.self.key.bias', 'text_model.encoder.layer.8.attention.output.LayerNorm.weight', 'text_model.encoder.layer.14.attention.self.value.bias', 'text_model.encoder.layer.8.attention.output.LayerNorm.bias', 'text_model.encoder.layer.8.intermediate.dense.weight', 'text_model.encoder.layer.27.intermediate.dense.weight', 'text_model.encoder.layer.14.output.dense.bias', 'text_model.encoder.layer.23.output.LayerNorm.weight', 'text_model.encoder.layer.20.output.dense.weight', 'text_model.encoder.layer.21.attention.output.dense.weight', 'text_model.encoder.layer.12.attention.output.LayerNorm.weight', 'text_model.encoder.layer.0.output.LayerNorm.bias', 'text_model.encoder.layer.21.attention.self.value.bias', 'text_model.encoder.layer.0.attention.output.dense.weight', 'text_model.encoder.layer.29.attention.self.key.weight', 'text_model.encoder.layer.0.output.dense.bias', 'text_model.encoder.layer.13.attention.self.query.weight', 'text_model.encoder.layer.17.attention.output.dense.bias', 'text_model.encoder.layer.18.attention.self.query.weight', 'text_model.encoder.layer.29.attention.output.dense.bias', 'text_model.encoder.layer.23.attention.self.value.weight', 'text_model.encoder.layer.29.attention.self.query.bias', 'text_model.encoder.layer.7.attention.self.key.weight', 'text_model.encoder.layer.15.output.dense.bias', 'text_model.encoder.layer.4.attention.self.query.bias', 'text_model.encoder.layer.10.attention.self.value.bias', 'text_model.encoder.layer.15.attention.self.key.weight', 'text_model.encoder.layer.12.attention.self.value.bias', 'text_model.encoder.layer.30.attention.self.value.weight', 'text_model.encoder.layer.27.output.dense.bias', 'text_model.encoder.layer.1.output.LayerNorm.weight', 'text_model.encoder.layer.2.attention.self.key.weight', 'text_model.encoder.layer.1.attention.output.dense.bias', 'text_model.encoder.layer.21.attention.output.LayerNorm.bias', 'text_model.encoder.layer.20.attention.self.key.bias', 'text_model.encoder.layer.6.intermediate.dense.weight', 'text_model.encoder.layer.1.attention.self.key.bias', 'text_model.encoder.layer.4.intermediate.dense.bias', 'text_model.encoder.layer.14.attention.output.dense.weight', 'text_model.encoder.layer.11.attention.self.key.weight', 'text_model.encoder.layer.0.output.dense.weight', 'text_model.encoder.layer.9.attention.self.query.weight', 'text_model.encoder.layer.24.attention.output.LayerNorm.bias', 'text_model.encoder.layer.16.attention.output.LayerNorm.weight', 'text_model.encoder.layer.2.attention.self.key.bias', 'text_model.encoder.layer.6.attention.self.query.weight', 'text_model.encoder.layer.21.attention.output.LayerNorm.weight', 'text_model.encoder.layer.5.attention.output.LayerNorm.bias', 'text_model.encoder.layer.29.output.LayerNorm.bias', 'text_model.encoder.layer.17.attention.self.query.weight', 'text_model.encoder.layer.10.attention.output.dense.weight', 'text_model.encoder.layer.6.output.LayerNorm.bias', 'text_model.encoder.layer.30.output.LayerNorm.weight', 'text_model.encoder.layer.18.output.LayerNorm.bias', 'text_model.encoder.layer.8.attention.self.value.weight', 'text_model.encoder.layer.24.output.LayerNorm.bias', 'text_model.encoder.layer.2.attention.self.query.weight', 'text_model.encoder.layer.7.attention.self.query.bias', 'text_model.encoder.layer.10.attention.self.key.weight', 'text_model.encoder.layer.30.attention.output.LayerNorm.bias', 'text_model.encoder.layer.0.attention.self.value.weight', 'text_model.encoder.layer.25.attention.self.value.bias', 'text_model.encoder.layer.1.output.LayerNorm.bias', 'text_model.encoder.layer.7.attention.self.key.bias', 'text_model.encoder.layer.30.intermediate.dense.bias', 'text_model.encoder.layer.25.attention.output.dense.weight', 'text_model.encoder.layer.25.output.LayerNorm.bias', 'text_model.encoder.layer.3.attention.self.query.weight', 'text_model.encoder.layer.3.intermediate.dense.weight', 'text_model.encoder.layer.3.intermediate.dense.bias', 'text_model.encoder.layer.29.output.dense.bias', 'text_model.encoder.layer.3.attention.self.value.bias', 'text_model.encoder.layer.14.attention.self.query.bias', 'text_model.encoder.layer.14.attention.output.dense.bias', 'text_model.encoder.layer.18.attention.self.value.bias', 'text_model.encoder.layer.4.output.LayerNorm.bias', 'text_model.encoder.layer.27.attention.self.value.bias', 'text_model.encoder.layer.17.attention.output.LayerNorm.bias', 'text_model.encoder.layer.5.attention.self.value.weight', 'text_model.encoder.layer.31.attention.self.value.bias', 'text_model.encoder.layer.23.intermediate.dense.bias', 'text_model.encoder.layer.9.intermediate.dense.bias', 'text_model.embeddings.position_embeddings.weight', 'text_model.encoder.layer.16.attention.self.key.weight', 'text_model.encoder.layer.3.output.dense.bias', 'text_model.encoder.layer.21.attention.self.value.weight', 'text_model.encoder.layer.10.intermediate.dense.weight', 'text_model.encoder.layer.20.attention.output.LayerNorm.bias', 'text_model.encoder.layer.18.attention.self.key.weight', 'text_model.encoder.layer.1.attention.self.value.weight', 'text_model.encoder.layer.22.output.dense.weight', 'text_model.encoder.layer.21.attention.output.dense.bias', 'text_model.encoder.layer.11.attention.output.LayerNorm.bias', 'text_model.encoder.layer.25.attention.output.dense.bias', 'text_model.encoder.layer.24.attention.self.value.bias', 'text_model.encoder.layer.6.attention.self.value.bias', 'text_model.encoder.layer.15.output.LayerNorm.bias', 'text_model.encoder.layer.24.intermediate.dense.weight', 'text_model.encoder.layer.31.output.LayerNorm.bias', 'text_model.encoder.layer.11.output.dense.weight', 'text_model.encoder.layer.12.attention.output.LayerNorm.bias', 'text_model.encoder.layer.23.output.LayerNorm.bias', 'text_model.encoder.layer.28.output.dense.bias', 'text_model.encoder.layer.22.attention.output.dense.weight', 'text_model.encoder.layer.31.attention.output.dense.bias', 'text_model.encoder.layer.0.attention.self.key.bias', 'text_model.encoder.layer.7.output.LayerNorm.weight', 'text_model.encoder.layer.6.output.dense.bias', 'text_model.encoder.layer.16.output.LayerNorm.bias', 'text_model.encoder.layer.19.attention.self.query.bias', 'text_model.encoder.layer.9.attention.self.key.weight', 'text_model.encoder.layer.21.attention.self.key.weight', 'text_model.encoder.layer.8.attention.self.key.weight', 'text_model.encoder.layer.18.attention.output.LayerNorm.bias', 'text_model.encoder.layer.27.attention.self.query.weight', 'text_model.encoder.layer.27.attention.self.key.bias', 'text_model.encoder.layer.23.attention.self.value.bias', 'text_model.encoder.layer.23.attention.output.LayerNorm.bias', 'text_model.encoder.layer.19.attention.output.dense.bias', 'text_model.encoder.layer.4.attention.self.value.bias', 'text_model.encoder.layer.0.attention.self.value.bias', 'text_model.encoder.layer.29.attention.self.value.weight', 'text_model.encoder.layer.25.output.dense.weight', 'text_model.encoder.layer.28.intermediate.dense.weight', 'text_model.encoder.layer.18.intermediate.dense.weight', 'text_model.encoder.layer.11.attention.self.value.bias', 'text_model.encoder.layer.13.attention.self.value.weight', 'text_model.encoder.layer.4.output.dense.bias', 'text_model.encoder.layer.23.intermediate.dense.weight', 'text_model.encoder.layer.23.attention.output.dense.bias', 'text_model.encoder.layer.26.attention.self.query.weight', 'text_model.encoder.layer.24.attention.output.dense.weight', 'text_model.encoder.layer.4.attention.self.query.weight', 'text_model.encoder.layer.21.output.dense.weight', 'text_model.encoder.layer.10.attention.self.key.bias', 'text_model.encoder.layer.27.attention.self.key.weight', 'text_model.encoder.layer.16.output.dense.weight', 'text_model.encoder.layer.21.attention.self.key.bias', 'text_model.encoder.layer.5.attention.self.key.bias', 'text_model.encoder.layer.13.attention.output.dense.bias', 'text_model.encoder.layer.19.intermediate.dense.bias', 'text_model.encoder.layer.4.attention.self.key.bias', 'text_model.encoder.layer.7.attention.self.value.bias', 'text_model.encoder.layer.11.intermediate.dense.bias', 'text_model.encoder.layer.18.attention.output.dense.bias', 'text_model.encoder.layer.7.intermediate.dense.bias', 'text_model.encoder.layer.13.attention.self.key.weight', 'text_model.encoder.layer.7.attention.self.query.weight', 'text_model.encoder.layer.14.attention.self.key.bias', 'text_model.encoder.layer.21.intermediate.dense.bias', 'text_model.encoder.layer.30.attention.self.value.bias', 'text_model.encoder.layer.5.attention.self.value.bias', 'text_model.encoder.layer.10.output.dense.bias', 'text_model.encoder.layer.13.output.LayerNorm.bias', 'text_model.encoder.layer.2.output.dense.weight', 'text_model.encoder.layer.1.attention.output.dense.weight', 'text_model.encoder.layer.11.attention.output.dense.weight', 'text_model.encoder.layer.6.attention.self.value.weight', 'text_model.encoder.layer.27.attention.self.query.bias', 'text_model.encoder.layer.7.output.LayerNorm.bias', 'text_model.encoder.layer.26.attention.self.value.bias', 'text_model.encoder.layer.25.attention.self.value.weight', 'text_model.encoder.layer.10.output.LayerNorm.bias', 'text_model.encoder.layer.1.attention.self.query.bias', 'text_model.encoder.layer.0.attention.output.LayerNorm.bias', 'text_model.encoder.layer.22.attention.self.query.bias', 'text_model.encoder.layer.28.attention.self.query.bias', 'text_model.encoder.layer.4.attention.self.value.weight', 'text_model.encoder.layer.9.output.LayerNorm.bias', 'text_model.encoder.layer.2.attention.self.value.bias', 'text_model.encoder.layer.13.output.LayerNorm.weight', 'text_model.encoder.layer.23.output.dense.weight', 'text_model.encoder.layer.9.attention.self.value.bias', 'text_model.encoder.layer.2.output.dense.bias', 'text_model.encoder.layer.15.attention.output.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0effe7c7258498c99f94d53a39c1961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)rocessor_config.json:   0%|          | 0.00/432 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3d1b3fe5fa64cecbd3e00bfe91cc966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/904 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2102060850094a27be93d55cd703a6e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "674af08081a84c46bbd02b3c7b119d2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d458a80e3f4a4ec18832417699852ff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8c0949b0753487e9f671607e7649724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/548 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import AutoProcessor, BlipModel\n",
    "\n",
    "model = BlipModel.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "inputs = processor(\n",
    "    text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True\n",
    ")\n",
    "\n",
    "outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.image_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.text_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
