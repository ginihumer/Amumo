{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install Amumo\n",
    "\n",
    "# need ipykernel > 6\n",
    "# ! pip install ipykernel==6.23.1\n",
    "# ! pip install git+https://github.com/ginihumer/Amumo.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_model' from partially initialized module 'amumo.model' (most likely due to a circular import) (d:\\jku\\w2023\\pw\\amumo\\amumo\\model.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# import \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mamumo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m model \u001b[38;5;28;01mas\u001b[39;00m am_model\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mamumo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data \u001b[38;5;28;01mas\u001b[39;00m am_data\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mamumo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m widgets \u001b[38;5;28;01mas\u001b[39;00m am_widgets\n",
      "File \u001b[1;32md:\\jku\\w2023\\pw\\amumo\\amumo\\model.py:14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoProcessor, BlipModel\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mamumo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mut\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCLIPModelInterface\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     available_models \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32md:\\jku\\w2023\\pw\\amumo\\amumo\\utils.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_model\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m model\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhierarchy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msch\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'get_model' from partially initialized module 'amumo.model' (most likely due to a circular import) (d:\\jku\\w2023\\pw\\amumo\\amumo\\model.py)"
     ]
    }
   ],
   "source": [
    "# import \n",
    "from amumo import model as am_model\n",
    "from amumo import data as am_data\n",
    "from amumo import widgets as am_widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset diffusiondb (C:/Users/husse/.cache/huggingface/datasets/poloclub___diffusiondb/2m_first_1k/0.9.1/b3bc1e64570dc7149af62c4bac49ecfbce16b683dd4fee083292fae1afa95f7c)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f01b7e648ebe491783c44459efe14a67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load dataset\n",
    "dataset = am_data.DiffusionDB_Dataset(path=\"2m_first_1k\", batch_size=100) # data helper for the diffusionDB dataset; for the interactive prototype, we only use a random subset of 100 samples\n",
    "all_images, all_prompts = dataset.get_data()\n",
    "cache_name = 'diffusiondb_random_100' # path used to cache the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found cached embeddings for diffusiondb_random_100_CLIP_RN50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2df9567f9ddb4b02b236c94528593955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CLIPExplorerWidget(children=(VBox(children=(HBox(children=(Dropdown(description='Model: ', options=('CLIP', 'C…"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "# This widget consists of a similarity heatmap that shows the similarities between all latent embeddings, a 2-dimensional projection of the embeddings, and a hover view of the currently hovered instance.\n",
    "# It also allows user input for choosing the model, an option to manually close the modality gap, an option to cluster the matrix rows by their similarity, and settings for the 2-dimensional projection. \n",
    "# To learn more about the proper use for closing the modality gap, see the interactive article: https://jku-vds-lab.at/amumo.\n",
    "# You can specify a list of models to compare; there is a predefined set of models, but you can also pass your own model class.\n",
    "# The widget takes a list of images and a list of prompts of the same size as input. Images and texts with the same index are considered pairs.\n",
    "am_widgets.CLIPExplorerWidget(cache_name, all_images, all_prompts, models=[am_model.CLIPModel(), 'CyCLIP', 'CLOOB_LAION400M']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True [False, False, False] {'CLIP': <amumo.model.CLIPModel object at 0x000001D877003E80>, 'CyCLIP': <amumo.model.CyCLIPModel object at 0x000001D8030227C0>, 'CLOOB_LAION400M': <amumo.model.CLOOB_LAION400M_Model object at 0x000001D807901F10>}\n",
      "found cached embeddings for diffusiondb_random_100_CLIP_RN50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\jku\\w2023\\pw\\amumo\\amumo\\widgets.py:844: FutureWarning:\n",
      "\n",
      "The input object of type 'PngImageFile' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'PngImageFile', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "\n",
      "d:\\jku\\w2023\\pw\\amumo\\amumo\\widgets.py:844: VisibleDeprecationWarning:\n",
      "\n",
      "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found cached embeddings for diffusiondb_random_100_CyCLIP_RN50\n",
      "found cached embeddings for diffusiondb_random_100_CLOOB-LAION400M_ViT-B-16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1fc5cd3c045412f9943e45aaf2570e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CLIPComparerWidget(children=(HoverWidget(children=(VBox(children=(HTML(value='', layout=Layout(width='300px'))…"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This widget shows similarity heatmaps for all models in the list for a better comparison between models.\n",
    "am_widgets.CLIPComparerWidget(cache_name, all_images, all_prompts, models=[am_model.CLIPModel(), 'CyCLIP', 'CLOOB_LAION400M']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use a Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "import numpy as np\n",
    "# You can use the CLIPModelInterface to create your own model class wrapper.\n",
    "class CustomModel(am_model.CLIPModelInterface):\n",
    "    available_models = clip.available_models()\n",
    "    model_name = 'MyCLIP'\n",
    "\n",
    "    def __init__(self, name='RN50', device='cpu') -> None:\n",
    "        super().__init__(name, device)\n",
    "        self.model, self.preprocess = clip.load(name, device=device)\n",
    "        self.model.eval()\n",
    "        self.logit_scale = self.model.logit_scale\n",
    "\n",
    "    def encode_image(self, images):\n",
    "        images = [self.preprocess(i) for i in images]\n",
    "        image_input = torch.tensor(np.stack(images)).to(self.device)\n",
    "        return self.model.encode_image(image_input).float().cpu()\n",
    "\n",
    "    def encode_text(self, texts):\n",
    "        text_tokens = clip.tokenize(texts, truncate = True).to(self.device)\n",
    "        return self.model.encode_text(text_tokens).float().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found cached embeddings for diffusiondb_random_100_MyCLIP_RN50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1c6cfef8bcc4f6fa671febf783b2604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CLIPExplorerWidget(children=(VBox(children=(HBox(children=(Dropdown(description='Model: ', options=('MyCLIP',)…"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The widget takes any instance of type CLIPModelInterface.\n",
    "am_widgets.CLIPExplorerWidget(cache_name, all_images, all_prompts, models=[CustomModel()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
