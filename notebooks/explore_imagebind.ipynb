{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting soundfile\n",
      "  Using cached soundfile-0.12.1-py2.py3-none-win_amd64.whl (1.0 MB)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\christina\\appdata\\roaming\\python\\python39\\site-packages (from soundfile) (1.15.1)\n",
      "Collecting pycparser (from cffi>=1.0->soundfile)\n",
      "  Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
      "Installing collected packages: pycparser, soundfile\n",
      "Successfully installed pycparser-2.21 soundfile-0.12.1\n"
     ]
    }
   ],
   "source": [
    "# ! pip install git+https://github.com/facebookresearch/ImageBind\n",
    "! pip install soundfile\n",
    "! pip install librosa\n",
    "! pip install \"imagebind @ git+https://github.com/facebookresearch/ImageBind@c6a47d6dc2b53eced51d398c181d57049ca59286\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d2c64d1867448b87724548bdda9b66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Christina\\anaconda3\\envs\\amumo_test\\lib\\site-packages\\datasets\\table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "# create triplet dataset\n",
    "from datasets import load_dataset\n",
    "import soundfile as sf\n",
    "import requests\n",
    "import pandas as pd\n",
    "# https://huggingface.co/datasets/agkphysics/AudioSet\n",
    "# dataset = load_dataset(\"agkphysics/AudioSet\", \"bal\", split=\"test\")\n",
    "dataset = load_dataset(\"arrow\", split=\"test\", data_files={\"test\":\"C:/Users/Christina/.cache/huggingface/datasets/agkphysics___audio_set/bal/audio_set-test-000**-of-00018.arrow\"})#, split=\"test[0:10]\")\n",
    "# https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/main_classes#datasets.Dataset.filter\n",
    "dataset_it = dataset.to_iterable_dataset()\n",
    "\n",
    "data_dir = \"C:\\\\Users\\\\Christina\\\\Data\\\\imagebind\\\\text-audio-image\\\\\"\n",
    "animals = [\"Bird\", \"Cat\", \"Dog\", \"Horse\"]\n",
    "\n",
    "infos = []\n",
    "idx = 0\n",
    "for animal in animals:\n",
    "    for example in dataset_it.filter(lambda x: animal in x[\"human_labels\"]).take(25):\n",
    "        # save audiofile\n",
    "        sf.write(data_dir + \"audio\\\\%03d.wav\"%idx, example[\"audio\"][\"array\"], example[\"audio\"][\"sampling_rate\"], format=\"wav\")\n",
    "\n",
    "        # save image\n",
    "        # https://stackoverflow.com/questions/2068344/how-do-i-get-a-youtube-video-thumbnail-from-the-youtube-api\n",
    "        url = 'https://img.youtube.com/vi/%s/hqdefault.jpg'%example[\"video_id\"]\n",
    "        data = requests.get(url).content \n",
    "        with open(data_dir + \"image\\\\%03d.jpg\"%idx,'wb') as f:\n",
    "            f.write(data) \n",
    "\n",
    "        # save info\n",
    "        infos.append({\"id\": idx, \"video_id\":example[\"video_id\"], \"labels\":example[\"human_labels\"], \"animal\": animal})\n",
    "\n",
    "        idx += 1\n",
    "\n",
    "pd.DataFrame(infos).to_csv(data_dir + \"info.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61a6e5092bd843aea7d2235064690b54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Christina\\anaconda3\\envs\\amumo_test\\lib\\site-packages\\datasets\\table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "# # https://huggingface.co/docs/datasets/loading#slice-splits\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# # dataset = load_dataset(\"agkphysics/AudioSet\", \"bal\", split=\"test\") #, streaming=True\n",
    "# dataset = load_dataset(\"arrow\", split=\"test\", data_files={\"test\":\"C:/Users/Christina/.cache/huggingface/datasets/agkphysics___audio_set/bal/audio_set-test-000**-of-00018.arrow\"})#, split=\"test[0:10]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image-Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mamumo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data \u001b[38;5;28;01mas\u001b[39;00m am_data\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mamumo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils \u001b[38;5;28;01mas\u001b[39;00m am_utils\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mamumo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m widgets \u001b[38;5;28;01mas\u001b[39;00m am_widgets\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# load dataset\u001b[39;00m\n\u001b[0;32m      7\u001b[0m dataset \u001b[38;5;241m=\u001b[39m am_data\u001b[38;5;241m.\u001b[39mDiffusionDB_Dataset(path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2m_first_1k\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m) \u001b[38;5;66;03m# data helper for the diffusionDB dataset; for the interactive prototype, we only use a random subset of 100 samples\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Christina\\anaconda3\\envs\\amumo_test\\lib\\site-packages\\amumo\\widgets.py:10\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PCA\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenTSNE\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TSNE\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mumap\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UMAP\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\decomposition\\__init__.py:9\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mThe :mod:`sklearn.decomposition` module includes matrix decomposition\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03malgorithms, including among others PCA, NMF or ICA. Most of the algorithms of\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03mthis module can be regarded as dimensionality reduction techniques.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextmath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m randomized_svd\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dict_learning\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     DictionaryLearning,\n\u001b[0;32m     11\u001b[0m     MiniBatchDictionaryLearning,\n\u001b[0;32m     12\u001b[0m     SparseCoder,\n\u001b[0;32m     13\u001b[0m     dict_learning,\n\u001b[0;32m     14\u001b[0m     dict_learning_online,\n\u001b[0;32m     15\u001b[0m     sparse_encode,\n\u001b[0;32m     16\u001b[0m )\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_factor_analysis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FactorAnalysis\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_fastica\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastICA, fastica\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\decomposition\\_dict_learning.py:23\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m linalg\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     18\u001b[0m     BaseEstimator,\n\u001b[0;32m     19\u001b[0m     ClassNamePrefixFeaturesOutMixin,\n\u001b[0;32m     20\u001b[0m     TransformerMixin,\n\u001b[0;32m     21\u001b[0m     _fit_context,\n\u001b[0;32m     22\u001b[0m )\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Lars, Lasso, LassoLars, orthogonal_mp_gram\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_array, check_random_state, gen_batches, gen_even_slices\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Hidden, Interval, StrOptions, validate_params\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\__init__.py:11\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearRegression\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bayes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ARDRegression, BayesianRidge\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_coordinate_descent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     12\u001b[0m     ElasticNet,\n\u001b[0;32m     13\u001b[0m     ElasticNetCV,\n\u001b[0;32m     14\u001b[0m     Lasso,\n\u001b[0;32m     15\u001b[0m     LassoCV,\n\u001b[0;32m     16\u001b[0m     MultiTaskElasticNet,\n\u001b[0;32m     17\u001b[0m     MultiTaskElasticNetCV,\n\u001b[0;32m     18\u001b[0m     MultiTaskLasso,\n\u001b[0;32m     19\u001b[0m     MultiTaskLassoCV,\n\u001b[0;32m     20\u001b[0m     enet_path,\n\u001b[0;32m     21\u001b[0m     lasso_path,\n\u001b[0;32m     22\u001b[0m )\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_glm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GammaRegressor, PoissonRegressor, TweedieRegressor\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_huber\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuberRegressor\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:20\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sparse\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiOutputMixin, RegressorMixin, _fit_context\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_cv\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_array, check_scalar\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval, StrOptions\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\__init__.py:3\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_plot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LearningCurveDisplay, ValidationCurveDisplay\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_search\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GridSearchCV, ParameterGrid, ParameterSampler, RandomizedSearchCV\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_split\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      6\u001b[0m     BaseCrossValidator,\n\u001b[0;32m      7\u001b[0m     BaseShuffleSplit,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m     train_test_split,\n\u001b[0;32m     25\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_plot.py:7\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_matplotlib_support\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_plotting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _interval_max_min_ratio, _validate_score_name\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m learning_curve, validation_curve\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01m_BaseCurveDisplay\u001b[39;00m:\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_plot_curve\u001b[39m(\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     13\u001b[0m         x_data,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m         errorbar_kw\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     24\u001b[0m     ):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py:29\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone, is_classifier\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FitFailedWarning\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_scoring, get_scorer_names\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_scorer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _check_multimetric_scoring, _MultimetricScorer\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\__init__.py:7\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mThe :mod:`sklearn.metrics` module includes score functions, performance metrics\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mand pairwise metrics and distance computations.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cluster\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_classification\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      9\u001b[0m     accuracy_score,\n\u001b[0;32m     10\u001b[0m     balanced_accuracy_score,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m     zero_one_loss,\n\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dist_metrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DistanceMetric\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\cluster\\__init__.py:9\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mThe :mod:`sklearn.metrics.cluster` submodule contains evaluation metrics for\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mcluster analysis results. There are two forms of evaluation:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m- unsupervised, which does not and measures the 'quality' of the model itself.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bicluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m consensus_score\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_supervised\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     adjusted_mutual_info_score,\n\u001b[0;32m     11\u001b[0m     adjusted_rand_score,\n\u001b[0;32m     12\u001b[0m     completeness_score,\n\u001b[0;32m     13\u001b[0m     contingency_matrix,\n\u001b[0;32m     14\u001b[0m     entropy,\n\u001b[0;32m     15\u001b[0m     expected_mutual_information,\n\u001b[0;32m     16\u001b[0m     fowlkes_mallows_score,\n\u001b[0;32m     17\u001b[0m     homogeneity_completeness_v_measure,\n\u001b[0;32m     18\u001b[0m     homogeneity_score,\n\u001b[0;32m     19\u001b[0m     mutual_info_score,\n\u001b[0;32m     20\u001b[0m     normalized_mutual_info_score,\n\u001b[0;32m     21\u001b[0m     pair_confusion_matrix,\n\u001b[0;32m     22\u001b[0m     rand_score,\n\u001b[0;32m     23\u001b[0m     v_measure_score,\n\u001b[0;32m     24\u001b[0m )\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_unsupervised\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     26\u001b[0m     calinski_harabasz_score,\n\u001b[0;32m     27\u001b[0m     davies_bouldin_score,\n\u001b[0;32m     28\u001b[0m     silhouette_samples,\n\u001b[0;32m     29\u001b[0m     silhouette_score,\n\u001b[0;32m     30\u001b[0m )\n\u001b[0;32m     32\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madjusted_mutual_info_score\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnormalized_mutual_info_score\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconsensus_score\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     52\u001b[0m ]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:29\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulticlass\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m type_of_target\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_array, check_consistent_length\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_expected_mutual_info_fast\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m expected_mutual_information\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_clusterings\u001b[39m(labels_true, labels_pred):\n\u001b[0;32m     33\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that the labels arrays are 1D and of same dimension.\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;124;03m        The predicted labels.\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:398\u001b[0m, in \u001b[0;36mparent\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from amumo import model as am_model\n",
    "from amumo import data as am_data\n",
    "from amumo import utils as am_utils\n",
    "from amumo import widgets as am_widgets\n",
    "\n",
    "# load dataset\n",
    "dataset = am_data.DiffusionDB_Dataset(path=\"2m_first_1k\", batch_size=100) # data helper for the diffusionDB dataset; for the interactive prototype, we only use a random subset of 100 samples\n",
    "all_images, all_prompts = dataset.get_data()\n",
    "cache_name = 'diffusiondb_random_100' # path used to cache the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 244M/244M [01:27<00:00, 2.92MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found cached embeddings for diffusiondb_random_100_ImageBind_huge image\n",
      "found cached embeddings for diffusiondb_random_100_ImageBind_huge text\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "019294e3992a433eb1b83e3af890c8e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CLIPExplorerWidget(children=(VBox(children=(HBox(children=(Dropdown(description='Model: ', options=('ImageBind…"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found cached embeddings for diffusiondb_random_100_CLIP_RN50 image\n",
      "found cached embeddings for diffusiondb_random_100_CLIP_RN50 text\n",
      "found cached embeddings for diffusiondb_random_100_ImageBind_huge image\n",
      "found cached embeddings for diffusiondb_random_100_ImageBind_huge text\n"
     ]
    }
   ],
   "source": [
    "am_widgets.CLIPExplorerWidget(cache_name, all_data={\"image\": all_images, \"text\": all_prompts}, models=[am_model.ImageBind_Model(), \"CLIP\"]) # {\"image\": all_images, \"depth\": all_depths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True [False, False] {'ImageBind': <amumo.model.ImageBind_Model object at 0x000001B878F032E0>, 'CLIP': <amumo.model.CLIPModel object at 0x000001B767795E20>}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\christina\\repositories\\icg\\researchstay\\amumo\\amumo\\widgets.py:940: FutureWarning:\n",
      "\n",
      "The input object of type 'PngImageFile' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'PngImageFile', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "\n",
      "c:\\users\\christina\\repositories\\icg\\researchstay\\amumo\\amumo\\widgets.py:940: VisibleDeprecationWarning:\n",
      "\n",
      "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found cached embeddings for diffusiondb_random_100_ImageBind_huge\n",
      "found cached embeddings for diffusiondb_random_100_CLIP_RN50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e29ae00ffa9f41edaaf7549652f884de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CLIPComparerWidget(children=(HoverWidget(children=(VBox(children=(HTML(value='', layout=Layout(width='300px'))…"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "am_widgets.CLIPComparerWidget(cache_name, all_images=all_images, all_prompts=all_prompts, models=[am_model.ImageBind_Model(), \"CLIP\"]) # {\"image\": all_images, \"depth\": all_depths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image-Text-Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christina\\AppData\\Roaming\\Python\\Python39\\site-packages\\umap\\distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "C:\\Users\\Christina\\AppData\\Roaming\\Python\\Python39\\site-packages\\umap\\distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "C:\\Users\\Christina\\AppData\\Roaming\\Python\\Python39\\site-packages\\umap\\distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "C:\\Users\\Christina\\AppData\\Roaming\\Python\\Python39\\site-packages\\umap\\umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "from amumo import model as am_model\n",
    "from amumo import data as am_data\n",
    "from amumo import utils as am_utils\n",
    "from amumo import widgets as am_widgets\n",
    "# ! pip install torch\n",
    "# ! pip install torchaudio\n",
    "import torch\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import io\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class AudioType(am_data.DataTypeInterface):\n",
    "    name = \"Audio\"\n",
    "\n",
    "    def __init__(self, data, sample_rate) -> None:\n",
    "        super().__init__(data)\n",
    "        self.sample_rate = sample_rate\n",
    "    \n",
    "    def getVisItem(self, idx):\n",
    "        import soundfile as sf\n",
    "        buffer = io.BytesIO()\n",
    "        sf.write(buffer, self.data[idx][0], self.sample_rate, format=\"wav\")\n",
    "        audio_bytes = buffer.getvalue() # wav bytes\n",
    "        return {\"displayType\": am_data.DisplayTypes.AUDIO, \"value\": audio_bytes}\n",
    "\n",
    "\n",
    "class Triplet_Dataset(am_data.DatasetInterface):\n",
    "    name='Triplet'\n",
    "\n",
    "    def __init__(self, path, seed=31415, batch_size=100, sample_rate=16000):\n",
    "        # create triplet dataset if it does not exist\n",
    "        super().__init__(path, seed, batch_size)\n",
    "        # path: path to the triplet dataset\n",
    "        image_paths = glob(path + \"image\\\\*.jpg\", recursive = True)\n",
    "        audio_paths = glob(path + \"audio\\\\*.wav\", recursive = True)\n",
    "\n",
    "        self.sample_rate = sample_rate\n",
    "        \n",
    "        all_images = []\n",
    "        for image_path in image_paths:\n",
    "            with open(image_path, \"rb\") as fopen:\n",
    "                image = Image.open(fopen).convert(\"RGB\")\n",
    "                all_images.append(image)\n",
    "\n",
    "        all_audios = []\n",
    "        for audio_path in audio_paths:\n",
    "            waveform, sr = torchaudio.load(audio_path)\n",
    "            if sample_rate != sr:\n",
    "                waveform = torchaudio.functional.resample(\n",
    "                    waveform, orig_freq=sr, new_freq=sample_rate\n",
    "                )\n",
    "            all_audios.append(waveform)\n",
    "        \n",
    "        self.all_infos = pd.read_csv(path + \"info.csv\", converters={\"labels\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \")})\n",
    "\n",
    "        # TODO... load on demand with a custom loader\n",
    "        self.all_images = np.array(all_images)\n",
    "        self.all_prompts = np.array(self.all_infos[\"labels\"].map(lambda x: \", \".join(x)))\n",
    "        self.all_audios = np.array(all_audios)\n",
    "    \n",
    "    \n",
    "    def get_data(self):\n",
    "        \n",
    "        if self.batch_size is None:\n",
    "            images = self.MODE1_Type(self.all_images)\n",
    "            texts = self.MODE2_Type(self.all_prompts)\n",
    "            audios = AudioType(self.all_audios, self.sample_rate)\n",
    "        \n",
    "            return images, texts, audios\n",
    "\n",
    "        # create a random batch\n",
    "        batch_idcs = self._get_random_subsample(len(self.all_images))\n",
    "\n",
    "        images = self.MODE1_Type(self.all_images[batch_idcs])\n",
    "        texts = self.MODE2_Type(self.all_prompts[batch_idcs])\n",
    "        audios = AudioType(self.all_audios[batch_idcs], self.sample_rate)\n",
    "        \n",
    "        return images, texts, audios\n",
    "    \n",
    "        \n",
    "    def get_filtered_data(self, filter_list, method=any):\n",
    "        # filter_list: a list of strings that are used for filtering\n",
    "        # method: any -> any substring given in filter_list is present; all -> all substrings must be contained in the string\n",
    "        if filter_list is None or len(filter_list) <= 0:\n",
    "            return self.get_data()\n",
    "\n",
    "        subset_ids = np.array([i for i in range(len(self.all_prompts)) if method(substring in self.all_prompts[i].lower() for substring in filter_list)])\n",
    "        if len(subset_ids) <= 0:\n",
    "            print(\"no filter matches found\")\n",
    "            return [], [], []\n",
    "        \n",
    "        # create a random batch\n",
    "        batch_idcs = self._get_random_subsample(len(subset_ids))\n",
    "        subset_ids = subset_ids[batch_idcs]\n",
    "        \n",
    "        images = self.MODE1_Type(self.all_images[subset_ids])\n",
    "        texts = self.MODE2_Type(self.all_prompts[subset_ids])\n",
    "        audios = AudioType(self.all_audios[subset_ids], self.sample_rate)\n",
    "        return images, texts, audios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_30488\\3688791554.py:54: FutureWarning: The input object of type 'Image' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Image', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  self.all_images = np.array(all_images)\n",
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_30488\\3688791554.py:54: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.all_images = np.array(all_images)\n",
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_30488\\3688791554.py:56: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  self.all_audios = np.array(all_audios)\n",
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_30488\\3688791554.py:56: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.all_audios = np.array(all_audios)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Christina\\anaconda3\\envs\\amumo_test\\lib\\site-packages\\torchvision\\transforms\\_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Christina\\anaconda3\\envs\\amumo_test\\lib\\site-packages\\torchvision\\transforms\\_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found cached embeddings for test_audio_ImageBind_huge text\n",
      "found cached embeddings for test_audio_ImageBind_huge image\n",
      "found cached embeddings for test_audio_ImageBind_huge audio\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffcae93c2a0d4d73929b84c77890dc6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CLIPExplorerWidget(children=(VBox(children=(HBox(children=(Dropdown(description='Model: ', options=('ImageBind…"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Triplet_Dataset(path=\"C:\\\\Users\\\\Christina\\\\Data\\\\imagebind\\\\text-audio-image\\\\\", batch_size=100)\n",
    "all_images, all_prompts, all_audios = dataset.get_data()\n",
    "print(len(all_images), len(all_prompts), len(all_audios))\n",
    "my_widget = am_widgets.CLIPExplorerWidget(\"test_audio\", all_data={\"text\": all_prompts, \"image\": all_images, \"audio\": all_audios }, models=[am_model.ImageBind_Model()]) \n",
    "my_widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image-Thermal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ThermalType(am_data.ImageType):\n",
    "    name = \"Thermal\"\n",
    "\n",
    "    def __init__(self, data) -> None:\n",
    "        super().__init__(data)\n",
    "\n",
    "\n",
    "class LLVIP_Dataset(am_data.DatasetInterface):\n",
    "    name='LLVIP'\n",
    "\n",
    "    def __init__(self, path, seed=31415, batch_size = 100):\n",
    "        # download dataset: https://bupt-ai-cz.github.io/LLVIP/\n",
    "        super().__init__(path, seed, batch_size)\n",
    "        # path: path to the LLVIP dataset\n",
    "        image_paths = glob(path + \"\\\\visible\\\\test\\\\*.jpg\", recursive = True)\n",
    "        thermal_paths = glob(path + \"\\\\infrared\\\\test\\\\*.jpg\", recursive = True)\n",
    "        \n",
    "        batch_idcs = self._get_random_subsample(len(image_paths))\n",
    "        image_paths = np.array(image_paths)[batch_idcs]\n",
    "        thermal_paths = np.array(thermal_paths)[batch_idcs]\n",
    "        \n",
    "        all_images = []\n",
    "        for image_path in image_paths:\n",
    "            with open(image_path, \"rb\") as fopen:\n",
    "                image = Image.open(fopen).convert(\"RGB\")\n",
    "                all_images.append(image)\n",
    "\n",
    "            \n",
    "        all_thermals = []\n",
    "        \n",
    "        for thermal_path in thermal_paths:\n",
    "            with open(thermal_path, \"rb\") as fopen:\n",
    "                thermal = Image.open(fopen).convert(\"L\")\n",
    "                all_thermals.append(thermal)\n",
    "\n",
    "        self.MODE2_Type = ThermalType\n",
    "\n",
    "        # TODO... load images and thermals on demand with a custom loader\n",
    "        self.all_images = np.array(all_images)\n",
    "        self.all_prompts = np.array(all_thermals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_24244\\3221552142.py:99: FutureWarning: The input object of type 'Image' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Image', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  self.all_images = np.array(all_images)\n",
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_24244\\3221552142.py:99: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.all_images = np.array(all_images)\n",
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_24244\\3221552142.py:100: FutureWarning: The input object of type 'Image' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Image', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  self.all_prompts = np.array(all_thermals)\n",
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_24244\\3221552142.py:100: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.all_prompts = np.array(all_thermals)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100\n",
      "found cached embeddings for test_thermal_ImageBind_huge image\n",
      "batch 1 of 1\n",
      "torch.Size([100, 1, 224, 224])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17508ad4cb6642ec8692300f7a57d876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CLIPExplorerWidget(children=(VBox(children=(HBox(children=(Dropdown(description='Model: ', options=('ImageBind…"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download dataset: https://bupt-ai-cz.github.io/LLVIP/\n",
    "dataset = LLVIP_Dataset(path=\"C:\\\\Users\\\\Christina\\\\Data\\\\imagebind\\\\LLVIP\\\\\", batch_size=100) \n",
    "all_images, all_thermals = dataset.get_data()\n",
    "print(len(all_images), len(all_thermals))\n",
    "\n",
    "am_widgets.CLIPExplorerWidget(\"test_thermal\", all_data={\"image\": all_images, \"thermal\": all_thermals}, models=[am_model.ImageBind_Model()]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image-Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class DepthType(am_data.ImageType):\n",
    "    name = \"Depth\"\n",
    "\n",
    "    def __init__(self, data) -> None:\n",
    "        # data is an array of (1,224,224) tensors\n",
    "        super().__init__(data)\n",
    "\n",
    "    def getVisItem(self, idx):\n",
    "        output_img = io.BytesIO()\n",
    "        plt.imsave(output_img, self.data[idx][0], cmap='gray')\n",
    "        plt.savefig(output_img, format='JPEG')\n",
    "        return {\"displayType\": am_data.DisplayTypes.IMAGE, \"value\": output_img.getvalue()}\n",
    "    \n",
    "    \n",
    "\n",
    "class SUNRGBD_Dataset(am_data.DatasetInterface):\n",
    "    name='SUNRGBD_NYU'\n",
    "\n",
    "    def __init__(self, path, seed=31415, batch_size = 100):\n",
    "        # download \"SUNRGBD_V1\" dataset from https://rgbd.cs.princeton.edu/\n",
    "        super().__init__(path, seed, batch_size)\n",
    "        # path: path to the SUNRGBD dataset\n",
    "        image_paths = glob(path + \"\\\\kv1\\\\NYUdata\\\\NYU*\\\\fullres\\\\*.jpg\", recursive = True)\n",
    "        depth_paths = glob(path + \"\\\\kv1\\\\NYUdata\\\\NYU*\\\\fullres\\\\*.png\", recursive = True)\n",
    "        \n",
    "        batch_idcs = self._get_random_subsample(len(image_paths))\n",
    "        image_paths = np.array(image_paths)[batch_idcs]\n",
    "        depth_paths = np.array(depth_paths)[batch_idcs]\n",
    "        \n",
    "        all_images = []\n",
    "        for image_path in image_paths:\n",
    "            with open(image_path, \"rb\") as fopen:\n",
    "                image = Image.open(fopen).convert(\"RGB\")\n",
    "                all_images.append(image)\n",
    "\n",
    "            \n",
    "        all_depths = []\n",
    "        for depth_path in depth_paths:\n",
    "            with open(depth_path, \"rb\") as fopen:\n",
    "                depth = Image.open(fopen)\n",
    "                depth = np.array(depth, dtype=int)\n",
    "                depth = depth.astype(np.float32) / depth.max() # TODO: need to normalize?\n",
    "                # depth = depth[np.newaxis,:,:] # need 1 channel -> (1,224,224)\n",
    "                depth = torch.from_numpy(depth).unsqueeze(0) # need 1 channel -> (1,224,224)\n",
    "                all_depths.append(depth)\n",
    "\n",
    "        self.MODE2_Type = DepthType\n",
    "\n",
    "        # TODO... load images and depths on demand with a custom loader\n",
    "        self.all_images = np.array(all_images)\n",
    "        self.all_prompts = np.array(all_depths)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_24244\\3221552142.py:64: FutureWarning:\n",
      "\n",
      "The input object of type 'Image' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Image', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "\n",
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_24244\\3221552142.py:64: VisibleDeprecationWarning:\n",
      "\n",
      "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\n",
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_24244\\3221552142.py:65: FutureWarning:\n",
      "\n",
      "The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "\n",
      "C:\\Users\\Christina\\AppData\\Local\\Temp\\ipykernel_24244\\3221552142.py:65: VisibleDeprecationWarning:\n",
      "\n",
      "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100\n",
      "found cached embeddings for test_depth_ImageBind_huge image\n",
      "found cached embeddings for test_depth_ImageBind_huge depth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f38b43e86ba4314a512d7d62837bade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CLIPExplorerWidget(children=(VBox(children=(HBox(children=(Dropdown(description='Model: ', options=('ImageBind…"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download \"SUNRGBD_V1\" dataset from https://rgbd.cs.princeton.edu/\n",
    "dataset = SUNRGBD_Dataset(path=\"C:\\\\Users\\\\Christina\\\\Data\\\\imagebind\\\\SUNRGBD\\\\\", batch_size=100)\n",
    "all_images, all_depths = dataset.get_data()\n",
    "print(len(all_images), len(all_depths))\n",
    "\n",
    "am_widgets.CLIPExplorerWidget(\"test_depth\", all_data={\"image\": all_images, \"depth\": all_depths}, models=[am_model.ImageBind_Model()]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amumo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
