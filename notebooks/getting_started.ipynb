{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install Amumo\n",
    "\n",
    "# need ipykernel > 6\n",
    "# ! pip install ipykernel==6.23.1\n",
    "# ! pip install git+https://github.com/ginihumer/Amumo.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christina\\AppData\\Roaming\\Python\\Python39\\site-packages\\umap\\distances.py:1063: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "C:\\Users\\Christina\\AppData\\Roaming\\Python\\Python39\\site-packages\\umap\\distances.py:1071: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "C:\\Users\\Christina\\AppData\\Roaming\\Python\\Python39\\site-packages\\umap\\distances.py:1086: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n",
      "C:\\Users\\Christina\\AppData\\Roaming\\Python\\Python39\\site-packages\\umap\\umap_.py:660: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "# import \n",
    "from amumo import model as am_model\n",
    "from amumo import data as am_data\n",
    "from amumo import widgets as am_widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset diffusiondb (C:/Users/Christina/.cache/huggingface/datasets/poloclub___diffusiondb/2m_first_1k/0.9.1/b3bc1e64570dc7149af62c4bac49ecfbce16b683dd4fee083292fae1afa95f7c)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ee9a91e6caa435ab528c50a51b9d385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load dataset\n",
    "dataset = am_data.DiffusionDB_Dataset(path=\"2m_first_1k\", batch_size=100) # data helper for the diffusionDB dataset; for the interactive prototype, we only use a random subset of 100 samples\n",
    "all_images, all_prompts = dataset.get_data()\n",
    "cache_name = 'diffusiondb_random_100' # path used to cache the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christina\\AppData\\Roaming\\Python\\Python39\\site-packages\\amumo\\widgets.py:525: FutureWarning: The input object of type 'PngImageFile' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'PngImageFile', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  self.all_images = np.array(all_images)\n",
      "C:\\Users\\Christina\\AppData\\Roaming\\Python\\Python39\\site-packages\\amumo\\widgets.py:525: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.all_images = np.array(all_images)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found cached embeddings for diffusiondb_random_100_CLIP_RN50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63554826eca142f18d1df0a7e7fcaf42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CLIPExplorerWidget(children=(VBox(children=(HBox(children=(Dropdown(description='Model: ', options=('CLIP', 'Câ€¦"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This widget consists of a similarity heatmap that shows the similarities between all latent embeddings, a 2-dimensional projection of the embeddings, and a hover view of the currently hovered instance.\n",
    "# It also allows user input for choosing the model, an option to manually close the modality gap, an option to cluster the matrix rows by their similarity, and settings for the 2-dimensional projection. \n",
    "# To learn more about the proper use for closing the modality gap, see the interactive article: https://jku-vds-lab.at/amumo.\n",
    "# You can specify a list of models to compare; there is a predefined set of models, but you can also pass your own model class.\n",
    "# The widget takes a list of images and a list of prompts of the same size as input. Images and texts with the same index are considered pairs.\n",
    "am_widgets.CLIPExplorerWidget(cache_name, all_images, all_prompts, models=[am_model.CLIPModel(), 'CyCLIP', 'CLOOB_LAION400M']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Christina\\Repositories\\ICG\\ResearchStay\\Amumo\\notebooks\\getting_started.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Christina/Repositories/ICG/ResearchStay/Amumo/notebooks/getting_started.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# This widget shows similarity heatmaps for all models in the list for a better comparison between models.\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Christina/Repositories/ICG/ResearchStay/Amumo/notebooks/getting_started.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m am_widgets\u001b[39m.\u001b[39;49mCLIPComparerWidget(cache_name, all_images, all_prompts, models\u001b[39m=\u001b[39;49m[am_model\u001b[39m.\u001b[39;49mCLIPModel(), \u001b[39m'\u001b[39;49m\u001b[39mCyCLIP\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mCLOOB_LAION400M\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\amumo\\widgets.py:831\u001b[0m, in \u001b[0;36mCLIPComparerWidget.__init__\u001b[1;34m(self, dataset_name, all_images, all_prompts, models, close_modality_gap)\u001b[0m\n\u001b[0;32m    829\u001b[0m     \u001b[39mif\u001b[39;00m m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodels\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    830\u001b[0m         modifier \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m%\u001b[39mm\u001b[39m.\u001b[39m\u001b[39m__hash__\u001b[39m()\n\u001b[1;32m--> 831\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodels[m\u001b[39m+\u001b[39mmodifier] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mget_model(m)\n\u001b[0;32m    832\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mtype\u001b[39m(m), model\u001b[39m.\u001b[39mCLIPModelInterface):\n\u001b[0;32m    833\u001b[0m     modifier \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\amumo\\model.py:254\u001b[0m, in \u001b[0;36mget_model\u001b[1;34m(clip_name, image_encoder_name, device)\u001b[0m\n\u001b[0;32m    251\u001b[0m     image_encoder_name \u001b[39m=\u001b[39m available_CLIP_models[clip_name]\u001b[39m.\u001b[39mavailable_models[\u001b[39m0\u001b[39m]\n\u001b[0;32m    252\u001b[0m \u001b[39massert\u001b[39;00m image_encoder_name \u001b[39min\u001b[39;00m available_CLIP_models[clip_name]\u001b[39m.\u001b[39mavailable_models, \u001b[39m'\u001b[39m\u001b[39mchoose one of \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(available_CLIP_models[clip_name]\u001b[39m.\u001b[39mavailable_models)\n\u001b[1;32m--> 254\u001b[0m \u001b[39mreturn\u001b[39;00m available_CLIP_models[clip_name](image_encoder_name, device\u001b[39m=\u001b[39;49mdevice)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\amumo\\model.py:214\u001b[0m, in \u001b[0;36mCLOOB_LAION400M_Model.__init__\u001b[1;34m(self, name, device)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m model_pt\u001b[39m.\u001b[39mget_pt_model(config)\n\u001b[0;32m    213\u001b[0m checkpoint \u001b[39m=\u001b[39m pretrained\u001b[39m.\u001b[39mdownload_checkpoint(config)\n\u001b[1;32m--> 214\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mload_state_dict(model_pt\u001b[39m.\u001b[39;49mget_pt_params(config, checkpoint))\n\u001b[0;32m    215\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39meval()\u001b[39m.\u001b[39mrequires_grad_(\u001b[39mFalse\u001b[39;00m)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    217\u001b[0m \u001b[39m# see: https://github.com/crowsonkb/cloob-training/blob/master/train.py#L215\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\amumo\\CLOOB_local\\cloob_training\\model_pt.py:283\u001b[0m, in \u001b[0;36mget_pt_params\u001b[1;34m(config, checkpoint)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[39massert\u001b[39;00m config[\u001b[39m'\u001b[39m\u001b[39mtext_encoder\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtransformer\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    282\u001b[0m cloob_params \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(\u001b[39mopen\u001b[39m(checkpoint, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m))[\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m--> 283\u001b[0m state \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconvert_jax_vit_image_params(cloob_params[\u001b[39m0\u001b[39m]), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconvert_jax_text_params(cloob_params[\u001b[39m1\u001b[39;49m])}\n\u001b[0;32m    284\u001b[0m \u001b[39mreturn\u001b[39;00m state\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\amumo\\CLOOB_local\\cloob_training\\model_pt.py:236\u001b[0m, in \u001b[0;36mconvert_jax_text_params\u001b[1;34m(params)\u001b[0m\n\u001b[0;32m    234\u001b[0m base \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtext_encoder\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    235\u001b[0m pt_base \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtext_encoder\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 236\u001b[0m params \u001b[39m=\u001b[39m map_to_tensor(params)\n\u001b[0;32m    237\u001b[0m state \u001b[39m=\u001b[39m {}\n\u001b[0;32m    238\u001b[0m \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m params\u001b[39m.\u001b[39mitems():\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\amumo\\CLOOB_local\\cloob_training\\model_pt.py:182\u001b[0m, in \u001b[0;36mmap_to_tensor\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmap_to_tensor\u001b[39m(x):\n\u001b[1;32m--> 182\u001b[0m     \u001b[39mreturn\u001b[39;00m {k: {k2: torch\u001b[39m.\u001b[39mtensor(v2) \u001b[39mfor\u001b[39;00m k2, v2 \u001b[39min\u001b[39;00m v\u001b[39m.\u001b[39mitems()} \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m x\u001b[39m.\u001b[39mitems()}\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\amumo\\CLOOB_local\\cloob_training\\model_pt.py:182\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmap_to_tensor\u001b[39m(x):\n\u001b[1;32m--> 182\u001b[0m     \u001b[39mreturn\u001b[39;00m {k: {k2: torch\u001b[39m.\u001b[39mtensor(v2) \u001b[39mfor\u001b[39;00m k2, v2 \u001b[39min\u001b[39;00m v\u001b[39m.\u001b[39mitems()} \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m x\u001b[39m.\u001b[39mitems()}\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\amumo\\CLOOB_local\\cloob_training\\model_pt.py:182\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmap_to_tensor\u001b[39m(x):\n\u001b[1;32m--> 182\u001b[0m     \u001b[39mreturn\u001b[39;00m {k: {k2: torch\u001b[39m.\u001b[39;49mtensor(v2) \u001b[39mfor\u001b[39;00m k2, v2 \u001b[39min\u001b[39;00m v\u001b[39m.\u001b[39mitems()} \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m x\u001b[39m.\u001b[39mitems()}\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# This widget shows similarity heatmaps for all models in the list for a better comparison between models.\n",
    "am_widgets.CLIPComparerWidget(cache_name, all_images, all_prompts, models=[am_model.CLIPModel(), 'CyCLIP', 'CLOOB_LAION400M']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use a Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "import numpy as np\n",
    "# You can use the CLIPModelInterface to create your own model class wrapper.\n",
    "class CustomModel(am_model.CLIPModelInterface):\n",
    "    available_models = clip.available_models()\n",
    "    model_name = 'MyCLIP'\n",
    "\n",
    "    def __init__(self, name='RN50', device='cpu') -> None:\n",
    "        super().__init__(name, device)\n",
    "        self.model, self.preprocess = clip.load(name, device=device)\n",
    "        self.model.eval()\n",
    "        self.logit_scale = self.model.logit_scale\n",
    "\n",
    "    def encode_image(self, images):\n",
    "        images = [self.preprocess(i) for i in images]\n",
    "        image_input = torch.tensor(np.stack(images)).to(self.device)\n",
    "        return self.model.encode_image(image_input).float().cpu()\n",
    "\n",
    "    def encode_text(self, texts):\n",
    "        text_tokens = clip.tokenize(texts, truncate = True).to(self.device)\n",
    "        return self.model.encode_text(text_tokens).float().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The widget takes any instance of type CLIPModelInterface.\n",
    "am_widgets.CLIPExplorerWidget(cache_name, all_images, all_prompts, models=[CustomModel()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
